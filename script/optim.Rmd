---
title: "Optimisation sans advection nouvelles simus"
author: " "
date: "`r Sys.Date()`"  # Affiche la date actuelle
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 3.5,
                      fig.align = 'center')
```

```{r lib, include=FALSE}
library(generain)
library(reshape2)
library(ggplot2)
source("load_libraries.R")
```


## Data from simulations

Dans un premier temps, j'ai pris des simulations de Brown-Resnick avec moins de sites
mais plus de pas de temps. J'ai pris des simulations avec 4 sites et 500 pas de temps.

```{r simu}
# spatial and temporal structures
ngrid <- 7
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:100

# Simulation
num_iterations <- 5
list_BR <- list()
for (i in 1:num_iterations) {
  file_path <- paste0("../data/simulations_BR/oldies/V0/sim_", ngrid^2, "s_",
                      length(temp), "t/rainBR_",
                      i, ".csv")
  df <- read.csv(file_path)
  list_BR[[i]] <- df
}

simu_df <- list_BR[[1]] # first simulation
nsites <- ncol(simu_df) # number of sites
data <- simu_df
plot(data[, 1])
```

```{r}
# get grid coordinates
sites_coords <- generate_grid_coords(sqrt(nsites))
# get distance matrix
distance_matrix <- as.matrix(dist(sites_coords))
rownames(distance_matrix) <- colnames(distance_matrix) <- paste0("S",
                                                          1:nrow(sites_coords))

print(distance_matrix)
```


```{r}
# Function to calculate excess indicators for each site
excesses_indicators <- function(data, q) {
  # Assume each row represents a moment in time
  data$time <- 1:nrow(data)

  # Convert data to long format
  data_long <- melt(data, id.vars = "time", variable.name = "site",
                    value.name = "value")
  colnames(data_long) <- c("time", "site", "value")

  # Calculate threshold for each site
  thresholds <- aggregate(value ~ site, data_long, function(x) quantile(x, q))

  # Add a column for excesses
  data_long <- merge(data_long, thresholds, by = "site",
                      suffixes = c("", "_threshold"))
  data_long$excess <- ifelse(data_long$value > data_long$value_threshold, 1, 0)

  return(data_long)
}

q <- 0.7 # quantile
data_long <- excesses_indicators(data, q)
print(head(data_long))
```

## Calculate joint excesses

\(k_{ij, \tau} = \sum_t 1_\{X_{s_i, t} > q, X_{s_j, t + \tau}>q\}\)


```{r}
# Function to calculate k_ij,tau for a pair of sites
calculate_kij_tau <- function(data, site_i, site_j, tau) {
  data_i <- data[data$site == site_i, ]
  data_j <- data[data$site == site_j, ]

  if (tau >= 0) {
    data_i_n <- data_i[1:(nrow(data_i) - tau), ]
    data_j_n <- data_j[(tau + 1):nrow(data_j), ]
  } else {
    data_i_n <- data_i[(-tau + 1):nrow(data_i), ]
    data_j_n <- data_j[1:(nrow(data_j) + tau), ]
  }

  sum(data_i_n$excess & data_j_n$excess)
}

site_i <- "S1"
site_j <- "S4"
tau <- 10

kij_tau <- calculate_kij_tau(data_long, site_i, site_j, tau)
print(kij_tau)

# verification
data_s1_lag <- data$S1[1:(nrow(data) - tau)]
data_s4_lag <- data$S4[(1 + tau):(nrow(data))]
kij_tau_verif <- sum(data_s1_lag > quantile(data$S1, q)
          & data_s4_lag > quantile(data$S4, q))
print(kij_tau_verif)
```


```{r}
# Function to calculate k_ij,tau for different tau values
calculate_kij_tau_list <- function(data_long, taus) {
  kij_tau_list <- list()
  sites <- unique(data_long$site)

  for (tau in taus) {
    # Initialize matrix
    kij_matrix <- matrix(0, nrow = length(sites), ncol = length(sites),
                         dimnames = list(sites, sites))
    for (i in 1:length(sites)) { # for each pair of sites
      for (j in 1:length(sites)) {
        # if (i != j) {
          kij_matrix[i, j] <- calculate_kij_tau(data_long, sites[i], sites[j],
                                                tau)
        # }
      }
    }
    kij_tau_list[[as.character(tau)]] <- kij_matrix # for each tau
  }

  return(kij_tau_list)
}

taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
print(kij_tau_list$`0`) # no temporal lag

# get all values in kij_tau_list
all_values <- unlist(kij_tau_list)
# remove 0
all_values <- all_values[all_values != 0]
plot(all_values, main = "Conditional excesses",
                ylab = "k_ij,tau")

# density plot
ggplot(data.frame(x = all_values), aes(x = x)) +
  geom_density() +
  labs(title = "Density plot of k_ij,tau values",
       x = "k_ij,tau",
       y = "Density") +
  theme_minimal()

```

## Calculate marginal excesses

\(n_{j, \tau} = \sum_t 1_\{X_{s_j, t + \tau\}>q}\)

Et ainsi on peut dire que \(k_{ij, \tau} | n_{j, \tau} \sim \text{Bin}(n_{j, \tau}, \chi_{ij, \tau})\).
Où \(\chi_{ij, \tau} = 2(1 - \Phi(\sqrt{0.5 \gamma_{ij, \tau}}))\) ou encore
\(\chi_{ij, \tau} = P(X_{s_i, t} > q | X_{s_j, t + \tau} > q)\) avec \(\gamma_{ij, \tau} = 2(\beta_1 ||h_{ij}||^{\alpha_1} + \beta_2 |\tau|^{\alpha_2})\).

```{r}
calculate_marginal_excesses <- function(data_long, taus, q) {
  results <- list()

  for (tau in taus) {
    shifted_data <- data_long

    # Create a new column for shifted values
    shifted_data$value_shifted <- NA

    # Shift the values for the given tau
    for (site in unique(data_long$site)) {
      site_data <- data_long[data_long$site == site, ]
      if (tau == 0) {
        shifted_data$value_shifted[shifted_data$site == site] <- site_data$value
      } else {
        shifted_data$value_shifted[shifted_data$site == site] <- c(rep(NA, tau),
                                    site_data$value[1:(nrow(site_data) - tau)])
      }
    }

    # Calculate the excesses for the shifted values
    shifted_data$excess <- shifted_data$value_shifted > q

    # Aggregate to calculate the sum of excesses for each site
    nj_tau <- aggregate(excess ~ site, shifted_data, sum, na.rm = TRUE)

    # Store the results in a list
    results[[as.character(tau)]] <- nj_tau
  }

  return(results)
}

nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# get all excess values
all_values <- unlist(lapply(nj_tau_list, function(x) x$excess))

plot(all_values, main = "Marginal excesses",
                ylab = "nj_tau")

# density plot
ggplot(data.frame(x = all_values), aes(x = x)) +
  geom_density() +
  labs(title = "Density plot of marginal excess values",
       x = "marginal excess",
       y = "Density") +
  theme_minimal()

```

## Calculate the log-likelihood

\[
\begin{aligned}
\ell(\theta) &= \sum_{\tau} \sum_{i} \sum_{j} k_{ij, \tau} \log(\chi_{ij, \tau}) + (n_{j, \tau} - k_{ij, \tau}) \log(1 - \chi_{ij, \tau}) \\
\end{aligned}
\]

```{r}
# Function to calculate gamma_theta
gamma_theta <- function(h, tau, beta1, beta2, alpha1, alpha2) {
  gamma <- 2 * (beta1 * h^alpha1 + beta2 * abs(tau)^alpha2)

  if (gamma < 0) {
    gamma <- 0
  }
  return(gamma)
}

# Function to calculate chi_ij with distance matrix
chi_ij <- function(site_i, site_j, tau, theta, distance_matrix) {
  h <- distance_matrix[site_i, site_j]
  beta1 <- theta[1]
  beta2 <- theta[2]
  alpha1 <- theta[3]
  alpha2 <- theta[4]
  gamma <- gamma_theta(h, tau, beta1, beta2, alpha1, alpha2)
  return(2 * (1 - pnorm(sqrt(0.5 * gamma))))
}

log_likelihood <- function(theta, data, distance_matrix, taus, kij_tau_list,
                           nj_tau_list) {
  # print(theta)
  ll <- 0
  sites <- unique(data$site)

  lower.bound <- c(1e-6, 1e-6, 1e-6, 1e-6)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)

  # Check if the parameters are in the bounds
  if (any(theta < lower.bound) || any(theta > upper.bound)) {
    message("out of bounds")
    return(1e8)
  }

  for (tau in taus) {
    kij_matrix <- kij_tau_list[[as.character(tau)]]
    nj_df <- nj_tau_list[[as.character(tau)]]
    for (i in 1:length(sites)) {
      for (j in 1:length(sites)) {
        if (i != j) {
          site_i <- sites[i]
          site_j <- sites[j]
          nj <- nj_df$excess[nj_df$site == site_j]
          kij <- kij_matrix[site_i, site_j]

          chi <- chi_ij(site_i, site_j, tau, theta, distance_matrix)

          ll <- ll + kij * log(chi) + (nj - kij) * log(1 - chi)
        }
      }
    }
  }

  return(-ll)  # Negative log likelihood
}
```


## Variation of beta1, fixing the other parameters

```{r}  
# Define the other theta parameters
beta1 <- 0.4
beta2 <- 0.2
alpha1 <- 1.5
alpha2 <- 1
```

Pour différentes valeurs proches de quantile, la convergence est plutôt
changeante pour \(\beta_1\) dont la vraie valeur est 0.4.
Pour un quantile de 0.6, on obtient une valeur de \(\beta_1\) proche de 0.4.
Pour un quantile de 0.62, on obtient une valeur de \(\beta_1\) proche de 0.5.
Pour un quantile de 0.65, on obtient une valeur de \(\beta_1\) proche de 0.6.

```{r}  
q <- 0.61 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for beta1
beta1_values <- seq(0.1, 2, length.out = 100)

# Calculate the log-likelihood for each beta1 value
log_likelihood_values <- sapply(beta1_values, function(beta) {
  theta <- c(beta, beta2, alpha1, alpha2)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(beta = beta1_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of beta1
ggplot(df, aes(x = beta, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of beta1",
       x = expression(beta[1]),
       y = "Log-Likelihood") +
  theme_minimal()

beta1_values[which.min(log_likelihood_values)]
```

## Variation of beta2, fixing the other parameters

Pour différentes valeurs proches de quantile, la convergence est plutôt
stable pour \(\beta_2\) pas très loin de la vraie valeur 0.2.
Si on prend un quantile de 0.6, on obtient une valeur de \(\beta_2\) proche de 0.14.
Et si on prend un quantile de 0.65, on obtient une valeur de \(\beta_2\) proche de 0.17.
Si on prend un quantile de 0.68, on obtient une valeur de \(\beta_2\) proche de 0.2.
Si on prend un quantile de 0.7, on obtient une valeur de \(\beta_2\) proche de 0.25.

```{r}
q <- 0.68 # quantile
data_long <- excesses_indicators(data, q)
print(head(data_long))

taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for beta2
beta2_values <- seq(0.1, 2, length.out = 100)

# Calculate the log-likelihood for each beta2 value
log_likelihood_values <- sapply(beta2_values, function(beta) {
  theta <- c(beta1, beta, alpha1, alpha2)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(beta = beta2_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of beta2
ggplot(df, aes(x = beta, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of beta2",
       x = expression(beta[2]),
       y = "Log-Likelihood") +
  theme_minimal()

beta2_values[which.min(log_likelihood_values)]
```


## Variation of alpha1, fixing the other parameters

Pour une petite variation de la valeur du quantile, on obtient une valeur de \(\alpha_1\) très 
variable et qui ne converge pas toujours vers la vraie valeur de \(\alpha_1\).

Si on prend un quantile de 0.62, on obtient une valeur de \(\alpha_1\) proche de 1.5.
Mais si on prend un quantile de 0.6, on obtient une valeur de \(\alpha_1\) proche de 1.
Et si on prend un quantile de 0.65, on obtient une valeur de \(\alpha_1\) proche de 2.

```{r}
q <- 0.62 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for alpha1
alpha1_values <- seq(0.1, 1.999, length.out = 100)

# Calculate the log-likelihood for each alpha1 value
log_likelihood_values <- sapply(alpha1_values, function(alpha) {
  theta <- c(beta1, beta2, alpha, alpha2)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(beta = alpha1_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of alpha1
ggplot(df, aes(x = beta, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of alpha1",
       x = expression(alpha[1]),
       y = "Log-Likelihood") +
  theme_minimal()

alpha1_values[which.min(log_likelihood_values)]
```


## Variation of alpha2, fixing the other parameters

Pour une petite variation de la valeur du quantile, on obtient une valeur de \(\alpha_2\) 
oscillant autour de 1 pour le paramètre \(\alpha_2\).

Si on prend un quantile de 0.62, on obtient une valeur de \(\alpha_2\) proche de 0.8.
Si on prend un quantile de 0.65, on obtient une valeur de \(\alpha_2\) proche de 0.9.
Si on prend un quantile de 0.68, on obtient une valeur de \(\alpha_2\) proche de 1.
Si on prend un quantile de 0.7, on obtient une valeur de \(\alpha_2\) proche de 1.
Si on prend un quantile de 0.75, on obtient une valeur de \(\alpha_2\) proche de 1.1.

```{r}
q <- 0.7 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for alpha2
alpha2_values <- seq(0.1, 1.999, length.out = 100)

# Calculate the log-likelihood for each alpha2 value
log_likelihood_values <- sapply(alpha2_values, function(alpha) {
  theta <- c(beta1, beta2, alpha1, alpha)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(alpha = alpha2_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of alpha2
ggplot(df, aes(x = alpha, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of alpha2",
       x = expression(alpha[2]),
       y = "Log-Likelihood") +
  theme_minimal()

alpha2_values[which.min(log_likelihood_values)]
```


## Optimization

Pour l'optimisation, un problème est donc de choisir une valeur
de quantile qui permet d'obtenir une convergence des paramètres alors que l'on 
a vu que ce n'est pas forcément la même valeur de quantile qui permet d'obtenir
une valeur proche des vrais paramètres avec des paramètres fixés et aussi une légère 
variation de quantile peut changer significativement la valeur des paramètres estimés
(surtout le paramètre \(\alpha_1\)).
Je décide de prendre la valeur de quantile pour laquelle on obtient une valeur de \(\alpha_1\)
proche de 1.5.

```{r, message=FALSE}
# Initialize theta parameters with true simulation parameters
theta_init <- c(0.4, 0.2, 1.5, 1)

q <- 0.9 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Optimize composite log-likelihood
optim_result <- optim(par = theta_init,
                      fn  = log_likelihood,
                      data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj = nj_tau_list,
                      method = "CG", # "L-BFGS-B"
                      control = list(maxit = 5000))

convergence <- optim_result$convergence
theta_opt <- optim_result$par

print(convergence) # 0 means convergence
print(theta_opt)
```

Cela converge mais ne donne pas les vraies valeurs des paramètres.
J'ai essayé avec d'autres méthodes d'optimisation mais cela donne le même résultat.

## Optimization, fixing some parameters

J'utilise une autre fonction d'optimisation qui me 
permet de fixer facilement des paramètres. Pour cela 
j'ai du modifier un peu la fonction de log-vraisemblance
pour qu'elle prenne en compte les paramètres fixes.

```{r}
log_likelihood_par <- function(beta1, beta2, alpha1, alpha2, data,
                               distance_matrix, taus, kij_tau_list, 
                               nj_tau_list) {
  theta <- c(beta1, beta2, alpha1, alpha2)
  ll <- 0
  sites <- unique(data$site)

  lower.bound <- c(1e-6, 1e-6, 1e-6, 1e-6)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)

  # Check if the parameters are in the bounds
  if (any(theta < lower.bound) || any(theta > upper.bound)) {
    print("out of bounds")
    return(1e8)
  }

  for (tau in taus) {
    kij_matrix <- kij_tau_list[[as.character(tau)]]
    nj_df <- nj_tau_list[[as.character(tau)]]
    for (i in 1:length(sites)) {
      for (j in 1:length(sites)) {
        if (i != j) {
          site_i <- sites[i]
          site_j <- sites[j]
          nj <- nj_df$excess[nj_df$site == site_j]
          kij <- kij_matrix[site_i, site_j]

          chi <- chi_ij(site_i, site_j, tau, theta, distance_matrix)

          ll <- ll + kij * log(chi) + (nj - kij) * log(1 - chi)
        }
      }
    }
  }

  return(-ll)  # Negative log likelihood
}
```

## Optimization, fixing \(\beta_2\), \(\alpha_1\) and \(\alpha_2\)

Je fixe les paramètres \(\beta_2\), \(\alpha_1\) et \(\alpha_2\)
```{r, message=FALSE}
library(bbmle)

# q = 0.62 (reminder)
res <- mle2(log_likelihood_par, start = list(beta1 = theta_init[1],
                                 beta2 = theta_init[2],
                                 alpha1 = theta_init[3],
                                 alpha2 = theta_init[4]),
                 data = list(data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj_tau_list = nj_tau_list,
                      method = "L-BFGS-B",
                      lower = c(1e-6, 1e-6, 1e-6, 1e-6),
                      upper = c(Inf, Inf, 1.999, 1.999)),
                  control = list(maxit = 10000),
                  fixed = list(beta2 = 0.2, alpha1 = 1.5, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```

Ici on obtient bien une valeur relativement proche de la vraie valeur de \(\beta_1\).
Et si on modifie le quantile on obtient une valeur encore plus proche de la vraie valeur de \(\beta_1\) étant 0.4.
C'est cohérent avec le plot de vraisemblance précédent.
Avec différentes méthodes d'optimisation, on obtient les mêmes résultats.

```{r, message=FALSE}
q <- 0.6 # modified quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

res <- mle2(log_likelihood_par, start = list(beta1 = theta_init[1],
                                 beta2 = theta_init[2],
                                 alpha1 = theta_init[3],
                                 alpha2 = theta_init[4]),
                 data = list(data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj_tau_list = nj_tau_list,
                      method = "CG"),
                  control = list(maxit = 100),
                  fixed = list(beta2 = 0.2, alpha1 = 1.5, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


## Optimization, fixing \(\beta_1\), \(\alpha_1\) and \(\alpha_2\)

Idem, fonctionne bien mais pour un bon choix de quantile pour récupérer une valeur
proche de la vraie valeur de \(\beta_2\) étant 0.2.

```{r, message=FALSE}
q <- 0.68 # modified quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

res <- mle2(log_likelihood_par, start = list(beta1 = theta_init[1],
                                 beta2 = theta_init[2],
                                 alpha1 = theta_init[3],
                                 alpha2 = theta_init[4]),
                 data = list( data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj_tau_list = nj_tau_list,
                      method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, alpha1 = 1.5, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```

## Optimization, fixing \(\beta_1\), \(\beta_2\) and \(\alpha_2\)

Idem pour obtenir une valeur proche de la vraie valeur de \(\alpha_1\) étant 1.5 en prenant 
un bon choix de quantile, sinon ça ne marche pas.

```{r, message=FALSE}
q <- 0.62 # modified quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

res <- mle2(log_likelihood_par, start = list(beta1 = theta_init[1],
                                 beta2 = theta_init[2],
                                 alpha1 = theta_init[3],
                                 alpha2 = theta_init[4]),
                 data = list(data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj_tau_list = nj_tau_list,
                      method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, beta2 = 0.2, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```

## Optimization, fixing \(\beta_1\), \(\beta_2\) and \(\alpha_1\)

Idem pour obtenir une valeur proche de la vraie valeur de \(\alpha_2\) étant 1 en prenant
un bon choix de quantile.

```{r, message=FALSE}
q <- 0.68 # modified quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

res <- mle2(log_likelihood_par, start = list(beta1 = theta_init[1],
                                 beta2 = theta_init[2],
                                 alpha1 = theta_init[3],
                                 alpha2 = theta_init[4]),
                 data = list(data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj_tau_list = nj_tau_list,
                      method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, beta2 = 0.2, alpha1 = 1.5))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


## Optimization, fixing \(\beta_1\), \(\alpha_1\) the spatial parameters

Ici cela ne fonctionne plus du tout surement car les quantiles pour obtenir 
une bonne convergence individuelle des paramètres ne sont pas les mêmes.

```{r,  message=FALSE}
q <- 0.6 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)
res <- mle2(log_likelihood_par, start = list(beta1 = theta_init[1],
                                 beta2 = theta_init[2],
                                 alpha1 = theta_init[3],
                                 alpha2 = theta_init[4]),
                 data = list(data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj_tau_list = nj_tau_list,
                      method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, alpha1 = 1.5))

print(res@details$conv) # 0 means convergence
print(res@coef)
```

Ici je trace la vraisemblance en fonction de \(\beta_1\) pour différentes valeurs de \(\alpha_1\).

```{r, message=FALSE}
q <- 0.65 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# variation of beta1 and alpha1
beta1_values <- seq(0.05, 0.2, length.out = 100)
alpha1_values <- seq(1.05, 1.99, length.out = 50)

# Calculate the log-likelihood for each beta1 and alpha1 value
log_likelihood_values <- matrix(0, nrow = length(beta1_values),
                                ncol = length(alpha1_values))

for (i in 1:length(beta1_values)) {
  for (j in 1:length(alpha1_values)) {
    theta <- c(beta1_values[i], beta2, alpha1_values[j], alpha2)
    log_likelihood_values[i, j] <- log_likelihood(theta, data_long,
                                                 distance_matrix, taus,
                                                 kij_tau_list, nj_tau_list)
  }
}

# Data frame for ggplot
df <- data.frame(beta1 = rep(beta1_values, each = length(alpha1_values)),
                 alpha1 = rep(alpha1_values, length(beta1_values)),
                 log_likelihood = as.vector(log_likelihood_values))

# Filter the data frame for the desired alpha1 values
df_alpha1 <- df[abs(df$alpha1 - c(0.5, 1, 1.5, 2)) < 0.1, ]

# Plot the log-likelihood as a function of beta1 for different alpha1
ggplot(df_alpha1, aes(x = beta1, y = log_likelihood, color = factor(alpha1))) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of beta1 for different alpha1",
    x = expression(beta[1]),
    y = "Log-Likelihood",
    color = expression(alpha[1])) +
  theme_minimal()


df_beta1 <- df[df$beta1 == beta1_values[which.min(abs(beta1_values - 0.4))], ]

# Plot the log-likelihood as a function of alpha1
ggplot(df_beta1, aes(x = alpha1, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of alpha1 for beta1 near 0.4",
       x = expression(alpha[1]),
       y = "Log-Likelihood") +
  theme_minimal()
```

On voit qu'on a aucun minimum ici même lorsque le beta1 est proche de 0.4.

## Optimization, fixing \(\beta_2\), \(\alpha_2\) the temporal parameters

Ici cela ne fonctionne plus du tout surement car les quantiles pour obtenir 
une bonne convergence individuelle des paramètres ne sont pas les mêmes.

```{r,  message=FALSE}
q <- 0.6 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)
res <- mle2(log_likelihood_par, start = list(beta1 = theta_init[1],
                                 beta2 = theta_init[2],
                                 alpha1 = theta_init[3],
                                 alpha2 = theta_init[4]),
                 data = list(data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj_tau_list = nj_tau_list,
                      method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta2 = 0.2, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


## New simulation

Simulation avec 25 sites et 300 pas de temps.

```{r}
# spatial and temporal structures
ngrid <- 5
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:300

# Simulation
num_iterations <- 5
list_BR <- list()
for (i in 1:num_iterations) {
  file_path <- paste0("../data/simulations_BR/sim_", ngrid^2, "s_",
                      length(temp), "t/br_", ngrid^2, "s_", length(temp), "t_",
                      i, ".csv")
  df <- read.csv(file_path)
  list_BR[[i]] <- df
}

simu_df <- list_BR[[1]] # first simulation
nsites <- ncol(simu_df) # number of sites
data <- simu_df
plot(data[, 1])
```


```{r}
# Calculate distance matrix
sites_coords <- expand.grid(x = spa, y = spa)
distance_matrix <- as.matrix(dist(sites_coords))

# Calculate excesses indicators
q <- 0.7 # quantile
data_long <- excesses_indicators(data, q)
print(head(data_long))

taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
print(kij_tau_list$`0`)
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)
print(nj_tau_list$`0`)
```


## Variation of beta1, fixing the other parameters

Pour un quantile de 0.8, on obtient une valeur de \(\beta_1\) proche de 0.48.
Pour un quantile de 0.75, on obtient une valeur de \(\beta_1\) proche de 0.35.
Pour un quantile de 0.7, on obtient une valeur de \(\beta_1\) proche de 0.25.

```{r, echo=FALSE}  
q <- 0.75 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for beta1
beta1_values <- seq(0.1, 2, length.out = 100)

# Calculate the log-likelihood for each beta1 value
log_likelihood_values <- sapply(beta1_values, function(beta) {
  theta <- c(beta, beta2, alpha1, alpha2)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(beta = beta1_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of beta1
ggplot(df, aes(x = beta, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of beta1 for q=0.75",
       x = expression(beta[1]),
       y = "Log-Likelihood") +
  theme_minimal()

beta1_values[which.min(log_likelihood_values)]
```



## Variation of beta2, fixing the other parameters


Si on prend un quantile de 0.7, on obtient une valeur de \(\beta_2\) proche de 0.13.
Si on prend un quantile de 0.75, on obtient une valeur de \(\beta_2\) proche de 0.21.

```{r, echo=FALSE}
q <- 0.75 # quantile
data_long <- excesses_indicators(data, q)
print(head(data_long))

taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for beta2
beta2_values <- seq(0.1, 2, length.out = 100)

# Calculate the log-likelihood for each beta2 value
log_likelihood_values <- sapply(beta2_values, function(beta) {
  theta <- c(beta1, beta, alpha1, alpha2)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(beta = beta2_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of beta2
ggplot(df, aes(x = beta, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of beta2 for q=0.7",
       x = expression(beta[2]),
       y = "Log-Likelihood") +
  theme_minimal()

beta2_values[which.min(log_likelihood_values)]
```


## Variation of alpha1, fixing the other parameters

Si on prend un quantile de 0.7, on obtient une valeur de \(\alpha_1\) proche de 1.
Si on prend un quantile de 0.75, on obtient une valeur de \(\alpha_1\) proche de 1.2.
Si on prend un quantile de 0.8, on obtient une valeur de \(\alpha_1\) proche de 1.5.

```{r}
q <- 0.8 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for alpha1
alpha1_values <- seq(0.1, 1.999, length.out = 100)

# Calculate the log-likelihood for each alpha1 value
log_likelihood_values <- sapply(alpha1_values, function(alpha) {
  theta <- c(beta1, beta2, alpha, alpha2)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(beta = alpha1_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of alpha1
ggplot(df, aes(x = beta, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of alpha1",
       x = expression(alpha[1]),
       y = "Log-Likelihood") +
  theme_minimal()

alpha1_values[which.min(log_likelihood_values)]
```


## Variation of alpha2, fixing the other parameters

Si on prend un quantile de 0.7, on obtient une valeur de \(\alpha_2\) proche de 0.7.
Si on prend un quantile de 0.75, on obtient une valeur de \(\alpha_2\) proche de 1.
Si on prend un quantile de 0.8, on obtient une valeur de \(\alpha_2\) proche de 1.2.

```{r, echo=FALSE}
q <- 0.75 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

# Generate a range of values for alpha2
alpha2_values <- seq(0.1, 1.999, length.out = 100)

# Calculate the log-likelihood for each alpha2 value
log_likelihood_values <- sapply(alpha2_values, function(alpha) {
  theta <- c(beta1, beta2, alpha1, alpha)
  log_likelihood(theta, data_long, distance_matrix, taus, kij_tau_list,
                 nj_tau_list)
})

# Data frame for ggplot
df <- data.frame(alpha = alpha2_values, log_likelihood = log_likelihood_values)

# Plot the log-likelihood as a function of alpha2
ggplot(df, aes(x = alpha, y = log_likelihood)) +
  geom_line() +
  labs(title = "Log-Likelihood as a function of alpha2",
       x = expression(alpha[2]),
       y = "Log-Likelihood") +
  theme_minimal()

alpha2_values[which.min(log_likelihood_values)]
```



## Optimization

Trop long et ne donne pas du tout les vraies valeurs des paramètres.

```{r, eval=FALSE}
q <- 0.8 # quantile
data_long <- excesses_indicators(data, q)
taus <- 0:10 # temporal lags
kij_tau_list <- calculate_kij_tau_list(data_long, taus) # conditional excesses
nj_tau_list <- calculate_marginal_excesses(data_long, taus, q)

theta_init <- c(0.4, 0.2, 1.5, 1) # true parameters
# Optimize composite log-likelihood
optim_result <- optim(par = theta_init,
                      fn  = log_likelihood,
                      data = data_long,
                      distance_matrix = distance_matrix,
                      taus = taus,
                      kij_tau_list = kij_tau_list,
                      nj = nj_tau_list,
                      method = "CG", # "L-BFGS-B"
                      control = list(maxit = 5000))

convergence <- optim_result$convergence
theta_opt <- optim_result$par

print(convergence) # 0 means convergence
print(theta_opt)
```

