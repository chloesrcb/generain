---
title: "Model on Montpellier rainfall"
author: " "
date: "`r Sys.Date()`" 
output:
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 5,
                      fig.align = 'center', message = FALSE, warning = FALSE,
                      fig.pos='H')
par(cex.main = 0.8,
    cex.lab = 0.7,
    cex.axis = 0.6)
```


```{r lib, echo=FALSE}
# Load libraries and set theme
# setwd("./script")
library(generain)
library(reshape2)
library(ggplot2)
source("./script/load_libraries.R")
library(kableExtra)
library(extRemes)
library(bbmle)
library(ismev)
library(extRemes)
library(evd)
library(latex2exp)
library(geosphere)

btf_theme <- theme_minimal() +
  theme(axis.text.x = element_text(size =  6, angle = 0),
        axis.text.y = element_text(size = 6),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 6),
        title = element_text(size = 10),
        axis.line = element_blank(),  # Remove axis lines
        panel.border = element_blank(),  # Remove plot border
        panel.background = element_rect(fill = "transparent", color = NA),
        # Remove plot background
        axis.text = element_blank(),  # Remove axis text
        axis.ticks = element_blank(),  # Remove axis ticks
        plot.background = element_rect(fill = "transparent", color = NA),
        panel.grid = element_line(color = "#5c595943"))

# my green color
btfgreen <- "#69b3a2"
```


```{r, echo=FALSE}
################################################################################
# LOCATION ---------------------------------------------------------------------
################################################################################
# get location of each rain gauge
# setwd("./script")
location_gauges <- read.csv("./data/PluvioMontpellier_1min/pluvio_mtp_loc.csv")
location_gauges$codestation <- c("iem", "mse", "poly", "um", "cefe", "cnrs",
                                 "crbm", "archiw", "archie", "um35", "chu1",
                                 "chu2", "chu3", "chu4", "chu5", "chu6", "chu7")

# Get distances matrix
dist_mat <- get_dist_mat(location_gauges)
df_dist <- reshape_distances(dist_mat)

################################################################################
# DATA -------------------------------------------------------------------------
################################################################################
# get rain measurements
# load data
load("./data/PluvioMontpellier_1min/rain_mtp_5min_2019_2022.RData")
rain <- rain.all5[c(1, 6:ncol(rain.all5))]
```


```{r, echo=FALSE}
# put dates as index
rownames(rain) <- rain$dates
# Remove the Time column to focus on site data
rain <- rain[, -1]

# Remove rows where all values are NA
rain <- rain[rowSums(is.na(rain)) != ncol(rain), ]
# head(rain)

dist_matrix <- distm(
location_gauges[, c("Longitude", "Latitude")],
fun = distHaversine
)

colnames(dist_matrix) <- location_gauges$codestation
rownames(dist_matrix) <- location_gauges$codestation

dist_df <- as.data.frame(as.table(dist_matrix))
names(dist_df) <- c("Site1", "Site2", "Distance")

dist_df <- dist_df[as.character(dist_df$Site1) != as.character(dist_df$Site2), ]

autocorr_pairs <- data.frame(
Site1 = location_gauges$codestation,
Site2 = location_gauges$codestation,
Distance = 0
)


# Combine the original distance dataframe with autocorr_pairs
dist_df <- rbind(dist_df, autocorr_pairs)

# Define breaks with 0 explicitly included at the beginning
breaks <- quantile(dist_df$Distance,
                     probs = seq(0, 1, length.out = 12))

dist_df$Class <- cut(dist_df$Distance, breaks = breaks,
                       labels = 0:10, include.lowest = TRUE)

# from breaks get intervals for each class
class_intervals <- data.frame(
  Class = 0:10,
  Interval = paste0("[", round(breaks[-length(breaks)],0), ", ", round(breaks[-1],0), "]")
)

# Print the resulting class intervals
print(class_intervals)


# Calculate the frequency of distances in each class
class_frequency <- table(dist_df$Class)

# Create a data frame with class intervals, frequency, and the width for each class
class_interval_summary <- data.frame(
  Class = as.integer(names(class_frequency)),
  Frequency = as.integer(class_frequency),
  IntervalStart = breaks[-length(breaks)],
  IntervalEnd = breaks[-1],
  IntervalWidth = breaks[-1] - breaks[-length(breaks)]
)


# Now create the data for geom_rect
df_for_rect <- data.frame(
  xmin = class_interval_summary$IntervalStart,  # Starting position (x-axis)
  xmax = class_interval_summary$IntervalEnd,    # Ending position (x-axis)
  ymin = 0,                                     # Bottom of the bars (y-axis)
  ymax = class_interval_summary$Frequency,       # Height of the bars (y-axis)
  Class = class_interval_summary$Class,          # Class for coloring
  IntervalWidth = class_interval_summary$IntervalWidth  # For sizing
)

# Plot the histogram using geom_rect for variable bar widths
ggplot(df_for_rect) +
  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), colour = "#f3eeee", fill=btfgreen, alpha=0.5) +
  theme_minimal() +
  labs(
    x = "Distance Class Interval",
    y = "Frequency (number of pairs)"
  ) +
  scale_x_continuous(breaks = breaks, labels = round(breaks, 0)) +  # Custom x-axis breaks
  btf_theme

# save plot
ggsave("./images/histogram_distance_class_intervals.png", width = 6, height = 4)

# Create matrix of radius
radius_intervals <- unique(breaks)
radius <- as.integer(radius_intervals)
rad_mat <- dist_mat
# Loop through radius and set distances in matrix
for (i in 2:length(radius)) {
  curr_radius <- radius[i]
  prev_radius <- radius[i - 1]
  rad_mat[dist_mat >= prev_radius & dist_mat < curr_radius] <- curr_radius
  # rad_mat[dist_mat > curr_radius] <- Inf
}

rad_mat[dist_mat == 0] <- 0
# Make a triangle
rad_mat[lower.tri(rad_mat)] <- NA
# in km
rad_mat <- rad_mat * 1000 # change les resultats...

```



```{r, fig.width = 4, fig.height = 3, echo=FALSE, fig.cap="Histogram of distance classes"}
# # spatial structure with an almost constant amount of pairs in each intervals
df_dist_order <- df_dist[order(df_dist$value), ]
num_intervals <- 15
quantiles_rad <- quantile(df_dist_order$value,
                            probs = seq(0, 1, length.out = num_intervals + 1))
radius_intervals <- unique(quantiles_rad)
radius <- as.integer(radius_intervals)
radius[length(radius)] <- 1550
dist_counts <- table(cut(df_dist$value, breaks = radius))

# Get dataframe for the histogram plot
df_hist <- data.frame(dist_counts)

colnames(df_hist) <- c("Interval", "Count")

# df_hist$Breaks <- gsub("e\\+0.", "0", df_hist$Interval)
# df_hist$Breaks <- gsub("\\.", "", df_hist$Breaks)

# # # Histogram
# histradius <- ggplot(df_hist, aes(x = Interval, y = Count)) +
#   btf_theme +
#   geom_bar(stat = "identity", fill = btfgreen, alpha = 0.8) +
#   xlab("Spatial lag") +
#   ylab("Pair count") +
#   theme(axis.text.x = element_text(angle = 45)) +
#   scale_x_discrete(labels = df_hist$Breaks) +
#   scale_y_continuous(breaks = c(0, 4, 6, 8, 10, 12))

# histradius

# Create matrix of radius
rad_mat <- dist_mat
# Loop through radius and set distances in matrix
for (i in 2:length(radius)) {
  curr_radius <- radius[i]
  prev_radius <- radius[i - 1]
  rad_mat[dist_mat >= prev_radius & dist_mat < curr_radius] <- curr_radius
  rad_mat[dist_mat > curr_radius] <- Inf
}

rad_mat[dist_mat == 0] <- 0
# Make a triangle
rad_mat[lower.tri(rad_mat)] <- NA
# in km
rad_mat <- rad_mat / 1000 # change les resultats...
```

<!-- 
We choose a quantile $q = 0.96$ without zeros for the spatial and temporal chi 
estimation. We get an estimate of the spatial and temporal variogram parameters using the
WLSE method from Buhl et al. (2019). -->

```{r, echo=FALSE, show=FALSE}
# get the empirical chi
rain_nozeros <- rain[rowSums(rain) > 0, ]
# reindex
rownames(rain_nozeros) <- 1:nrow(rain_nozeros)

# Spatial chi
q <- 0.998
chispa <- spatial_chi(rad_mat, rain, quantile = q)
spa_estim <- get_estimate_variospa(chispa, weights = "exp", summary = F)

tmax <- 10
q <- 0.998
chitemp <- temporal_chi(rain, tmax = tmax, quantile = q)
temp_estim <- get_estimate_variotemp(chitemp, tmax, npoints = ncol(rain_nozeros),
                                      weights = "exp", summary = F)

df_result <- data.frame(beta1 =  spa_estim[1],
                        beta2 = temp_estim[1],
                        alpha1 = spa_estim[2],
                        alpha2 = temp_estim[2])

# colnames(df_result) <- c("beta1", "alpha1", "beta2", "alpha2")

# kable(df_result, format = "latex") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed",
#   "responsive"), latex_options = "H")
```

# COMEPHORE data

An other dataset is considered, the COMEPHORE radar renalysis data from Météo France.
We consider 59 pixels in the Montpellier area.

```{r, echo=FALSE}
# setwd("./script")
# load data
df_comephore <- read.csv("./data/comephore/inside_mtp.csv", sep = ",")
loc_px <- read.csv("./data/comephore/loc_pixels_mtp.csv", sep = ",")
# put date in index
rownames(df_comephore) <- df_comephore$date
# ncol(df_comephore) # nb of pixels + dates
comephore <- df_comephore[-1] # remove dates column
# Get distances matrix
dist_mat <- get_dist_mat(loc_px)
df_dist <- reshape_distances(dist_mat)
```

# Quantile choice

```{r}
# # we want a quantile for u = 1
# find_q_for_threshold <- function(col, threshold) {
#   if (all(is.na(col))) return(NA)  
#   ecdf(col)(threshold) 
# }

# # Appliquer à chaque colonne du dataframe
# q_values <- sapply(comephore, find_q_for_threshold, threshold = 1)

# # Afficher les résultats
# q <- mean(q_values)
```

```{r}
# Remove lines with just zeros
# comephore_nozeros <- comephore[rowSums(comephore) > 0, ]

# # Choose two site and remove zeros
# comephore_pair <- comephore_nozeros[, c(1,15)]
# comephore_pair <- comephore_pair[rowSums(comephore_pair) > 0, ]
# chiplot(comephore_pair, xlim = c(0.8, 1), ylim1 = c(0.5, 1), which = 1,
#         qlim = c(0.8, 0.999))
# abline(v = 0.95, col = "red", lty = 2)

# # count conjoint excesses
# q <- 0.95
# # uniformize the data
# n <- nrow(comephore_pair)
# data_unif <- cbind(rank(comephore_pair[, 1]) / (n + 1),
#                           rank(comephore_pair[, 2]) / (n + 1))

# count_excesses <- sum(data_unif[, 1] > q & data_unif[, 2] > q)
# print(count_excesses)

# # With all zeros
# comephore_pair <- comephore[, c(1,10)]
# chiplot(comephore_pair, xlim = c(0.98, 1), ylim1 = c(0.5, 1), which = 1,
#         qlim = c(0.98, 0.9995))

# threshold <- quantile(comephore$p102, probs = 0.998, na.rm = T)
# # get the quantile from threshold in the data without 0 when the quantile is
# # 0.998 with zeros inside data
# empirical_cdf <- ecdf(comephore_pair$p102)
# quantile_in_nozeros <- empirical_cdf(threshold)
# print(quantile_in_nozeros)

# # Temporal chi
# rain_nolag <- comephore_nozeros$p142[1:(length(comephore_nozeros$p142) - 5)]
# rain_lag <- comephore_nozeros$p142[6:length(comephore_nozeros$p142)]
# comephore_pair <- cbind(rain_nolag, rain_lag)
# comephore_pair <- comephore_pair[rowSums(comephore_pair) > 0, ]
# chiplot(comephore_pair, xlim = c(0.9, 1), ylim1 = c(0, 1), which = 1,
#         qlim = c(0.9, 0.995))
# abline(v = 0.95, col = "red", lty = 2)

# n <- nrow(comephore_pair)
# data_unif <- cbind(rank(comephore_pair[, 1]) / (n + 1),
#                           rank(comephore_pair[, 2]) / (n + 1))
# q <- 0.95
# count_excesses <- sum(data_unif[, 1] > q & data_unif[, 2] > q)
# print(count_excesses)

# # We choose q = 0.95
# q <- 0.95
```

# Empirical chi and WLSE

```{r, echo=FALSE}
# tmax <- 10
# nsites <- length(loc_px$pixel_name)
# # Temporal chi with spatial lag fixed at 0
# # compute chiplot of every site with itself but lagged in time
# start_time <- Sys.time()
# chimat_dtlag <- temporal_chi(comephore_nozeros, quantile = q, tmax = tmax,
#                              mean = FALSE)
# end_time <- Sys.time()
# elapsed_time <- end_time - start_time
# print(elapsed_time)
```

## Temporal chi

```{r, echo=FALSE}
# remove 0
q <- 0.996
tmax <- 10
chimat_dt_mean <- temporal_chi(comephore, tmax, quantile = q,
                               mean = TRUE)
# get h axis in minutes ie x5 minutes
df <- data.frame(lag = c(1:tmax), chi = chimat_dt_mean)
chitemp_plot <- ggplot(df, aes(x = lag, y = chi)) +
  geom_point(color = btfgreen) +
  btf_theme +
  xlab("Temporal lag") +
  ylab(TeX(r"($\hat{\chi}$)"))

wlse_temp <- get_estimate_variotemp(chimat_dt_mean, tmax, nsites,
                                    weights = "exp", summary = T)

print(wlse_temp)
alpha2 <- wlse_temp[[2]]
beta2 <- wlse_temp[[1]]
c2 <- log(beta2)

dftemp <- data.frame(lag = log(df$lag), chi = eta(df$chi))

chitemp_eta_estim <- ggplot(dftemp, aes(x = lag, y = chi)) +
  geom_point(color = btfgreen) +
  btf_theme +
  xlab(TeX(r"($\log(\tau)$)")) +
  ylab(TeX(r"($\eta(\widehat{\chi}(0,\tau))$)")) +
  geom_line(aes(x = lag, y = alpha2 * lag + c2),
            alpha = 0.5, color = "darkred", linewidth = 0.5)
```

```{r, echo=FALSE, fig.height = 3, fig.width = 5, fig.cap="Empirical temporal extremogram"}
chitemp_plot
```

```{r, echo=FALSE, fig.height = 3, fig.width = 5, fig.cap="Empirical temporal extremogram with eta transformation and WLSE"}
chitemp_eta_estim
```

## Spatial chi

```{r, echo=FALSE}
# Spatial chi ------------------------------------------------------------------
df_dist <- reshape_distances(dist_mat)
df_dist$value <- ceiling(df_dist$value / 100) * 100  # / 1000 in km
df_dist_km <- df_dist
df_dist_km$value <- df_dist$value / 1000

# q <- 0.996
chispa_df <- spatial_chi_alldist(df_dist_km, data_rain = comephore,
                                 quantile = q, hmax = 7)

etachispa_df <- data.frame(chi = eta(chispa_df$chi),
                           lagspa = log(chispa_df$lagspa))

chispa_plot <- ggplot(chispa_df, aes(lagspa, chi)) +
  btf_theme +
  geom_point(col = btfgreen) +
  xlab(TeX(r"($h$)")) +
  ylab(TeX(r"($\widehat{\chi}(h, 0)$)")) +
  ylim(0, 1)

# WLSE
wlse_spa <- get_estimate_variospa(chispa_df, weights = "exp", summary = T)
print(wlse_spa)

alpha1 <- wlse_spa[[2]]
beta1 <- wlse_spa[[1]]
c1 <- log(beta1)

chispa_eta_estim <- ggplot(etachispa_df, aes(lagspa, chi)) +
  btf_theme +
  geom_point(col = btfgreen) +
  xlab(TeX(r"($\log(h)$)")) +
  ylab(TeX(r"($\eta(\widehat{\chi}(h, 0))$)")) +
  geom_line(aes(x = lagspa, y = alpha1 * lagspa + c1), alpha = 0.6,
            color = "darkred", linewidth = 0.5)
```

```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Empirical spatial extremogram"}
chispa_plot
```

```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Empirical spatial extremogram with eta transformation and WLSE"}
chispa_eta_estim
```

Results of the WLSE method on the COMEPHORE data are:

```{r, echo=FALSE}
df_result <- data.frame(beta1 =  beta1,
                        beta2 = beta2,
                        alpha1 = alpha1,
                        alpha2 = alpha2)

colnames(df_result) <- c("beta1", "beta2", "alpha1", "alpha2")

kable(df_result, format = "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",
  "responsive"), latex_options = "H")
```

# Optimization of the composite likelihood

## Choose conditional points

```{r, echo=FALSE}
# Function to choose conditional points
# choose_conditional_points <- function(sites_coords, data, quantile,
#                                       min_spatial_dist, min_time_dist) {
#   # Ensure the quantile is within range
#   if (quantile < 0 || quantile > 1) {
#     stop("Quantile must be between 0 and 1.")
#   }

#   # Uniformize the data
#   data_unif <- data
#   for (i in 1:ncol(data)) {
#     data_unif[, i] <- rank(data[, i]) / (nrow(data) + 1)
#   }

#   # Identify valid (site, time) where X_s,t > quantile
#   valid_indices <- which(as.matrix(data_unif) > quantile, arr.ind = TRUE)

#   if (nrow(valid_indices) == 0) {
#     stop("No points exceed the given quantile for any time step.")
#   }

#   # Initialize the selected points
#   selected_points <- list()

#   # Variable to store the last selected point
#   last_selected <- NULL

#   # Iterate over valid indices to select points
#   for (i in seq_len(nrow(valid_indices))) {
#     current_index <- valid_indices[i, ] # Current index (time, site)
#     current_point <- list(
#       s0 = sites_coords[current_index[2], ],
#       t0 = as.numeric(current_index[1])
#     )

#     if (is.null(last_selected)) {
#       selected_points <- append(selected_points, list(current_point))
#       last_selected <- current_point # Update last selected point
#     } else {
#       # Check if this point is valid given previously selected points
#       # and the minimum spatial and time distances
#       dist_spa <- distHaversine(current_point$s0, last_selected$s0) / 1000
#       dist_time <- abs(current_point$t0 - last_selected$t0)
#       if (dist_spa >= min_spatial_dist || dist_time >= min_time_dist) {
#         selected_points <- append(selected_points, list(current_point))
#         last_selected <- current_point  # Update last selected point
#       }
#     }
#   }

#   if (length(selected_points) == 0) {
#     stop("No spatio-temporal points satisfy the quantile, the minimum spatial 
#           and/or time distances.")
#   }

#   return(selected_points)
# }



select_extreme_episodes <- function(sites_coords, data, quantile,
                                    min_spatial_dist, min_time_dist,
                                    delta, n_max_episodes, beta = 0) {
  # get maximal value
  max_value <- max(na.omit(data))
  max_indices <- which(data == max_value, arr.ind = TRUE)
  data_temp <- data # copy of the data

  # initialize the selected points list
  selected_points <- data.frame(s0 = character(), t0 = integer())

  # transform the data to a uniform
  # data_unif <- data
  # for (i in 1:ncol(data)) {
  #   data_unif[, i] <- rank(data[, i]) / (nrow(data) + 1)
  # }
  # data_temp <- data_unif
  i <- 0
  nb_episode <- 0 # number of episodes (selected points)
  while (nb_episode < n_max_episodes && max_value > quantile) {
    best_candidate <- NULL

    for (idx in 1:nrow(max_indices)) {
      s0_candidate <- colnames(data)[max_indices[idx, 2]]
      t0_candidate <- max_indices[idx, 1]

      if (nrow(selected_points) == 0) { # first episode
        best_candidate <- data.frame(s0 = s0_candidate, t0 = t0_candidate)
        nb_episode <- nb_episode + 1
        break
      } else {
        # get distances and time differences within the selected points
        distances <- distHaversine(sites_coords[selected_points$s0, ],
                                   sites_coords[s0_candidate, ]) / 1000
        time_differences <- abs(selected_points$t0 - t0_candidate)

        # check if the candidate is valid given the minimum spatial and
        # time distances
        if (all(distances >= min_spatial_dist) ||
                                      all(time_differences >= min_time_dist)) {
          best_candidate <- data.frame(s0 = s0_candidate, t0 = t0_candidate)
          nb_episode <- nb_episode + 1
          break # stop the loop
        } else { # not valid
          # "remove" the candidate from the data to avoid selecting it again
          data_temp[t0_candidate, s0_candidate] <- -Inf
        }
      }
    }

    # add the best candidate to the selected points
    selected_points <- rbind(selected_points, best_candidate)

    # "remove" the episode from the data
    t_inf <- best_candidate$t0 - (delta - 1) - beta
    t_sup <- best_candidate$t0 + (delta - 1) + beta
    data_temp[t_inf:t_sup, best_candidate$s0] <- -Inf

    # get the new maximal value
    max_value <- max(na.omit(data_temp))
    max_indices <- which(data_temp == max_value, arr.ind = TRUE)
    i <- i + 1
  }
  return(selected_points)
}

get_extreme_episodes <- function(selected_points, data, delta, beta = 0) {
  # initialize the list of episodes
  episodes <- list()
  for (i in 1:nrow(selected_points)) {
    # s0 <- selected_points$s0[i]
    t0 <- selected_points$t0[i]
    t_inf <- t0 - (delta - 1) - beta
    t_sup <- t0 + (delta - 1) + beta
    episode <- data[t_inf:t_sup, ] # get the episode
    episodes <- append(episodes, list(episode))
  }
  return(episodes)
}

```

```{r, echo=FALSE}
min_spatial_dist <- 5  # in km
min_time_dist <- 10  # in hours
delta <- 8 # step for the episode before and after the max value

# Get coords
sites_coords <- loc_px[, c("Longitude", "Latitude")]
rownames(sites_coords) <- loc_px$pixel_name

# Get the selected episodes
selected_points <- select_extreme_episodes(sites_coords, comephore, q,
                                        min_spatial_dist, min_time_dist,
                                        delta = delta, n_max_episodes = 10000)

length(selected_points$s0)
length(unique(selected_points$s0)) # can be same s0
length(unique(selected_points$t0)) # never same t0

# Get t0 of same s0
selected_points <- selected_points[order(selected_points$s0), ]
# histogram of s0 (s0 as factor)
hist_s0 <- ggplot(selected_points, aes(x = s0)) +
  geom_bar(fill = btfgreen, alpha = 0.8) +
  btf_theme +
  xlab("Site") +
  ylab("Count") +
  theme(axis.text.x = element_text(angle = 45))
hist_s0

# for s0 == "p142" plot each t0
# plot each episode
selected_points_p142 <- selected_points[selected_points$s0 == "p142", ]
# order the t0
selected_points_p142 <- selected_points_p142[order(selected_points_p142$t0), ]
plot(selected_points_p142$t0, type = "l", col = btfgreen, xlab = "Episode",
     ylab = "t0", main = "t0 for s0 = p142")


list_episodes <- get_extreme_episodes(selected_points, comephore,
                                      delta = delta)
head(list_episodes[[1]])
s0_list <- selected_points$s0
t0_list <- selected_points$t0

```

```{r, echo=TRUE}

# # remove 0
# rain_new <- comephore_nozeros
# quantile <- 0.96
# min_spatial_dist <- 4 # in km
# min_time_dist <- 24*30 # in hours

# selected_points <- choose_conditional_points(
#   sites_coords = sites_coords,
#   data = rain_new,
#   quantile = quantile,
#   min_spatial_dist = min_spatial_dist,
#   min_time_dist = min_time_dist
# )

# # Extract s0 and t0
# s0_list <- lapply(selected_points, `[[`, "s0")
# s_unique <- unique(s0_list)
# t0_list <- lapply(selected_points, `[[`, "t0")

# length(s0_list)
# length(s_unique)

```

We have `r length(s0_list)` conditional points. We compute the lags and excesses for each of them.

```{r, echo=FALSE}
library(parallel)

threshold <- quantile(comephore$p102, probs = q, na.rm = T)
# Compute the lags and excesses for each conditional point
list_results <- mclapply(1:length(s0_list), function(i) {
  s0 <- s0_list[i]
  s0_coords <- sites_coords[s0, ]
  t0 <- t0_list[i]
  episode <- list_episodes[[i]]
  lags <- get_conditional_lag_vectors(sites_coords, s0_coords, t0, 
                                  tau_max = tmax, latlon = TRUE)
  lags$hx <- lags$hx / 1000  # in km
  lags$hy <- lags$hy / 1000  # in km
  lags$hnorm <- lags$hnorm / 1000  # in km
  excesses <- empirical_excesses(episode, threshold, lags, type = "rpareto",
                                  threshold = TRUE)
  list(lags = lags, excesses = excesses)
}, mc.cores = detectCores() - 1)

list_lags <- lapply(list_results, `[[`, "lags")
list_excesses <- lapply(list_results, `[[`, "excesses")
```


```{r, echo=FALSE}
# neg_ll_composite_rpar <- function(params, list_episodes, list_lags, quantile,
#                     list_excesses, hmax = NA, s0_list = NA,
#                     t0_list = NA, threshold = FALSE) {
#   # print(params)
#   # # Bounds for the parameters
#   # lower.bound <- c(1e-8, 1e-8, 1e-8, 1e-8)
#   # upper.bound <- c(Inf, Inf, 1.999, 1.999)
#   # if (length(params) == 6) {
#   #   lower.bound <- c(lower.bound, -Inf, -Inf)
#   #   upper.bound <- c(upper.bound, Inf, Inf)
#   # }

#   # # Check if the parameters are in the bounds
#   # if (any(params < lower.bound) || any(params > upper.bound)) {
#   #   return(1e50)
#   # }

#   m <- length(list_excesses) # number of r-pareto processes
#   nll_composite <- 0 # composite negative log-likelihood
#   for (i in 1:m) {
#     # extract lags and excesses from i-th r-pareto process from data
#     df_lags <- list_lags[[i]]
#     excesses <- list_excesses[[i]]
#     episode <- list_episodes[[i]]
#     s0 <- s0_list[i]
#     t0 <- t0_list[i]
#     nll_i <- neg_ll(params, episode, df_lags, quantile, hmax = hmax,
#                     excesses = excesses, s0 = s0, t0 = t0,
#                     threshold = threshold)
#     nll_composite <- nll_composite + nll_i
#   }
#   return(nll_composite)
# }
```

```{r, echo=FALSE}
# init_param <- c(0.01, 0.2, 1.5, 1, -0.1, -0.2)
# q <- 1
# result <- optim(par = init_param, fn = neg_ll_composite_rpar,
#         list_episodes = list_episodes, quantile = q, list_lags = list_lags,
#         list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
#         t0_list = t0_list, threshold = TRUE,
#         method = "L-BFGS-B",
#         lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
#         upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
#         control = list(maxit = 10000),
#         hessian = F)
# estimated_params_noadv <- result$par
# # Check the convergence
# if (result$convergence != 0) {
#   warning("The optimization did not converge.")
# }

# # Extract the results
# df_result_noadv <- data.frame(beta1 =  result$par[1],
#                         beta2 = result$par[2],
#                         alpha1 = result$par[3],
#                         alpha2 = result$par[4],
#                         adv1 = result$par[5],
#                         adv2 = result$par[6])

# colnames(df_result_noadv) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

# kable(df_result_noadv, format = "latex") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed",
#   "responsive"), latex_options = "H")
```


Same but now we consider an advection of 0.1 for both directions. It converges and the results of 
the optimization on the COMEPHORE data are:

```{r, echo=FALSE}
# init_param <- c(beta1, beta2, alpha1, alpha2, 0.2, 0.1)

# result <- optim(par = init_param, fn = neg_ll_composite_rpar,
#         data = rain_new, quantile = q, list_lags = list_lags,
#         list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
#         t0_list = t0_list, threshold = FALSE,
#         method = "L-BFGS-B",
#         lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
#         upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
#         control = list(maxit = 10000),
#         hessian = F)
# estimated_params <- result$par
# # Check the convergence
# if (result$convergence != 0) {
#   warning("The optimization did not converge.")
# }

# # Extract the results
# df_result <- data.frame(beta1 =  result$par[1],
#                         beta2 = result$par[2],
#                         alpha1 = result$par[3],
#                         alpha2 = result$par[4],
#                         adv1 = result$par[5],
#                         adv2 = result$par[6])

# colnames(df_result) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

# kable(df_result, format = "latex") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed",
#   "responsive"), latex_options = "H")
```


Same but now we consider an advection of $0.01$ for both directions. It converges and the results of 
the optimization on the COMEPHORE data are:

```{r, echo=FALSE}
# init_param <- c(beta1, beta2, alpha1, alpha2, 0.01, 0.01)

# result <- optim(par = init_param, fn = neg_ll_composite_rpar,
#         data = rain_new, quantile = q, list_lags = list_lags,
#         list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
#         t0_list = t0_list, threshold = FALSE,
#         method = "L-BFGS-B",
#         lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
#         upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
#         control = list(maxit = 10000),
#         hessian = F)
# estimated_params2 <- result$par
# # Check the convergence
# if (result$convergence != 0) {
#   warning("The optimization did not converge.")
# }

# # Extract the results
# df_result2 <- data.frame(beta1 =  result$par[1],
#                         beta2 = result$par[2],
#                         alpha1 = result$par[3],
#                         alpha2 = result$par[4],
#                         adv1 = result$par[5],
#                         adv2 = result$par[6])

# colnames(df_result2) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

# kable(df_result2, format = "latex") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed",
#   "responsive"), latex_options = "H")
```


Same but now we consider an initial advection of $(0.2, 0.1)$. It converges and the results of 
the optimization on the COMEPHORE data are:

```{r, echo=FALSE}
# init_param <- c(beta1, beta2, alpha1, alpha2, 0.2, 0.1)

# result <- optim(par = init_param, fn = neg_ll_composite_rpar,
#         data = rain_new, quantile = q, list_lags = list_lags,
#         list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
#         t0_list = t0_list, threshold = FALSE,
#         method = "L-BFGS-B",
#         lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
#         upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
#         control = list(maxit = 10000),
#         hessian = F)
# estimated_params2 <- result$par
# # Check the convergence
# if (result$convergence != 0) {
#   warning("The optimization did not converge.")
# }

# # Extract the results
# df_result2 <- data.frame(beta1 =  result$par[1],
#                         beta2 = result$par[2],
#                         alpha1 = result$par[3],
#                         alpha2 = result$par[4],
#                         adv1 = result$par[5],
#                         adv2 = result$par[6])

# colnames(df_result2) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

# kable(df_result2, format = "latex") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed",
#   "responsive"), latex_options = "H")
```

```{r, echo=FALSE}
# # Get the hessian
# hessian <- result$hessian
# cov_matrix <- solve(hessian)

# eigenvalues <- eigen(hessian)$values
# if (any(eigenvalues <= 0)) {
#   stop("Hessian matrix is not positive definite.")
# }

# eigen_decomp <- eigen(hessian)
# eigenvalues <- eigen_decomp$values
# eigenvectors <- eigen_decomp$vectors

# tol <- 1e-6
# eigenvalues[eigenvalues <= tol] <- tol

# hessian_reg <- eigenvectors %*% diag(eigenvalues) %*% t(eigenvectors)

# cov_matrix <- solve(hessian_reg)
# se <- sqrt(diag(cov_matrix))
# conf_int <- cbind(result$par - 1.96 * se, result$par + 1.96 * se)
```


```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Variogram estimate with no initial advection in the optimization"}
# compute variogram with parameters
# tau_values <- c(0)
# result <- df_result_noadv
# df_lags <- list_lags[[5]]
# generate_variogram_plots(result, df_lags, estimated_params_noadv, tau_values, chi=F)

# tau_values <- c(5)
# result <- df_result_noadv
# df_lags <- list_lags[[5]]
# generate_variogram_plots(result, df_lags, estimated_params_noadv, tau_values, chi=F)

# tau_values <- c(10)
# result <- df_result_noadv
# df_lags <- list_lags[[5]]
# generate_variogram_plots(result, df_lags, estimated_params_noadv, tau_values, chi=F)
```


```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Variogram estimate with initial advection of 0.1 in the optimization"}
# compute variogram with parameters
# tau_values <- c(0)
# result <- df_result
# df_lags <- list_lags[[5]]
# generate_variogram_plots(result, df_lags, estimated_params, tau_values, chi=F)

# tau_values <- c(5)
# result <- df_result
# df_lags <- list_lags[[5]]
# generate_variogram_plots(result, df_lags, estimated_params, tau_values, chi=F)

# tau_values <- c(10)
# result <- df_result
# df_lags <- list_lags[[5]]
# generate_variogram_plots(result, df_lags, estimated_params, tau_values, chi=F)
```

# Add wind per episode


```{r, echo=FALSE}
# get wind data
wind_mtp <- read.csv("./data/wind/data_gouv/wind_mtp.csv")

library(dplyr)

# Convert datetime to POSIXct
wind_mtp$datetime <- as.POSIXct(wind_mtp$datetime,
                                format = "%Y-%m-%d %H:%M:%S", tz = "UTC")

convert_to_cardinal <- function(degrees) {
  if (is.na(degrees)) {
    return(NA)  # Return NA if the input is NA
  }

  directions <- c("N", "NE", "E", "SE", "S", "SW", "W", "NW", "N")
  breaks <- c(0, 22.5, 67.5, 112.5, 157.5, 202.5, 247.5, 292.5, 337.5, 360)

  return(directions[findInterval(degrees, breaks, rightmost.closed = TRUE)])
}

# Apply function to the DD column
wind_mtp$cardDir <- sapply(wind_mtp$DD, convert_to_cardinal)
wind_mtp$cardDir <- as.character(wind_mtp$cardDir)  # Ensure it's character, not factor
wind_mtp$cardDir[is.na(wind_mtp$DD)] <- NA
# Check if NA values are properly handled
summary(wind_mtp)

head(wind_mtp$cardDir)

get_mode_dir <- function(x) {
  x <- na.omit(x)  # Remove NA values
  uniq_values <- unique(x)  # Get unique values
  freq_table <- tabulate(match(x, uniq_values))  # Count occurrences
  mode_value <- uniq_values[which.max(freq_table)]  # Most frequent value
  return(mode_value)
}

get_mode_excess_dir <- function(x, episode, quantile, s0) {
  x_excesses <- x[episode[,s0] > quantile]
  x_excesses <- na.omit(x_excesses)
  uniq_values <- unique(x_excesses)  # Get unique values
  freq_table <- tabulate(match(x_excesses, uniq_values))  # Count occurrences
  mode_value <- uniq_values[which.max(freq_table)]  # Most frequent value
  return(mode_value)
}

get_mean_excess_dir <- function(x, episode, quantile, s0) {
  # get the mean for all excess above quantile
  x_excesses <- x[episode[,s0] > quantile]
  x_excesses <- na.omit(x_excesses)
  mean_dir <- mean(x_excesses)
  return(mean_dir)
}

# Function to compute the mean wind speed and direction for each episode
compute_wind_episode <- function(episode, delta, quantile) {
  timestamps <- as.POSIXct(rownames(episode),
                            format = "%Y-%m-%d %H:%M:%S", tz = "UTC")

  # get column name of the max value
  max_value <- max(episode, na.rm = TRUE)
  s0 <- names(episode)[which(sapply(episode, 
                      function(col) max(col, na.rm = TRUE)) == max_value)]

  # Subset the wind data for the episode
  wind_subset <- wind_mtp %>%
    filter(datetime %in% timestamps)

  # Compute the mean wind speed and direction for the episode if there is data
  if (nrow(wind_subset) > 0) {
    FF <- wind_subset$FF[1 + delta]
    cardDir <- get_mode_dir(wind_subset$cardDir)
    DD <- mean(wind_subset$DD[wind_subset$cardDir == cardDir])
    cardDir_excess <- get_mode_excess_dir(wind_subset$cardDir, episode,
                                                              quantile, s0)
    DD_excess <- get_mean_excess_dir(wind_subset$DD, episode, quantile, s0)
    DD_t0 <- wind_subset$DD[1 + delta]
  } else {
    FF <- NA
    DD <- NA
    cardDir <- NA
    cardDir_excess <- NA
    DD_excess <- NA
    DD_t0 <- NA
  }

  return(data.frame(FF = FF, DD = DD, cardDir = cardDir,
                    cardDir_excess = cardDir_excess,
                    DD_excess = DD_excess, DD_t0 = DD_t0))
}

wind_per_episode <- lapply(list_episodes, compute_wind_episode, delta = delta,
                          quantile = threshold)

wind_ep_df <- do.call(rbind, wind_per_episode)
head(wind_ep_df)

wind_ep_df$cardDirt0 <- sapply(wind_ep_df$DD_t0, convert_to_cardinal)

# Load necessary libraries
library(ggplot2)
library(dplyr)

# Ensure cardDir is a factor with correct order
wind_ep_df$cardDir <- factor(wind_ep_df$cardDir, 
                             levels = c("N", "NE", "E", "SE", "S", "SW", "W", "NW"))

# Plot wind rose
ggplot(wind_ep_df, aes(x = cardDir)) +
  geom_bar(color = "#797474", fill = btfgreen, alpha=0.5, width = 1) +  # Bar plot for wind direction
  coord_polar(start = 15*pi/8) +  # Convert to polar coordinates, starting at North
  labs(x = "Wind Direction", y = "Count", title = "") +
  theme_minimal() +  # Clean theme
  theme(axis.text.x = element_text(size = 12, face = "bold"))  # Improve readability


# save the wind data
filename <- paste(im_folder, "wind/datagouv/wind_card_dir_per_episode.png", sep = "")
ggsave(filename, width = 30, height = 15, units = "cm")

# Ensure cardDir is a factor with correct order
wind_ep_df$cardDirt0 <- factor(wind_ep_df$cardDirt0, 
                             levels = c("N", "NE", "E", "SE", "S", "SW", "W", "NW"))

# Plot wind rose
ggplot(wind_ep_df, aes(x = cardDirt0)) +
  geom_bar(color = "#797474", fill = btfgreen, alpha=0.5, width = 1) +  # Bar plot for wind direction
  coord_polar(start = 15*pi/8) +  # Convert to polar coordinates, starting at North
  labs(x = "Wind Direction in t0", y = "Count", title = "") +
  theme_minimal() +  # Clean theme
  theme(axis.text.x = element_text(size = 12, face = "bold"))  # Improve readability


# save the wind data
filename <- paste(im_folder, "wind/datagouv/wind_card_dir_t0_per_episode.png", sep = "")
ggsave(filename, width = 30, height = 15, units = "cm")

# Plot wind rose
ggplot(wind_ep_df, aes(x = DD_t0)) +
  geom_bar(color = "#797474", fill = btfgreen, alpha=0.5, width = 5) +  # Bar plot for wind direction
  coord_polar(start = 15.85*pi/8) +  # Convert to polar coordinates, starting at North
  labs(x = "Wind Direction in t0", y = "Count", title = "") +
  theme_minimal() +  # Clean theme
  theme(axis.text.x = element_text(size = 12, face = "bold"))  # Improve readability

# save the wind data
filename <- paste(im_folder, "wind/datagouv/wind_dir_t0_per_episode.png", sep = "")
ggsave(filename, width = 30, height = 15, units = "cm")

# Mean by excess in episode
wind_ep_df$cardDir_excess <- factor(wind_ep_df$cardDir_excess, 
                             levels = c("N", "NE", "E", "SE", "S", "SW", "W", "NW"))

# Plot wind rose
ggplot(wind_ep_df, aes(x = cardDir_excess)) +
  geom_bar(color = "#797474", fill = btfgreen, alpha=0.5, width = 1) +  # Bar plot for wind direction
  coord_polar(start = 15*pi/8) +  # Convert to polar coordinates, starting at North
  labs(x = "Most frequent wind direction in excess for s0", y = "Count", title = "") +
  theme_minimal() +  # Clean theme
  theme(axis.text.x = element_text(size = 12, face = "bold"))  # Improve readability

# save the wind data
filename <- paste(im_folder, "wind/datagouv/wind_card_dir_excess_per_episode.png", sep = "")
ggsave(filename, width = 30, height = 15, units = "cm")

wind_ep_df$DD_excess <- as.numeric(wind_ep_df$DD_excess)

# Plot wind rose
ggplot(wind_ep_df, aes(x = DD_excess)) +
  geom_bar(color = "#797474", fill = btfgreen, alpha=0.5, width = 5) +  # Bar plot for wind direction
  coord_polar(start = 15.85*pi/8) +  # Convert to polar coordinates, starting at North
  labs(x = "Mean wind direction in excess for s0", y = "Count", title = "") +
  theme_minimal() +  # Clean theme
  theme(axis.text.x = element_text(size = 12, face = "bold"))  # Improve readability

# save the wind data
filename <- paste(im_folder, "wind/datagouv/wind_dir_excess_per_episode.png", sep = "")
ggsave(filename, width = 30, height = 15, units = "cm")

# Plot wind forces FF
ggplot(wind_ep_df, aes(x = FF)) +
  geom_bar(aes(y = ..count..), fill = btfgreen, color = "black") +
  btf_theme +
  xlab("Wind speed (m/s)") +
  ylab("Count")

filename <- paste(im_folder, "wind/datagouv/wind_speed_per_episode.png", sep = "")
ggsave(filename, width = 30, height = 15, units = "cm")


# Plot count of cardinal directions for cardDir and DD_t0
library(tidyr)
library(dplyr)


# Create a new column that checks if cardDir matches DD_t0
wind_ep_df$Match <- wind_ep_df$cardDir == wind_ep_df$cardDir_excess

# Bar plot of matches vs mismatches
ggplot(wind_ep_df, aes(x = Match)) +
  geom_bar(aes(fill = Match), color = "#797474", alpha = 0.7, width = 0.5) +
  scale_fill_manual(values = c("TRUE" = btfgreen, "FALSE" = "tomato")) +
  labs(x = "Cardinal direction t0 == Most frequent cardinal direction", y = "Count", title = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, face = "bold"))

filename <- paste(im_folder, "wind/datagouv/wind_dir_match.png", sep = "")
ggsave(filename, width = 30, height = 15, units = "cm")
```

```{r, echo=FALSE}
wind_ep_df$vx <- wind_ep_df$FF * cos(wind_ep_df$DD_t0)
wind_ep_df$vy <- wind_ep_df$FF * sin(wind_ep_df$DD_t0)
head(wind_ep_df)
```

## Optim with wind

```{r, echo=FALSE}
# Fonction pour calculer la distance Haversine entre deux points (x1, y1) et (x2, y2)
haversine <- function(x1, y1, x2, y2) {
  R <- 6371  # Rayon de la Terre en kilomètres
  dx <- (x2 - x1) * pi / 180  # Différence en longitude (en radians)
  dy <- (y2 - y1) * pi / 180  # Différence en latitude (en radians)
  
  a <- sin(dy / 2)^2 + cos(y1 * pi / 180) * cos(y2 * pi / 180) * sin(dx / 2)^2
  c <- 2 * atan2(sqrt(a), sqrt(1 - a))
  
  distance <- R * c  # Distance en kilomètres
  return(distance)
}

theorical_chi <- function(params, df_lags, wind_vect = NA) {
  beta1 <- params[1]
  beta2 <- params[2]
  alpha1 <- params[3]
  alpha2 <- params[4]
  if (all(is.na(wind_vect))) {
    adv <- params[5:6]
  } else {
    eta1 <- params[5]
    eta2 <- params[6]
    # adv <- exp(eta1 + eta2 * wind_vect)
    adv <- (abs(wind_vect)^eta1) * sign(wind_vect) * eta2
  }

  chi_df <- df_lags[c("s1", "s2", "tau")]
  # Get vario and chi for each lagtemp in meters
  chi_df$s1xv <- df_lags$s1x * 111320
  chi_df$s1yv <- df_lags$s1y * 111320 * cos(df_lags$s1x * pi / 180)
  chi_df$s2xv <- df_lags$s2x  * 111320 + adv[1] * df_lags$tau
  chi_df$s2yv <- df_lags$s2y  * 111320 * cos(df_lags$s1x * pi / 180) 
                                                          adv[2] * df_lags$tau
  chi_df$hnormV <- sqrt((chi_df$s2xv - chi_df$s1xv)^2 +
                        (chi_df$s2yv - chi_df$s1yv)^2)
  # chi_df$hnormV <- haversine(chi_df$s1xv, chi_df$s1yv,
  #                                chi_df$s2xv, chi_df$s2yv)
  # chi_df$hx <- df_lags$hx - adv[1] * df_lags$tau
  # chi_df$hy <- df_lags$hy - adv[2] * df_lags$tau
  # chi_df$hnormV <- sqrt(chi_df$hx^2 + chi_df$hy^2)
  # chi_df$hnorm <- norm_Lp(chi_df$hy, chi_df$hx, p = alpha1)

  chi_df$vario <- (2 * beta1) * chi_df$hnormV^alpha1 +
                  (2 * beta2) * abs(chi_df$tau)^alpha2

  chi_df$chi <- 2 * (1 - pnorm(sqrt(0.5 * chi_df$vario)))
  return(chi_df)
}

neg_ll <- function(params, data, df_lags, quantile, excesses, wind_vect = NA,
                      hmax = NA,  s0 = NA, t0 = NA, threshold = FALSE) {
  if (length(params) == 4) {
    params <- c(params, 0, 0)
  } else if (length(params) != 6) {
    stop("The number of initial parameters must be 4 or 6.")
  }

  Tmax <- nrow(data) # number of total observations

  if (all(!is.na(s0))) { # if we have a conditioning location
    p <- 1 # sure excess for r-Pareto process in (s0,t0)
  } else {
    # number of marginal excesses
    nmarg <- get_marginal_excess(data, quantile, threshold)
    p <- nmarg / Tmax # probability of marginal excesses
  }

  # Bounds for the parameters
  lower.bound <- c(1e-08, 1e-08, 1e-08, 1e-08)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)
  if (length(params) == 6 && any(is.na(wind_vect))) {
    lower.bound <- c(lower.bound, -Inf, -Inf)
    upper.bound <- c(upper.bound, Inf, Inf)
  } else if (length(params) == 6 && all(!is.na(wind_vect))) {
    lower.bound <- c(lower.bound, 1e-08, 1e-08)
    upper.bound <- c(upper.bound, Inf, Inf)
  }

  # Check if the parameters are in the bounds
  if (any(params < lower.bound) || any(params > upper.bound)) {
    return(1e50)
  }

  chi <- theorical_chi(params, df_lags, wind_vect) # get chi matrix
  ll_df <- df_lags # copy the dataframe
  ll_df$kij <- excesses$kij # number of excesses
  ll_df$Tobs <- excesses$Tobs
  ll_df$hnormV <- chi$hnormV
  ll_df$chi <- chi$chi
  ll_df$chi <- ifelse(ll_df$chi <= 0, 1e-10, ll_df$chi)
  ll_df$pchi <- 1 - p * ll_df$chi
  ll_df$pchi <- ifelse(ll_df$pchi <= 0, 1e-10, ll_df$pchi)

  # number of non-excesses
  ll_df$non_excesses <- ll_df$Tobs - ll_df$kij
  ll_df$ll <- ll_df$kij * log(ll_df$chi) +
              ll_df$non_excesses * log(ll_df$pchi)
  if (!is.na(hmax)) {
    ll_df <- ll_df[ll_df$hnorm <= hmax, ]
  }

  nll <- -sum(ll_df$ll, na.rm = TRUE)
  return(nll)
}

neg_ll_composite_rpar <- function(params, list_episodes, list_lags, quantile,
                    list_excesses, wind_df, hmax = NA, s0_list = NA,
                    t0_list = NA, threshold = FALSE) {
  if (any(is.na(wind_df))) {
    wind_vect <- NA
  }
  m <- length(list_excesses) # number of r-pareto processes
  nll_composite <- 0 # composite negative log-likelihood
  for (i in 1:m) {
    # extract lags and excesses from i-th r-pareto process from data
    df_lags <- list_lags[[i]]
    excesses <- list_excesses[[i]]
    episode <- list_episodes[[i]]
    if (all(!is.na(wind_df))) {
      wind_vx <- wind_df$vx[i]
      wind_vy <- wind_df$vy[i]
      wind_vect <- c(wind_vx, wind_vy)
    }
    s0 <- s0_list[i]
    t0 <- t0_list[i]
    nll_i <- neg_ll(params, episode, df_lags, quantile, wind_vect = wind_vect,
                    hmax = hmax, excesses = excesses, s0 = s0, t0 = t0,
                    threshold = threshold)
    nll_composite <- nll_composite + nll_i
  }
  return(nll_composite)
}

```

```{r, echo=FALSE}
init_param <- c(beta1, beta2, alpha1, alpha2, 1, 1)
# q <- 1
result <- optim(par = init_param, fn = neg_ll_composite_rpar,
        list_episodes = list_episodes, quantile = threshold,
        list_lags = list_lags,
        list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
        wind_df = wind_ep_df,
        t0_list = t0_list, threshold = TRUE,
        method = "L-BFGS-B",
        lower = c(1e-08, 1e-08, 1e-08, 1e-08, 1e-08, 1e-08),
        upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
        control = list(maxit = 10000),
        hessian = F)
estimated_params_noadv <- result$par

# Check the convergence
if (result$convergence != 0) {
  warning("The optimization did not converge.")
}

# Extract the results
df_result_noadv <- data.frame(beta1 =  result$par[1],
                        beta2 = result$par[2],
                        alpha1 = result$par[3],
                        alpha2 = result$par[4],
                        adv1 = result$par[5],
                        adv2 = result$par[6])

colnames(df_result_noadv) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1",
                                "adv2")

kable(df_result_noadv, format = "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",
  "responsive"), latex_options = "H")
```