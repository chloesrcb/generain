---
title: "Model on Montpellier rainfall"
author: " "
date: "`r Sys.Date()`" 
output:
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 5,
                      fig.align = 'center', message = FALSE, warning = FALSE,
                      fig.pos='H')
par(cex.main = 0.8,
    cex.lab = 0.7,
    cex.axis = 0.6)
```


```{r lib, echo=FALSE}
# Load libraries and set theme
# setwd("./script")
library(generain)
library(reshape2)
library(ggplot2)
source("load_libraries.R")
library(kableExtra)
library(extRemes)
library(bbmle)
library(ismev)
library(extRemes)
library(evd)
library(latex2exp)
library(geosphere)

btf_theme <- theme_minimal() +
  theme(axis.text.x = element_text(size =  6, angle = 0),
        axis.text.y = element_text(size = 6),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 6),
        title = element_text(size = 10),
        axis.line = element_blank(),  # Remove axis lines
        panel.border = element_blank(),  # Remove plot border
        panel.background = element_rect(fill = "transparent", color = NA),
        # Remove plot background
        axis.text = element_blank(),  # Remove axis text
        axis.ticks = element_blank(),  # Remove axis ticks
        plot.background = element_rect(fill = "transparent", color = NA),
        panel.grid = element_line(color = "#5c595943"))

# my green color
btfgreen <- "#69b3a2"
```


```{r, echo=FALSE}
################################################################################
# LOCATION ---------------------------------------------------------------------
################################################################################
# get location of each rain gauge
# setwd("./script")
location_gauges <- read.csv("./data/PluvioMontpellier_1min/pluvio_mtp_loc.csv")
location_gauges$codestation <- c("iem", "mse", "poly", "um", "cefe", "cnrs",
                                 "crbm", "archiw", "archie", "um35", "chu1",
                                 "chu2", "chu3", "chu4", "chu5", "chu6", "chu7")

# Get distances matrix
dist_mat <- get_dist_mat(location_gauges)
df_dist <- reshape_distances(dist_mat)

################################################################################
# DATA -------------------------------------------------------------------------
################################################################################
# get rain measurements
# load data
load("./data/PluvioMontpellier_1min/rain_mtp_5min_2019_2022.RData")
rain <- rain.all5[c(1, 6:ncol(rain.all5))]
```


```{r, echo=FALSE}
# put dates as index 
rownames(rain) <- rain$dates
# Remove the Time column to focus on site data
rain <- rain[, -1]

# Remove rows where all values are NA
rain <- rain[rowSums(is.na(rain)) != ncol(rain), ]
# head(rain)

dist_matrix <- distm(
location_gauges[, c("Longitude", "Latitude")],
fun = distHaversine
)

colnames(dist_matrix) <- location_gauges$codestation
rownames(dist_matrix) <- location_gauges$codestation

dist_df <- as.data.frame(as.table(dist_matrix))
names(dist_df) <- c("Site1", "Site2", "Distance")

dist_df <- dist_df[as.character(dist_df$Site1) != as.character(dist_df$Site2), ]

autocorr_pairs <- data.frame(
Site1 = location_gauges$codestation,
Site2 = location_gauges$codestation,
Distance = 0
)


# Combine the original distance dataframe with autocorr_pairs
dist_df <- rbind(dist_df, autocorr_pairs)

# Define breaks with 0 explicitly included at the beginning
breaks <- quantile(dist_df$Distance,
                     probs = seq(0, 1, length.out = 12))

dist_df$Class <- cut(dist_df$Distance, breaks = breaks,
                       labels = 0:10, include.lowest = TRUE)

# from breaks get intervals for each class
class_intervals <- data.frame(
  Class = 0:10,
  Interval = paste0("[", round(breaks[-length(breaks)],0), ", ", round(breaks[-1],0), "]")
)

# Print the resulting class intervals
print(class_intervals)


# Calculate the frequency of distances in each class
class_frequency <- table(dist_df$Class)

# Create a data frame with class intervals, frequency, and the width for each class
class_interval_summary <- data.frame(
  Class = as.integer(names(class_frequency)),
  Frequency = as.integer(class_frequency),
  IntervalStart = breaks[-length(breaks)],
  IntervalEnd = breaks[-1],
  IntervalWidth = breaks[-1] - breaks[-length(breaks)]
)


# Now create the data for geom_rect
df_for_rect <- data.frame(
  xmin = class_interval_summary$IntervalStart,  # Starting position (x-axis)
  xmax = class_interval_summary$IntervalEnd,    # Ending position (x-axis)
  ymin = 0,                                     # Bottom of the bars (y-axis)
  ymax = class_interval_summary$Frequency,       # Height of the bars (y-axis)
  Class = class_interval_summary$Class,          # Class for coloring
  IntervalWidth = class_interval_summary$IntervalWidth  # For sizing
)

# Plot the histogram using geom_rect for variable bar widths
ggplot(df_for_rect) +
  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), colour = "#f3eeee", fill=btfgreen, alpha=0.5) +
  theme_minimal() +
  labs(
    x = "Distance Class Interval",
    y = "Frequency (number of pairs)"
  ) +
  scale_x_continuous(breaks = breaks, labels = round(breaks, 0)) +  # Custom x-axis breaks
  btf_theme

# save plot
ggsave("./images/histogram_distance_class_intervals.png", width = 6, height = 4)

# Create matrix of radius
radius_intervals <- unique(breaks)
radius <- as.integer(radius_intervals)
rad_mat <- dist_mat
# Loop through radius and set distances in matrix
for (i in 2:length(radius)) {
  curr_radius <- radius[i]
  prev_radius <- radius[i - 1]
  rad_mat[dist_mat >= prev_radius & dist_mat < curr_radius] <- curr_radius
  # rad_mat[dist_mat > curr_radius] <- Inf
}

rad_mat[dist_mat == 0] <- 0
# Make a triangle
rad_mat[lower.tri(rad_mat)] <- NA
# in km
rad_mat <- rad_mat * 1000 # change les resultats...

```



```{r, fig.width = 4, fig.height = 3, echo=FALSE, fig.cap="Histogram of distance classes"}
# # spatial structure with an almost constant amount of pairs in each intervals
df_dist_order <- df_dist[order(df_dist$value), ]
num_intervals <- 15
quantiles_rad <- quantile(df_dist_order$value,
                            probs = seq(0, 1, length.out = num_intervals + 1))
radius_intervals <- unique(quantiles_rad)
radius <- as.integer(radius_intervals)
radius[length(radius)] <- 1550
dist_counts <- table(cut(df_dist$value, breaks = radius))

# Get dataframe for the histogram plot
df_hist <- data.frame(dist_counts)

colnames(df_hist) <- c("Interval", "Count")

# df_hist$Breaks <- gsub("e\\+0.", "0", df_hist$Interval)
# df_hist$Breaks <- gsub("\\.", "", df_hist$Breaks)

# # # Histogram
# histradius <- ggplot(df_hist, aes(x = Interval, y = Count)) +
#   btf_theme +
#   geom_bar(stat = "identity", fill = btfgreen, alpha = 0.8) +
#   xlab("Spatial lag") +
#   ylab("Pair count") +
#   theme(axis.text.x = element_text(angle = 45)) +
#   scale_x_discrete(labels = df_hist$Breaks) +
#   scale_y_continuous(breaks = c(0, 4, 6, 8, 10, 12))

# histradius

# Create matrix of radius
rad_mat <- dist_mat
# Loop through radius and set distances in matrix
for (i in 2:length(radius)) {
  curr_radius <- radius[i]
  prev_radius <- radius[i - 1]
  rad_mat[dist_mat >= prev_radius & dist_mat < curr_radius] <- curr_radius
  rad_mat[dist_mat > curr_radius] <- Inf
}

rad_mat[dist_mat == 0] <- 0
# Make a triangle
rad_mat[lower.tri(rad_mat)] <- NA
# in km
rad_mat <- rad_mat / 1000 # change les resultats...
```

<!-- 
We choose a quantile $q = 0.96$ without zeros for the spatial and temporal chi 
estimation. We get an estimate of the spatial and temporal variogram parameters using the
WLSE method from Buhl et al. (2019). -->

```{r, echo=FALSE, show=FALSE}
# get the empirical chi
rain_nozeros <- rain[rowSums(rain) > 0, ]
# reindex
rownames(rain_nozeros) <- 1:nrow(rain_nozeros)

# Spatial chi
q <- 0.998
chispa <- spatial_chi(rad_mat, rain, quantile = q)
spa_estim <- get_estimate_variospa(chispa, weights = "exp", summary = F)

tmax <- 10
q <- 0.998
chitemp <- temporal_chi(rain, tmax = tmax, quantile = q)
temp_estim <- get_estimate_variotemp(chitemp, tmax, npoints = ncol(rain_nozeros),
                                      weights = "exp", summary = F)

df_result <- data.frame(beta1 =  spa_estim[1],
                        beta2 = temp_estim[1],
                        alpha1 = spa_estim[2],
                        alpha2 = temp_estim[2])

# colnames(df_result) <- c("beta1", "alpha1", "beta2", "alpha2")

# kable(df_result, format = "latex") %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed",
#   "responsive"), latex_options = "H")
```

# COMEPHORE data

An other dataset is considered, the COMEPHORE radar renalysis data from Météo France.
We consider 59 pixels in the Montpellier area.

```{r, echo=FALSE}
# setwd("./script")
# load data
df_comephore <- read.csv("./data/comephore/inside_mtp.csv", sep = ",")
loc_px <- read.csv("./data/comephore/loc_pixels_mtp.csv", sep = ",")
# put date in index
rownames(df_comephore) <- df_comephore$date
# ncol(df_comephore) # nb of pixels + dates
comephore <- df_comephore[-1] # remove dates column
# Get distances matrix
dist_mat <- get_dist_mat(loc_px)
df_dist <- reshape_distances(dist_mat)
```

# Quantile choice

```{r}
# Remove lines with just zeros
comephore_nozeros <- comephore[rowSums(comephore) > 0, ]

# Choose two site and remove zeros
comephore_pair <- comephore_nozeros[, c(1,15)]
comephore_pair <- comephore_pair[rowSums(comephore_pair) > 0, ]
chiplot(comephore_pair, xlim = c(0.8, 1), ylim1 = c(0.5, 1), which = 1,
        qlim = c(0.8, 0.999))
abline(v = 0.95, col = "red", lty = 2)

# count conjoint excesses
q <- 0.95
# uniformize the data
n <- nrow(comephore_pair)
data_unif <- cbind(rank(comephore_pair[, 1]) / (n + 1),
                          rank(comephore_pair[, 2]) / (n + 1))

count_excesses <- sum(data_unif[, 1] > q & data_unif[, 2] > q)
print(count_excesses)

# With all zeros
comephore_pair <- comephore[, c(1,10)]
chiplot(comephore_pair, xlim = c(0.98, 1), ylim1 = c(0.5, 1), which = 1,
        qlim = c(0.98, 0.9995))

threshold <- quantile(comephore$p102, probs = 0.998, na.rm = T)
# get the quantile from threshold in the data without 0 when the quantile is
# 0.998 with zeros inside data
empirical_cdf <- ecdf(comephore_pair$p102)
quantile_in_nozeros <- empirical_cdf(threshold)
print(quantile_in_nozeros)

# Temporal chi
rain_nolag <- comephore_nozeros$p142[1:(length(comephore_nozeros$p142) - 5)]
rain_lag <- comephore_nozeros$p142[6:length(comephore_nozeros$p142)]
comephore_pair <- cbind(rain_nolag, rain_lag)
comephore_pair <- comephore_pair[rowSums(comephore_pair) > 0, ]
chiplot(comephore_pair, xlim = c(0.9, 1), ylim1 = c(0, 1), which = 1,
        qlim = c(0.9, 0.995))
abline(v = 0.95, col = "red", lty = 2)

n <- nrow(comephore_pair)
data_unif <- cbind(rank(comephore_pair[, 1]) / (n + 1),
                          rank(comephore_pair[, 2]) / (n + 1))
q <- 0.95
count_excesses <- sum(data_unif[, 1] > q & data_unif[, 2] > q)
print(count_excesses)

# We choose q = 0.95
q <- 0.95
```

# Empirical chi and WLSE

```{r, echo=FALSE}
tmax <- 10
nsites <- length(loc_px$pixel_name)
# Temporal chi with spatial lag fixed at 0
# compute chiplot of every site with itself but lagged in time
start_time <- Sys.time()
chimat_dtlag <- temporal_chi(comephore_nozeros, quantile = q, tmax = tmax,
                             mean = FALSE)
end_time <- Sys.time()
elapsed_time <- end_time - start_time
# print(elapsed_time)
```

## Temporal chi

```{r, echo=FALSE}
par(mfrow = c(1, 1))
chi_df_dt <- data.frame(chimat_dtlag)
colnames(chi_df_dt) <- c(1:tmax) # temporal lags from 1 to tmax
rownames(chi_df_dt) <- c(1:nsites) # stations

# remove 0
q <- 0.95
chimat_dt_mean <- temporal_chi(comephore, tmax, quantile = q,
                               mean = TRUE)
# get h axis in minutes ie x5 minutes
df <- data.frame(lag = c(1:tmax), chi = chimat_dt_mean)
chitemp_plot <- ggplot(df, aes(x = lag, y = chi)) +
  geom_point(color = btfgreen) +
  btf_theme +
  xlab("Temporal lag") +
  ylab(TeX(r"($\hat{\chi}$)"))

wlse_temp <- get_estimate_variotemp(chimat_dt_mean, tmax, nsites,
                                    weights = "exp", summary = F)

# print(wlse_temp)
alpha2 <- wlse_temp[[2]]
beta2 <- wlse_temp[[1]]
c2 <- log(beta2)

dftemp <- data.frame(lag = log(df$lag), chi = eta(df$chi))

chitemp_eta_estim <- ggplot(dftemp, aes(x = lag, y = chi)) +
  geom_point(color = btfgreen) +
  btf_theme +
  xlab(TeX(r"($\log(\tau)$)")) +
  ylab(TeX(r"($\eta(\widehat{\chi}(0,\tau))$)")) +
  geom_line(aes(x = lag, y = alpha2 * lag + c2),
            alpha = 0.5, color = "darkred", linewidth = 0.5)
```

```{r, echo=FALSE, fig.height = 3, fig.width = 5, fig.cap="Empirical temporal extremogram"}
chitemp_plot
```

```{r, echo=FALSE, fig.height = 3, fig.width = 5, fig.cap="Empirical temporal extremogram with eta transformation and WLSE"}
chitemp_eta_estim
```

## Spatial chi

```{r, echo=FALSE}
# Spatial chi ------------------------------------------------------------------
df_dist$value <- ceiling(df_dist$value / 100) * 100  # / 1000 in km
df_dist_km <- df_dist
df_dist_km$value <- df_dist$value / 1000

q <- 0.95
chispa_df <- spatial_chi_alldist(df_dist_km, data_rain = comephore_nozeros,
                                 quantile = q, hmax = 7)

etachispa_df <- data.frame(chi = eta(chispa_df$chi),
                           lagspa = log(chispa_df$lagspa))

chispa_plot <- ggplot(chispa_df, aes(lagspa, chi)) +
  btf_theme +
  geom_point(col = btfgreen) +
  xlab(TeX(r"($h$)")) +
  ylab(TeX(r"($\widehat{\chi}(h, 0)$)")) +
  ylim(0, 1)

# WLSE
wlse_spa <- get_estimate_variospa(chispa_df, weights = "exp", summary = T)

alpha1 <- wlse_spa[[2]]
beta1 <- wlse_spa[[1]]
c1 <- log(beta1)

chispa_eta_estim <- ggplot(etachispa_df, aes(lagspa, chi)) +
  btf_theme +
  geom_point(col = btfgreen) +
  xlab(TeX(r"($\log(h)$)")) +
  ylab(TeX(r"($\eta(\widehat{\chi}(h, 0))$)")) +
  geom_line(aes(x = lagspa, y = alpha1 * lagspa + c1), alpha = 0.6,
            color = "darkred", linewidth = 0.5)
```

```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Empirical spatial extremogram"}
chispa_plot
```

```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Empirical spatial extremogram with eta transformation and WLSE"}
chispa_eta_estim
```

Results of the WLSE method on the COMEPHORE data are:

```{r, echo=FALSE}
df_result <- data.frame(beta1 =  beta1,
                        beta2 = beta2,
                        alpha1 = alpha1,
                        alpha2 = alpha2)

colnames(df_result) <- c("beta1", "beta2", "alpha1", "alpha2")

kable(df_result, format = "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",
  "responsive"), latex_options = "H")
```

# Optimization of the composite likelihood

## Choose conditional points

```{r, echo=FALSE}
# Function to choose conditional points
# choose_conditional_points <- function(sites_coords, data, quantile,
#                                       min_spatial_dist, min_time_dist) {
#   # Ensure the quantile is within range
#   if (quantile < 0 || quantile > 1) {
#     stop("Quantile must be between 0 and 1.")
#   }

#   # Uniformize the data
#   data_unif <- data
#   for (i in 1:ncol(data)) {
#     data_unif[, i] <- rank(data[, i]) / (nrow(data) + 1)
#   }

#   # Identify valid (site, time) where X_s,t > quantile
#   valid_indices <- which(as.matrix(data_unif) > quantile, arr.ind = TRUE)

#   if (nrow(valid_indices) == 0) {
#     stop("No points exceed the given quantile for any time step.")
#   }

#   # Initialize the selected points
#   selected_points <- list()

#   # Variable to store the last selected point
#   last_selected <- NULL

#   # Iterate over valid indices to select points
#   for (i in seq_len(nrow(valid_indices))) {
#     current_index <- valid_indices[i, ] # Current index (time, site)
#     current_point <- list(
#       s0 = sites_coords[current_index[2], ],
#       t0 = as.numeric(current_index[1])
#     )

#     if (is.null(last_selected)) {
#       selected_points <- append(selected_points, list(current_point))
#       last_selected <- current_point # Update last selected point
#     } else {
#       # Check if this point is valid given previously selected points
#       # and the minimum spatial and time distances
#       dist_spa <- distHaversine(current_point$s0, last_selected$s0) / 1000
#       dist_time <- abs(current_point$t0 - last_selected$t0)
#       if (dist_spa >= min_spatial_dist || dist_time >= min_time_dist) {
#         selected_points <- append(selected_points, list(current_point))
#         last_selected <- current_point  # Update last selected point
#       }
#     }
#   }

#   if (length(selected_points) == 0) {
#     stop("No spatio-temporal points satisfy the quantile, the minimum spatial 
#           and/or time distances.")
#   }

#   return(selected_points)
# }



select_extreme_episodes <- function(sites_coords, data, quantile,
                                    min_spatial_dist, min_time_dist,
                                    delta, n_max_episodes, beta = 0) {
  # get maximal value
  max_value <- max(na.omit(data))
  max_indices <- which(data == max_value, arr.ind = TRUE)
  data_temp <- data # copy of the data

  # initialize the selected points list
  selected_points <- data.frame(s0 = character(), t0 = integer())

  i <- 0
  nb_episode <- 0 # number of episodes (selected points)
  while (nb_episode < n_max_episodes && max_value > quantile) {
    best_candidate <- NULL

    for (idx in 1:nrow(max_indices)) {
      s0_candidate <- colnames(data)[max_indices[idx, 2]]
      t0_candidate <- max_indices[idx, 1]

      if (nrow(selected_points) == 0) { # first episode
        best_candidate <- data.frame(s0 = s0_candidate, t0 = t0_candidate)
        nb_episode <- nb_episode + 1
        break
      } else {
        # get distances and time differences within the selected points
        distances <- distHaversine(sites_coords[selected_points$s0, ],
                                   sites_coords[s0_candidate, ]) / 1000
        time_differences <- abs(selected_points$t0 - t0_candidate)

        # check if the candidate is valid given the minimum spatial and
        # time distances
        if (all(distances >= min_spatial_dist) ||
                                      all(time_differences >= min_time_dist)) {
          best_candidate <- data.frame(s0 = s0_candidate, t0 = t0_candidate)
          nb_episode <- nb_episode + 1
          break # stop the loop
        } else { # not valid
          # "remove" the candidate from the data to avoid selecting it again
          data_temp[t0_candidate, s0_candidate] <- -Inf
        }
      }
    }

    # add the best candidate to the selected points
    selected_points <- rbind(selected_points, best_candidate)

    # "remove" the episode from the data
    t_inf <- best_candidate$t0 - (delta - 1) - beta
    t_sup <- best_candidate$t0 + (delta - 1) + beta
    data_temp[t_inf:t_sup, best_candidate$s0] <- -Inf

    # get the new maximal value
    max_value <- max(na.omit(data_temp))
    max_indices <- which(data_temp == max_value, arr.ind = TRUE)
    i <- i + 1
  }
  return(selected_points)
}

get_extreme_episodes <- function(selected_points, data, delta, beta = 0) {
  # initialize the list of episodes
  episodes <- list()
  for (i in 1:nrow(selected_points)) {
    # s0 <- selected_points$s0[i]
    t0 <- selected_points$t0[i]
    t_inf <- t0 - (delta - 1) - beta
    t_sup <- t0 + (delta - 1) + beta
    episode <- data[t_inf:t_sup, ] # get the episode
    episodes <- append(episodes, list(episode))
  }
  return(episodes)
}

```

```{r, echo=FALSE}
library(geosphere)
min_spatial_dist <- 2  # in km
min_time_dist <- 5  # in hours
delta <- 15 # step for the episode before and after the max value

# quantile_non_standardise <- quantile(comephore$p142, 0.98)
# quantile_standardise <- quantile(rank(comephore$p142) / (nrow(comephore) + 1), 0.98)
# quantile_standardise <- quantile(data_standardise$p142, 0.98)
# quantile <- quantile_standardise
# data_standardise <- as.data.frame(scale(comephore))
# standardize with rank
# data_standardise <- rank[data_standardise] / (nrow(data_standardise) + 1)

# Get the selected episodes
selected_points <- select_extreme_episodes(sites_coords, comephore, 1,
                                           min_spatial_dist, min_time_dist,
                                           delta = 15, n_max_episodes = 1000)
list_episodes <- get_extreme_episodes(selected_points, comephore,  delta = 15)

s0_list <- selected_points$s0
t0_list <- selected_points$t0
```

```{r, echo=TRUE}
# Get coords
sites_coords <- loc_px[, c("Longitude", "Latitude")]
rownames(sites_coords) <- loc_px$pixel_name

# # remove 0
# rain_new <- comephore_nozeros
# quantile <- 0.96
# min_spatial_dist <- 4 # in km
# min_time_dist <- 24*30 # in hours

# selected_points <- choose_conditional_points(
#   sites_coords = sites_coords,
#   data = rain_new,
#   quantile = quantile,
#   min_spatial_dist = min_spatial_dist,
#   min_time_dist = min_time_dist
# )

# # Extract s0 and t0
# s0_list <- lapply(selected_points, `[[`, "s0")
# s_unique <- unique(s0_list)
# t0_list <- lapply(selected_points, `[[`, "t0")

# length(s0_list)
# length(s_unique)

```

We have `r length(s0_list)` conditional points. We compute the lags and excesses for each of them.

```{r, echo=FALSE}
library(parallel)
quantile <- 1
list_results <- mclapply(1:length(s0), function(i) {
  s0 <- s0_list[i]
  s0_coords <- sites_coords[s0, ]
  t0 <- t0_list[i]
  episode <- selected_episodes[[i]]
  lags <- get_conditional_lag_vectors(sites_coords, s0_coords, t0, tau_max = 10,
                                      latlon = TRUE)
  lags$hx <- lags$hx / 1000  # in km
  lags$hy <- lags$hy / 1000  # in km
  lags$hnorm <- lags$hnorm / 1000  # in km
  excesses <- empirical_excesses(episode, quantile, lags, type = "rpareto")
  list(lags = lags, excesses = excesses)
}, mc.cores = detectCores() - 1)
```


## Optimization results

```{r, echo=FALSE}
neg_ll_composite_rpar <- function(params, list_episodes, list_lags, quantile,
                    list_excesses, hmax = NA, s0_list = NA,
                    t0_list = NA, threshold = FALSE) {
  # print(params)
  # # Bounds for the parameters
  # lower.bound <- c(1e-8, 1e-8, 1e-8, 1e-8)
  # upper.bound <- c(Inf, Inf, 1.999, 1.999)
  # if (length(params) == 6) {
  #   lower.bound <- c(lower.bound, -Inf, -Inf)
  #   upper.bound <- c(upper.bound, Inf, Inf)
  # }

  # # Check if the parameters are in the bounds
  # if (any(params < lower.bound) || any(params > upper.bound)) {
  #   return(1e50)
  # }

  m <- length(list_excesses) # number of r-pareto processes
  nll_composite <- 0 # composite negative log-likelihood
  for (i in 1:m) {
    # extract lags and excesses from i-th r-pareto process from data
    df_lags <- list_lags[[i]]
    excesses <- list_excesses[[i]]
    episode <- list_episodes[[i]]
    s0 <- s0_list[i]
    t0 <- t0_list[i]
    nll_i <- neg_ll(params, episode, df_lags, quantile, hmax = hmax,
                    excesses = excesses, s0 = s0, t0 = t0,
                    threshold = threshold)
    nll_composite <- nll_composite + nll_i
  }
  return(nll_composite)
}
```

We initialize the parameters with the values obtained from the WLSE method and without advection. 
It converges and the results of the optimization on the COMEPHORE data are:

```{r, echo=FALSE}
init_param <- c(0.01, 0.2, 1.5, 1, -0.1, -0.2)
q <- 1
result <- optim(par = init_param, fn = neg_ll_composite_rpar,
        list_episodes = list_episodes, quantile = q, list_lags = list_lags,
        list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
        t0_list = t0_list, threshold = TRUE,
        method = "L-BFGS-B",
        lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
        upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
        control = list(maxit = 10000),
        hessian = F)
estimated_params_noadv <- result$par
# Check the convergence
if (result$convergence != 0) {
  warning("The optimization did not converge.")
}

# Extract the results
df_result_noadv <- data.frame(beta1 =  result$par[1],
                        beta2 = result$par[2],
                        alpha1 = result$par[3],
                        alpha2 = result$par[4],
                        adv1 = result$par[5],
                        adv2 = result$par[6])

colnames(df_result_noadv) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

kable(df_result_noadv, format = "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",
  "responsive"), latex_options = "H")
```


Same but now we consider an advection of 0.1 for both directions. It converges and the results of 
the optimization on the COMEPHORE data are:

```{r, echo=FALSE}
init_param <- c(beta1, beta2, alpha1, alpha2, 0.2, 0.1)

result <- optim(par = init_param, fn = neg_ll_composite_rpar,
        data = rain_new, quantile = q, list_lags = list_lags,
        list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
        t0_list = t0_list, threshold = FALSE,
        method = "L-BFGS-B",
        lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
        upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
        control = list(maxit = 10000),
        hessian = F)
estimated_params <- result$par
# Check the convergence
if (result$convergence != 0) {
  warning("The optimization did not converge.")
}

# Extract the results
df_result <- data.frame(beta1 =  result$par[1],
                        beta2 = result$par[2],
                        alpha1 = result$par[3],
                        alpha2 = result$par[4],
                        adv1 = result$par[5],
                        adv2 = result$par[6])

colnames(df_result) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

kable(df_result, format = "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",
  "responsive"), latex_options = "H")
```


Same but now we consider an advection of $0.01$ for both directions. It converges and the results of 
the optimization on the COMEPHORE data are:

```{r, echo=FALSE}
init_param <- c(beta1, beta2, alpha1, alpha2, 0.01, 0.01)

result <- optim(par = init_param, fn = neg_ll_composite_rpar,
        data = rain_new, quantile = q, list_lags = list_lags,
        list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
        t0_list = t0_list, threshold = FALSE,
        method = "L-BFGS-B",
        lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
        upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
        control = list(maxit = 10000),
        hessian = F)
estimated_params2 <- result$par
# Check the convergence
if (result$convergence != 0) {
  warning("The optimization did not converge.")
}

# Extract the results
df_result2 <- data.frame(beta1 =  result$par[1],
                        beta2 = result$par[2],
                        alpha1 = result$par[3],
                        alpha2 = result$par[4],
                        adv1 = result$par[5],
                        adv2 = result$par[6])

colnames(df_result2) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

kable(df_result2, format = "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",
  "responsive"), latex_options = "H")
```


Same but now we consider an initial advection of $(0.2, 0.1)$. It converges and the results of 
the optimization on the COMEPHORE data are:

```{r, echo=FALSE}
init_param <- c(beta1, beta2, alpha1, alpha2, 0.2, 0.1)

result <- optim(par = init_param, fn = neg_ll_composite_rpar,
        data = rain_new, quantile = q, list_lags = list_lags,
        list_excesses = list_excesses, hmax = 7, s0_list = s0_list,
        t0_list = t0_list, threshold = FALSE,
        method = "L-BFGS-B",
        lower = c(1e-08, 1e-08, 1e-08, 1e-08, -Inf, -Inf),
        upper = c(Inf, Inf, 1.999, 1.999, Inf, Inf),
        control = list(maxit = 10000),
        hessian = F)
estimated_params2 <- result$par
# Check the convergence
if (result$convergence != 0) {
  warning("The optimization did not converge.")
}

# Extract the results
df_result2 <- data.frame(beta1 =  result$par[1],
                        beta2 = result$par[2],
                        alpha1 = result$par[3],
                        alpha2 = result$par[4],
                        adv1 = result$par[5],
                        adv2 = result$par[6])

colnames(df_result2) <- c("beta1", "beta2", "alpha1", "alpha2", "adv1", "adv2")

kable(df_result2, format = "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed",
  "responsive"), latex_options = "H")
```

```{r, echo=FALSE}
# # Get the hessian
# hessian <- result$hessian
# cov_matrix <- solve(hessian)

# eigenvalues <- eigen(hessian)$values
# if (any(eigenvalues <= 0)) {
#   stop("Hessian matrix is not positive definite.")
# }

# eigen_decomp <- eigen(hessian)
# eigenvalues <- eigen_decomp$values
# eigenvectors <- eigen_decomp$vectors

# tol <- 1e-6
# eigenvalues[eigenvalues <= tol] <- tol

# hessian_reg <- eigenvectors %*% diag(eigenvalues) %*% t(eigenvectors)

# cov_matrix <- solve(hessian_reg)
# se <- sqrt(diag(cov_matrix))
# conf_int <- cbind(result$par - 1.96 * se, result$par + 1.96 * se)
```

## Variogram

### Without initial advection

```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Variogram estimate with no initial advection in the optimization"}
# compute variogram with parameters
tau_values <- c(0)
result <- df_result_noadv
df_lags <- list_lags[[5]]
generate_variogram_plots(result, df_lags, estimated_params_noadv, tau_values, chi=F)

tau_values <- c(5)
result <- df_result_noadv
df_lags <- list_lags[[5]]
generate_variogram_plots(result, df_lags, estimated_params_noadv, tau_values, chi=F)

tau_values <- c(10)
result <- df_result_noadv
df_lags <- list_lags[[5]]
generate_variogram_plots(result, df_lags, estimated_params_noadv, tau_values, chi=F)
```


### With initial advection of 0.1

```{r, echo=FALSE, fig.width = 5, fig.height = 3, fig.cap="Variogram estimate with initial advection of 0.1 in the optimization"}
# compute variogram with parameters
tau_values <- c(0)
result <- df_result
df_lags <- list_lags[[5]]
generate_variogram_plots(result, df_lags, estimated_params, tau_values, chi=F)

tau_values <- c(5)
result <- df_result
df_lags <- list_lags[[5]]
generate_variogram_plots(result, df_lags, estimated_params, tau_values, chi=F)

tau_values <- c(10)
result <- df_result
df_lags <- list_lags[[5]]
generate_variogram_plots(result, df_lags, estimated_params, tau_values, chi=F)
```
