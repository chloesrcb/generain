---
title: "Debug negative log-likelihood"
author: " "
date: "`r Sys.Date()`" 
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 3.5,
                      fig.align = 'center', message = FALSE, warning = FALSE)
```


```{r lib, echo=FALSE}
# setwd("./script")
library(generain)
library(reshape2)
library(ggplot2)
source("load_libraries.R")
library(kableExtra)
library(extRemes)
library(bbmle)
```


```{r, echo=FALSE}
save_results_optim <- function(result, filename) {
  if (result$convergence == 0) {
    rmse <- sqrt((result$par - true_param)^2)
    df_rmse <- data.frame(estim = result$par, rmse = rmse)
    rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2", "Vx", "Vy")
    # save the results
    write.csv(t(df_rmse), file = paste0("../data/simulations_BR/results/",
                          filename, ".csv"))
  } else {
    print("No convergence")
  }
}

get_results_optim <- function(filename) {
  df_rmse <- read.csv(paste0("../data/simulations_BR/results/", filename, 
                      ".csv"))
  return(df_rmse)
}
```


```{r, echo=FALSE}
sim_BR <- function(beta1, beta2, alpha1, alpha2, x, y, z, n.BR) { 
  ## Setup 
  RandomFields::RFoptions(spConform=FALSE) 
  lx <- length(sx <- seq_along(x)) 
  ly <- length(sy <- seq_along(y)) 
  lz <- length(sz <- seq_along(z)) 
  ## Model-Variogram BuhlCklu 
  modelBuhlCklu <- RandomFields::RMfbm(alpha=alpha1, var=beta1, proj=1) + 
                  RandomFields::RMfbm(alpha=alpha1, var=beta1, proj=2) + 
                  RandomFields::RMfbm(alpha=alpha2, var=beta2, proj=3)
  
  ## Construct grid 
  Nxy <- lx * ly 
  N <- Nxy * lz 
  grid <- matrix(0, nrow=N, ncol=3) # (N,3)-matrix 

  for (i in sx) 
    for (j in seq_len(ly*lz)) 
      grid[i+(j-1)*ly, 1] <- i 
  
  for (i in sy) 
    for (j in sx) 
      for(k in sz) 
        grid[j+lx*(i-1)+(k-1)*Nxy, 2] <- i 
  
  for (i in sz) 
    for (j in seq_len(Nxy)) 
      grid[j+Nxy*(i-1), 3] <- i

  ## Construct shifted variogram
  Varm1 <- vapply(seq_len(N), function(n) 
      RandomFields::RFvariogram(modelBuhlCklu,
        x=sx-grid[n,1], 
        y=sy-grid[n,2], 
        z=sz-grid[n,3]), 
        array(NA_real_, dim=c(lx, ly, lz))) ## => (lx, ly, lz, N)-array

  ## Main 
  set.seed(123)
  Z <- array(, dim=c(lx, ly, lz, n.BR)) # 4d array 
  E <- matrix(rexp(n.BR * N), nrow=n.BR, ncol=N) 
  for (i in seq_len(n.BR)) { ## n=1 
    V <- 1/E[i,1] 
    W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z, n=1) 
    Y <- exp(W - W[1] - Varm1[,,,1]) 
    Z[,,,i] <- V * Y 
    ## n in {2,..,N} 
    for(n in 2:N) { 
      Exp <- E[i,n] 
      V <- 1/Exp 
      while(V > Z[N*(i-1)+n]) { 
        W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z) 
        Y <- exp(W - W[n] - Varm1[,,,n]) 
        if(all(V*Y[seq_len(n-1)] < Z[(N*(i-1)+1):(N*(i-1)+(n-1))])) 
          Z[,,,i] <- pmax(V*Y, Z[,,,i]) 
          Exp <- Exp + rexp(1) 
          V <- 1/Exp 
      } 
    } 
  } 
  ## Return 
  Z 
}

sim_BR_adv <- function(beta1, beta2, alpha1, alpha2, x, y, z, n.BR,
                       adv) {
  ## Setup
  RandomFields::RFoptions(spConform = FALSE)
  lx <- length(sx <- seq_along(x))
  ly <- length(sy <- seq_along(y))
  lz <- length(sz <- seq_along(z))

  ## Model-Variogram BuhlCklu
  modelBuhlCklu <- RandomFields::RMfbm(alpha = alpha1, var = beta1, proj = 1) +
                   RandomFields::RMfbm(alpha = alpha1, var = beta1, proj = 2) +
                   RandomFields::RMfbm(alpha = alpha2, var = beta2, proj = 3)

  ## Construct grid
  Nxy <- lx * ly
  N <- Nxy * lz
  grid <- matrix(0, nrow = N, ncol = 3) # (N,3)-matrix

  for (i in sx)
    for (j in seq_len(ly * lz))
      grid[i + (j - 1) * ly, 1] <- i

  for (i in sy)
    for (j in sx)
      for (k in sz)
        grid[j + lx * (i - 1) + (k - 1) * Nxy, 2] <- i

  for (i in sz)
    for (j in seq_len(Nxy))
      grid[j + Nxy * (i - 1), 3] <- i

  # Construct shifted grid with advected coordinates
  grid[, 1] <- grid[, 1] - grid[, 3] * adv[1]
  grid[, 2] <- grid[, 2] - grid[, 3] * adv[2]

  ## Construct shifted variogram
  Varm1 <- vapply(seq_len(N), function(n)
      RandomFields::RFvariogram(modelBuhlCklu,
        x = sx - grid[n, 1],
        y = sy - grid[n, 2],
        z = sz - grid[n, 3]),
        array(NA_real_, dim = c(lx, ly, lz))) ## => (lx, ly, lz, N)-array

  ## Main
  set.seed(123)
  Z <- array(, dim = c(lx, ly, lz, n.BR)) # 4d array
  E <- matrix(rexp(n.BR * N), nrow = n.BR, ncol = N)
  for (i in seq_len(n.BR)) { ## n=1 
    V <- 1 / E[i, 1]
    W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z, n = 1)
    Y <- exp(W - W[1] - Varm1[, , , 1])
    Z[, , , i] <- V * Y
    ## n in {2,..,N}
    for (n in 2:N) {
      Exp <- E[i, n]
      V <- 1 / Exp 
      while(V > Z[N * (i - 1) + n]) {
        W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z)
        Y <- exp(W - W[n] - Varm1[, , , n])
        if(all(V * Y[seq_len(n-1)] < Z[(N*(i-1)+1):(N*(i-1)+(n-1))]))
          Z[, , , i] <- pmax(V * Y, Z[, , , i])
          Exp <- Exp + rexp(1) 
          V <- 1 / Exp 
      }
    }
  }
  ## Return
  Z
}
```

# Simulation

```{r}
true_param <- c(0.4, 0.2, 1.5, 1)
ngrid <- 5
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:300

# load the simulations
file_path <- paste0("../data/simulations_BR/sim_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df <- read.csv(file_path)

nsites <- ncol(simu_df)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
```


```{r}
# get only two sites
site1 <- 1
site2 <- 25
simu_df_cp <- simu_df[, c(site1, site2)]
colnames(simu_df_cp) <- c("site1", "site2")

# get the distance between the two sites
sites_coords_cp <- sites_coords[c(site1, site2), ]
hnorm <- sqrt((sites_coords_cp[2, 1] - sites_coords_cp[1, 1])^2 +
              (sites_coords_cp[2, 2] - sites_coords_cp[1, 2])^2)

# get the lag vectors
df_lags <- get_lag_vectors(sites_coords_cp, true_param,
                          hmax = NA, tau_vect = 0:10)

print(df_lags)
```



# Set of lags

```{r}
get_lag_vectors <- function(df_coords, params, hmax = NA, tau_vect = 1:10) {

  # Advection vector
  adv <- if (length(params) == 6) params[5:6] else c(0, 0)

  n <- nrow(df_coords)
  tau_len <- length(tau_vect)

  # Create index combinations
  indices <- combn(n, 2)
  i_vals <- indices[1, ]
  j_vals <- indices[2, ]

  # Calculate lags
  lag_latitudes <- df_coords$Latitude[j_vals] - df_coords$Latitude[i_vals]
  lag_longitudes <- df_coords$Longitude[j_vals] - df_coords$Longitude[i_vals]

  # Calculate hnorm for all pairs
  hnorms <- sqrt(lag_latitudes^2 + lag_longitudes^2)

  # Filter based on hmax
  if (!is.na(hmax)) {
    valid_indices <- which(hnorms <= hmax)
    i_vals <- i_vals[valid_indices]
    j_vals <- j_vals[valid_indices]
    lag_latitudes <- lag_latitudes[valid_indices]
    lag_longitudes <- lag_longitudes[valid_indices]
    hnorms <- hnorms[valid_indices]
  }

  # Replicate for tau_vect
  num_pairs <- length(i_vals)
  i_vals <- rep(i_vals, each = tau_len)
  j_vals <- rep(j_vals, each = tau_len)
  lag_latitudes <- rep(lag_latitudes, each = tau_len)
  lag_longitudes <- rep(lag_longitudes, each = tau_len)
  hnorms <- rep(hnorms, each = tau_len)
  taus <- rep(tau_vect, times = num_pairs)

  # Apply advection
  if (all(adv != c(0, 0))) {
    lag_latitudes <- lag_latitudes - adv[1] * taus
    lag_longitudes <- lag_longitudes - adv[2] * taus
    hnorms <- sqrt(lag_latitudes^2 + lag_longitudes^2)
  }

  # Create final dataframe
  lags <- data.frame(
    s1 = i_vals,
    s2 = j_vals,
    h1 = lag_latitudes,
    h2 = lag_longitudes,
    tau = taus,
    hnorm = hnorms
  )

  return(lags)
}

```

```{r}
sites_coords <- generate_grid_coords(sqrt(nsites))
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = NA, tau_vect = 0:10)

print(head(df_lags))
```


# Optimisation

## Get excesses

```{r}
empirical_excesses <- function(data_rain, quantile, df_lags) {
  excesses <- df_lags # copy the dataframe
  unique_tau <- unique(df_lags$tau) # unique temporal lags

  for (t in unique_tau) { # loop over temporal lags
    df_h_t <- df_lags[df_lags$tau == t, ] # get the dataframe for each lag

    for (i in seq_len(nrow(df_h_t))) { # loop over each pair of sites
      # get the indices of the sites
      ind_s2 <- as.numeric(as.character(df_h_t$s2[i]))
      ind_s1 <- df_h_t$s1[i]

      # get the data for the pair of sites
      rain_cp <- data_rain[, c(ind_s1, ind_s2), drop = FALSE]
      rain_cp <- as.data.frame(na.omit(rain_cp))
      colnames(rain_cp) <- c("s1", "s2")

      Tmax <- nrow(rain_cp) # number of time steps
      rain_unif <- cbind(rank(rain_cp$s1) / (Tmax + 1),
                         rank(rain_cp$s2) / (Tmax + 1))
      marginal_excesses <- sum(rain_unif[, 2] > quantile) # number of excesses

      rain_nolag <- rain_cp$s1[1:(Tmax - t)] # get the data without lag
      rain_lag <- rain_cp$s2[(1 + t):Tmax] # get the data with lag

      Tobs <- length(rain_nolag) # number of observations T - tau
      # transform the data in uniform data
      rain_unif <- cbind(rank(rain_nolag) / (Tobs + 1), 
                         rank(rain_lag) / (Tobs + 1))
      # get the conditional excesses on s2
      cp_cond <- rain_unif[rain_unif[, 2] > quantile,]
      joint_excesses <- sum(cp_cond[, 1] > quantile) # number of excesses for s1
                                                   # given those of s2

      # store the number of excesses
      excesses$Tobs[excesses$s1 == ind_s1
                      & excesses$s2 == ind_s2
                      & excesses$tau == t] <- Tobs
      excesses$nj[excesses$s1 == ind_s1
                      & excesses$s2 == ind_s2
                      & excesses$tau == t] <- marginal_excesses
      excesses$kij[excesses$s1 == ind_s1
                    & excesses$s2 == ind_s2
                    & excesses$tau == t] <- joint_excesses
    }
  }
  return(excesses)
}

q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
print(head(excesses))

# density plot of the number of excesses
ggplot(excesses, aes(x = kij)) +
  geom_density() +
  labs(title = "Density plot of the number of excesses",
       x = "Number of excesses", y = "Density")
```


### Verif

```{r}
# For a couple (s1, s2)
s1 <- 1
s2 <- 2
tau <- 2
rain_cp <- simu_df[, c(s1, s2)]
rain_cp <- na.omit(rain_cp)
colnames(rain_cp) <- c("s1", "s2")
# quantile
q <- 0.9
rain_cp_q <- quantile(rain_cp$s2, probs = q)

Tmax <- nrow(rain_cp) # number of time steps == length(temp)

# get the number of marginal excesses
n_marg <- sum(rain_cp$s2 > rain_cp_q)
p_hat <- n_marg / Tmax # probability of marginal excesses

rain_lag <- rain_cp$s2[(1 + tau):Tmax] # get the data with lag
rain_nolag <- rain_cp$s1[1:(Tmax - tau)] # get the data without lag
# get the number of joint excesses
Tobs <- length(rain_nolag) # T - tau
rain_unif <- cbind(rank(rain_nolag) / (Tobs + 1), 
                         rank(rain_lag) / (Tobs + 1))
# get the conditional excesses on s2
cp_cond <- rain_unif[rain_unif[, 2] > q,]
joint_excesses <- sum(cp_cond[, 1] > q) # number of excesses for s1

print(paste("Number of marginal excesses: ", n_marg))

print(paste("Number of joint excesses: ", joint_excesses))
```

Avec la fonction `empirical_excesses` on retrouve bien les mêmes résultats:

```{r}
q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
excesses_s1_s2 <- excesses[excesses$s1 == 1 & excesses$s2 == 2, ]
print(excesses_s1_s2)
```



Pour un meme site s1 et s2, on doit avoir le meme nombre de dépassements
marginale et conjoint sans décalage temporel. Prenons:

```{r}
# For a couple (s1, s2)
s1 <- 1
s2 <- 1
tau <- 0
```

```{r, echo=FALSE}
# For a couple (s1, s2)
s1 <- 1
s2 <- 1
tau <- 0
rain_cp <- simu_df[, c(s1, s2)]
rain_cp <- na.omit(rain_cp)
colnames(rain_cp) <- c("s1", "s2")
# quantile
q <- 0.9
rain_cp_q <- quantile(rain_cp$s2, probs = q)

Tmax <- nrow(rain_cp) # number of time steps == length(temp)

# get the number of marginal excesses
n_marg <- sum(rain_cp$s2 > rain_cp_q)
p_hat <- n_marg / Tmax # probability of marginal excesses

rain_lag <- rain_cp$s2[(1 + tau):Tmax] # get the data with lag
rain_nolag <- rain_cp$s1[1:(Tmax - tau)] # get the data without lag
# get the number of joint excesses
Tobs <- length(rain_nolag) # T - tau
rain_unif <- cbind(rank(rain_nolag) / (Tobs + 1), 
                         rank(rain_lag) / (Tobs + 1))
# get the conditional excesses on s2
cp_cond <- rain_unif[rain_unif[, 2] > q,]
joint_excesses <- sum(cp_cond[, 1] > q) # number of excesses for s1

print(paste("Number of marginal excesses: ", n_marg))

print(paste("Number of joint excesses: ", joint_excesses))
```



## Theorical chi

```{r}
theorical_chi_ind <- function(params, h, tau) {
  # get variogram parameter
  beta1 <- params[1]
  beta2 <- params[2]
  alpha1 <- params[3]
  alpha2 <- params[4]

  # Get vario and chi for each lagtemp
  varioval <- 2 * (beta1 * h^alpha1 + beta2 * tau^alpha2)
  phi <- pnorm(sqrt(0.5 * varioval))
  chival <- 2 * (1 - phi)

  return(chival)
}


theorical_chi <- function(params, df_lags) {
  chi_df <- df_lags # copy the dataframe
  chi_df$chi <- theorical_chi_ind(params, df_lags$hnorm, df_lags$tau)
  return(chi_df)
}
```

```{r}
chi_theorical <- theorical_chi(true_param, df_lags)
print(tail(chi_theorical))

true_param <- c(0.4, 0.2, 1.5, 1)
# for one hnorm and one tau
hnorm <- 1
tau <- 3
semivar <- true_param[1]*hnorm^true_param[3] + true_param[2]*tau^true_param[4]
chi_h_t_verif <- 2 * (1 - pnorm(sqrt(semivar)))

chi_h_t <- chi_theorical$chi[chi_theorical$hnorm == hnorm &
                                   chi_theorical$tau == tau]

print(chi_h_t) # all the same proba: good
print(unique(chi_h_t) == chi_h_t_verif) # ok
```


```{r}
# Empirical spatio-temporal chi
chispatemp_dt <- function(data_rain, df_lags, quantile) {
  chi_st <- df_lags
  chi_st$chiemp <- NA
  # get mid values for each intervals for the plot
  h_vect <- unique(df_lags$hnorm)
  Tmax <- nrow(data_rain)
  for (t in unique(df_lags$tau)) {
    for (h in h_vect) {
      # station number inside h lag
      indices <- df_lags[df_lags$hnorm == h & df_lags$tau == t,]
      print(paste0("h = ", h))
      nb_pairs <- dim(indices)[1]
      if (nb_pairs == 0) {
        chi_st$chiemp[chi_st$hnorm == h & chi_st$tau == t] <- NA
      } else {
        # get index pairs
        ind_s1 <- indices$s1
        ind_s2 <- indices$s2
        chi_val_h <- c()
        for (i in 1:nb_pairs){
          print(i)
          rain_cp <- na.omit(data_rain[, c(ind_s1[i], ind_s2[i])])
          colnames(rain_cp) <- c("s1", "s2")
          rain_lag <- rain_cp$s1[(t + 1):Tmax]
          rain_nolag <- rain_cp$s2[1:(Tmax - t)]
          data <- cbind(rain_lag, rain_nolag) # get couple
          chi_val <- get_chiq(data, quantile)
          chi_st$chiemp[chi_st$s1 == ind_s1[i] & chi_st$s2 == ind_s2[i] &
                        chi_st$hnorm == h & chi_st$tau == t] <- chi_val
        }
      }
    }
  }
  return(chi_st)
}


chi_st <- chispatemp_dt(simu_df, df_lags, 0.9)
chi_st$chiemp <- ifelse(chi_st$chiemp <= 0, 0.000001, chi_st$chiemp)
print(head(chi_st))
chi_theorical <- theorical_chi(true_param, df_lags)
chi <- unique(chi_theorical$chi)
plot(chi)
print(head(chi_theorical))
```



## Log likelihood

```{r}
neg_ll <- function(params, simu, df_lags, locations, quantile,
                   latlon = FALSE, simu_exp = FALSE, excesses = NULL) {
  hmax <- max(df_lags$hnorm)
  tau <- unique(df_lags$tau)

  # print(params)
  if (length(params) == 6) {
    adv <- params[5:6]
  } else {
    adv <- c(0, 0)
  }

  # Bounds for the parameters
  lower.bound <- c(1e-6, 1e-6, 1e-6, 1e-6)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)
  if (length(params) == 6) {
    lower.bound <- c(lower.bound, -Inf, -Inf)
    upper.bound <- c(upper.bound, Inf, Inf)
  }

  # Check if the parameters are in the bounds
  if (any(params < lower.bound) || any(params > upper.bound)) {
    message("out of bounds")
    return(1e9)
  }

  if (!all(adv == c(0, 0))) { # if we have the advection parameters
    # then the lag vectors are different
    df_lags <- get_lag_vectors(locations, params, hmax = hmax, tau_vect = tau)
  }

  if (is.null(excesses)) {
    excesses <- empirical_excesses(simu, quantile, df_lags)
  }

  nj <- excesses$nj # number of marginal excesses
  Tobs <- excesses$Tobs # T - tau
  p <- nj / Tobs # probability of marginal excesses
  kij <- excesses$kij # number of joint excesses
  chi <- theorical_chi(params, df_lags) # get chi matrix
  # transform in chi vector
  chi_vect <- as.vector(chi$chi)
  chi_vect <- ifelse(chi_vect <= 0, 0.000001, chi_vect) # avoid log(0)

  non_excesses <- Tobs - kij # number of non-excesses
  # log-likelihood vector
  ll_vect <- kij * log(chi_vect) + non_excesses * log(1 - p * chi_vect)

  # final negative log-likelihood
  nll <- -sum(ll_vect, na.rm = TRUE)
  return(nll)
}
```

```{r}
q <- 0.9
nll <- neg_ll(true_param, simu_df, df_lags, sites_coords, quantile = q)
print(nll)
nll <- neg_ll(true_param + 0.05, simu_df, df_lags, sites_coords, quantile = q)
print(nll)
nll <- neg_ll(true_param - 0.1, simu_df, df_lags, sites_coords, quantile = q)
print(nll)
```

# Verif optimisation : distribution des dépassements

```{r}
q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

n_marg <- max(excesses$nj)
Tobs <- excesses$Tobs # T - tau
Tmax <- nrow(simu_df)
# p_hat <- n_marg / Tmax # probability of marginal excesses
kij <- excesses$kij # number of joint excesses

chi_theorical <- theorical_chi(true_param, df_lags)

tau <- 1
hnorm <- 2
chi_tau_h <- chi_theorical$chi[chi_theorical$tau == tau & 
                               chi_theorical$hnorm == hnorm]
k_tau_h <- excesses$kij[excesses$tau == tau & 
                          excesses$hnorm == hnorm]
proba_tau_h <- unique(chi_tau_h * p_hat)
n <- Tmax - tau # t - tau
p_hat_tau <- n_marg / n
proba_tau_h <- unique(chi_tau_h * p_hat_tau)

Tobs_tau_h <- unique(Tobs[excesses$tau == tau & 
                   excesses$hnorm == hnorm])
par(mfrow = c(1, 1))
x <- 0:Tobs_tau_h
# Density
plot(density(k_tau_h), main = "Density vs Binomial distribution",
     xlab = "Number of excesses", ylim = c(0, 0.2))
# binomial density
lines(x, dbinom(x, size = Tobs_tau_h, prob = proba_tau_h), col = "red", lwd = 2)
```


```{r, fig.height=7, fig.width=7, echo=FALSE}
q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

n_marg <- unique(excesses$nj)
# Tobs <- excesses$Tobs # T - tau
Tmax <- nrow(simu_df)
p_hat <- n_marg / Tmax # probability of marginal excesses
kij <- excesses$kij # number of joint excesses
describe(kij)

chi_theorical <- theorical_chi(true_param, df_lags)

tau_values <- unique(excesses$tau)

plot_data <- data.frame()

for (tau in tau_values) {
  hnorm <- 1 # fixed hnorm
  chi_tau_h <- chi_theorical$chi[chi_theorical$tau == tau &
                                 chi_theorical$hnorm == hnorm]
  k_tau_h <- excesses$kij[excesses$tau == tau &
                          excesses$hnorm == hnorm]

  proba_tau_h <- unique(chi_tau_h * p_hat)
  Tobs <- Tmax - tau # t - tau
  # p_hat_tau <- n_marg / Tobs
  # proba_tau_h <- unique(chi_tau_h * p_hat_tau)
  # Empirical density
  density_data <- density(k_tau_h)
  density_df <- data.frame(x = density_data$x, 
          y = density_data$y, tau = as.factor(tau), type = "Empirical")

  # Theorical binomial distribution
  x_vals <- 0:Tobs
  binom_y <- dbinom(x_vals, size = Tobs, prob = proba_tau_h)
  binom_df <- data.frame(x = x_vals, y = binom_y,
                         tau = as.factor(tau), type = "Binomial")

  # Combine
  plot_data <- rbind(plot_data, density_df, binom_df)
}

ggplot(plot_data, aes(x = x, y = y, color = type, linetype = type)) +
  geom_line(size = 1) +
  facet_wrap(~tau, scales = "free_y") +
  labs(title = paste0("Density vs Binomial distribution for q = ", q,
              " and hnorm = ", hnorm, " for each tau"),
       x = "Number of excesses",
       y = "Density") +
  theme_minimal() +
  xlim(0, 40) +
  theme(legend.title = element_blank())

```



```{r, fig.height=7, fig.width=7, echo=FALSE}
q <- 0.82
tau_vect <- 0:1
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = tau_vect)
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

n_marg <- max(excesses$nj)
# Tobs <- excesses$Tobs # T - tau
Tmax <- nrow(simu_df)
p_hat <- n_marg / Tmax # probability of marginal excesses
kij <- excesses$kij # number of joint excesses

chi_theorical <- theorical_chi(true_param, df_lags)

tau_values <- unique(excesses$tau)

plot_data <- data.frame()

for (tau in tau_values) {
  hnorm <- 2 # fixed hnorm
  chi_tau_h <- chi_theorical$chi[chi_theorical$tau == tau &
                                 chi_theorical$hnorm == hnorm]
  k_tau_h <- excesses$kij[excesses$tau == tau &
                          excesses$hnorm == hnorm]

  proba_tau_h <- unique(chi_tau_h * p_hat)
  Tobs <- Tmax - tau # t - tau

  # Empirical density
  density_data <- density(k_tau_h)
  density_df <- data.frame(x = density_data$x, 
          y = density_data$y, tau = as.factor(tau), type = "Empirical")

  # Theorical binomial distribution
  x_vals <- 0:Tobs
  binom_y <- dbinom(x_vals, size = Tobs, prob = proba_tau_h)
  binom_df <- data.frame(x = x_vals, y = binom_y,
                         tau = as.factor(tau), type = "Binomial")

  # Combine
  plot_data <- rbind(plot_data, density_df, binom_df)
}

ggplot(plot_data, aes(x = x, y = y, color = type, linetype = type)) +
  geom_line(size = 1) +
  facet_wrap(~tau, scales = "free_y") +
  labs(title = paste0("Density vs Binomial distribution for q = ", q,
              " and hnorm = ", hnorm, " for each tau"),
       x = "Number of excesses",
       y = "Density") +
  theme_minimal() +
  xlim(0, 100) +
  theme(legend.title = element_blank())

```

Si je réduis le nombre de décalage temporel c'est beaucoup plus stable et j'ai de meilleures estimations.

```{r}
tau1 <- 0:1 # 0:tmax
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = tau1)

q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)
result_tau1 <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))
# print(result_tau1$convergence) # 0 if it has converged
# print(result_tau1$par)
estim_tau1 <- result_tau1$par
rmse_tau1 <- sqrt((estim_tau1 - true_param)^2)
```


```{r, echo=FALSE}
tau2 <- 0:2
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = tau2)

# q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)
result_tau2 <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))
# print(result_tau2$convergence) # 0 if it has converged
# print(result_tau2$par)

estim_tau2 <- result_tau2$par
rmse_tau2 <- sqrt((estim_tau2 - true_param)^2)
```


```{r, echo=FALSE}
tau3 <- 0:4
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = tau3)

# q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)

result_tau3 <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))

# print(result_tau3$convergence) # 0 if it has converged
# print(result_tau3$par)

estim_tau3 <- result_tau3$par
rmse_tau3 <- sqrt((estim_tau3 - true_param)^2)
```


```{r, echo=FALSE}
tau4 <- 0:6
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = tau4)

# q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)

result_tau4 <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))

# print(result_tau4$convergence) # 0 if it has converged
# print(result_tau4$par)

estim_tau4 <- result_tau4$par
rmse_tau4 <- sqrt((estim_tau4 - true_param)^2)
```

```{r, echo=FALSE}
tau5 <- 0:8
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = tau5)

# q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)

result_tau5 <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))

# print(result_tau5$convergence) # 0 if it has converged
# print(result_tau5$par)

estim_tau5 <- result_tau5$par
rmse_tau5 <- sqrt((estim_tau5 - true_param)^2)
```


```{r, echo=FALSE}
tau6 <- 0:10
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = tau6)

# q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)

result_tau6 <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))

# print(result_tau5$convergence) # 0 if it has converged
# print(result_tau5$par)

estim_tau6 <- result_tau6$par
rmse_tau6 <- sqrt((estim_tau6 - true_param)^2)
```

```{r, echo=FALSE}
# Create a dataframe with the results
df_rmse <- data.frame(tmax = c(1, 2, 4, 6, 8, 10),
                      rmse_beta1 = c(rmse_tau1[1], rmse_tau2[1], rmse_tau3[1],
                                rmse_tau4[1], rmse_tau5[1], rmse_tau6[1]),
                      rmse_beta2 = c(rmse_tau1[2], rmse_tau2[2], rmse_tau3[2],
                                rmse_tau4[2], rmse_tau5[2], rmse_tau6[2]),
                      rmse_alpha1 = c(rmse_tau1[3], rmse_tau2[3], rmse_tau3[3],
                                rmse_tau4[3], rmse_tau5[3], rmse_tau6[3]),
                      rmse_alpha2 = c(rmse_tau1[4], rmse_tau2[4], rmse_tau3[4],
                                rmse_tau4[4], rmse_tau5[4], rmse_tau6[4]))

kable(df_rmse, "latex", booktabs = TRUE,
      caption = "RMSE for each parameter and 
                  different sets of temporal lags 0:tmax")
# print(df_rmse)
```




## Other simulations

```{r}
true_param <- c(0.4, 0.2, 1.5, 1)
ngrid <- 5
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:500

# load the simulations
file_path <- paste0("../data/simulations_BR/sim_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df <- read.csv(file_path)

nsites <- ncol(simu_df)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
```

Pour tout quantile ce n'est pas binomial... il y a un problème.

```{r}
q <- 0.92
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = 0:10)
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

n_marg <- max(excesses$nj)
Tobs <- excesses$Tobs # T - tau
Tmax <- nrow(simu_df)
p_hat <- n_marg / Tmax # probability of marginal excesses
kij <- excesses$kij # number of joint excesses

chi_theorical <- theorical_chi(true_param, df_lags)

tau <- 0
hnorm <- 1
chi_tau_h <- chi_theorical$chi[chi_theorical$tau == tau & 
                               chi_theorical$hnorm == hnorm]
k_tau_h <- excesses$kij[excesses$tau == tau & 
                          excesses$hnorm == hnorm]
proba_tau_h <- unique(chi_tau_h * p_hat)
n <- Tmax - tau 
Tobs_tau_h <- unique(Tobs[excesses$tau == tau & 
                   excesses$hnorm == hnorm])
par(mfrow = c(1, 1))
x <- 0:Tobs
# Density
plot(density(k_tau_h), main = "Density vs Binomial distribution",
     xlab = "Number of excesses", ylim = c(0, 0.2))
# binomial density
lines(x, dbinom(x, size = Tobs_tau_h, prob = proba_tau_h), col = "red", lwd = 2)
```


```{r, fig.height=7, fig.width=7, echo=FALSE}
q <- 0.92
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

n_marg <- max(excesses$nj)
# Tobs <- excesses$Tobs # T - tau
Tmax <- nrow(simu_df)
p_hat <- n_marg / Tmax # probability of marginal excesses
kij <- excesses$kij # number of joint excesses

chi_theorical <- theorical_chi(true_param, df_lags)

tau_values <- unique(excesses$tau)

plot_data <- data.frame()

for (tau in tau_values) {
  hnorm <- 1 # fixed hnorm
  chi_tau_h <- chi_theorical$chi[chi_theorical$tau == tau &
                                 chi_theorical$hnorm == hnorm]
  k_tau_h <- excesses$kij[excesses$tau == tau &
                          excesses$hnorm == hnorm]

  proba_tau_h <- unique(chi_tau_h * p_hat)
  Tobs <- Tmax - tau # t - tau

  # Empirical density
  density_data <- density(k_tau_h)
  density_df <- data.frame(x = density_data$x, 
          y = density_data$y, tau = as.factor(tau), type = "Empirical")

  # Theorical binomial distribution
  x_vals <- 0:Tobs
  binom_y <- dbinom(x_vals, size = Tobs, prob = proba_tau_h)
  binom_df <- data.frame(x = x_vals, y = binom_y,
                         tau = as.factor(tau), type = "Binomial")

  # Combine
  plot_data <- rbind(plot_data, density_df, binom_df)
}

ggplot(plot_data, aes(x = x, y = y, color = type, linetype = type)) +
  geom_line(size = 1) +
  facet_wrap(~tau, scales = "free_y") +
  labs(title = paste0("Density vs Binomial distribution for q = ", q,
              " and hnorm = ", hnorm, " for each tau"),
       x = "Number of excesses",
       y = "Density") +
  theme_minimal() +
  xlim(0, 40) +
  theme(legend.title = element_blank())
```


## Optimisation en fixant des parametres


```{r}
neg_ll_par <- function(beta1, beta2, alpha1, alpha2,
                  simu, df_lags, locations,
                  latlon = FALSE, quantile = 0.9,
                  simu_exp = FALSE, excesses = NULL) {
  # adv <- c(adv1, adv2)
  params <- c(beta1, beta2, alpha1, alpha2)
  # if (adv1 == 0 && adv2 == 0) {
  #   params <- params[1:4]
  # }
  print(params)
  # hmax <- max(df_lags$hnorm)
  # tau <- unique(df_lags$tau)
  # print(params)

  lower.bound <- c(1e-6, 1e-6, 1e-6, 1e-6)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)
  if (length(params) == 6) {
    lower.bound <- c(lower.bound, -1e-6, -1e-6)
    upper.bound <- c(upper.bound, Inf, Inf)
  }

  # Check if the parameters are in the bounds
  if (any(params < lower.bound) || any(params > upper.bound)) {
    message("out of bounds")
    return(1e9)
  }

  # if (!all(adv == c(0, 0))) { # if we have the advection parameters
  #   # then the lag vectors are different
  #   df_lags <- get_lag_vectors(locations, params, hmax = hmax, tau_vect = tau)
  # }
  # excesses <- empirical_excesses(simu, quantile, df_lags)

  nj <- unique(excesses$nj) # number of marginal excesses
  n <- nrow(simu)
  pj <- nj / n
  kij <- excesses$kij # number of joint excesses
  chi <- theorical_chi(params, df_lags) # get chi matrix
  # transform in chi vector
  chi_vect <- as.vector(chi$chi)
  chi_vect <- ifelse(chi_vect <= 0, 0.000001, chi_vect) # avoid log(0)

  non_excesses <- n - kij # number of non-excesses
  # log-likelihood vector
  ll_vect <- kij * log(chi_vect) + non_excesses * log(1 - pj * chi_vect)

  # final negative log-likelihood
  nll <- -sum(ll_vect, na.rm = TRUE)
  return(nll)
}
```

```{r}
q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
res <- mle2(neg_ll_par, start = list(beta1 = true_param[1],
                                 beta2 = true_param[2],
                                 alpha1 = true_param[3],
                                 alpha2 = true_param[4]),
                 data = list(simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        excesses = excesses,
                        method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta2 = 0.2,
                               alpha1 = 1.5, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


```{r}
res <- mle2(neg_ll_par, start = list(beta1 = true_param[1],
                                 beta2 = true_param[2],
                                 alpha1 = true_param[3],
                                 alpha2 = true_param[4]),
                 data = list(simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        excesses = excesses,
                        method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4,
                               alpha1 = 1.5, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


```{r}
res <- mle2(neg_ll_par, start = list(beta1 = true_param[1],
                                 beta2 = true_param[2],
                                 alpha1 = true_param[3],
                                 alpha2 = true_param[4]),
                 data = list(simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        excesses = excesses,
                        method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, beta2 = 0.2, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


```{r}
res <- mle2(neg_ll_par, start = list(beta1 = true_param[1],
                                 beta2 = true_param[2],
                                 alpha1 = true_param[3],
                                 alpha2 = true_param[4]),
                 data = list(simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        excesses = excesses,
                        method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, beta2 = 0.2, alpha1 = 1.5))

print(res@details$conv) # 0 means convergence
print(res@coef)
```

```{r}
q <- 0.9
res <- mle2(neg_ll_par, start = list(beta1 = true_param[1],
                                 beta2 = true_param[2],
                                 alpha1 = true_param[3],
                                 alpha2 = true_param[4]),
                 data = list(simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        excesses = excesses,
                        method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta2 = 0.2, alpha2 = 1))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


```{r}
res <- mle2(neg_ll_par, start = list(beta1 = true_param[1],
                                 beta2 = true_param[2],
                                 alpha1 = true_param[3],
                                 alpha2 = true_param[4]),
                 data = list(simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        excesses = excesses,
                        method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, alpha1 = 1.5))

print(res@details$conv) # 0 means convergence
print(res@coef)
```

