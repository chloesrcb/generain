---
title: "Model on Montpellier rainfall"
author: " "
date: "`r Sys.Date()`" 
output:
  pdf_document:
    extra_dependencies: ["float"]
    encoding: "UTF-8"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 7,
                     fig.align = 'center', message = FALSE, warning = FALSE,
                      fig.pos='H')
par(cex.main = 0.8,
    cex.lab = 0.7,
    cex.axis = 0.6)
```


```{r lib, echo=FALSE}
# setwd("./script")
library(generain)
library(reshape2)
library(ggplot2)
source("load_libraries.R")
library(kableExtra)
library(extRemes)
library(bbmle)
library(ismev)
library(extRemes)
library(evd)
library(latex2exp)
library(geosphere)
library(dplyr)

btf_theme <- theme_minimal() +
  theme(axis.text.x = element_text(size =  6, angle = 0),
        axis.text.y = element_text(size = 6),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 6),
        title = element_text(size = 10),
        axis.line = element_blank(),  # Remove axis lines
        panel.border = element_blank(),  # Remove plot border
        panel.background = element_rect(fill = "transparent", color = NA),
        # Remove plot background
        axis.text = element_blank(),  # Remove axis text
        axis.ticks = element_blank(),  # Remove axis ticks
        plot.background = element_rect(fill = "transparent", color = NA),
        panel.grid = element_line(color = "#5c595943"))

# my green color
btfgreen <- "#69b3a2"
```

# Data 

```{r}
################################################################################
# HSM --------------------------------------------------------------------------
################################################################################
# get location of each rain gauge
setwd("./script")
location_gauges <- read.csv("../data/PluvioMontpellier_1min/pluvio_mtp_loc.csv")
location_gauges$codestation <- c("iem", "mse", "poly", "um", "cefe", "cnrs",
                                 "crbm", "archiw", "archie", "um35", "chu1",
                                 "chu2", "chu3", "chu4", "chu5", "chu6", "chu7")

# Get distances matrix
dist_mat <- get_dist_mat(location_gauges)
df_dist_hsm <- reshape_distances(dist_mat)

# get rain measurements
# load data
load("../data/PluvioMontpellier_1min/rain_mtp_5min_2019_2022.RData")
rain_hsm <- rain.all5[c(1, 6:ncol(rain.all5))]

################################################################################
# COMEPHORE --------------------------------------------------------------------
################################################################################
# load data
df_comephore <- read.csv("../data/comephore/comephore_full.csv", sep = ",")
loc_px <- read.csv("../data/comephore/coords_pixels_wgs84.csv", sep = ",")

rain_com <- df_comephore
colnames(rain_com)[1] <- "date"

# Get distances matrix
dist_mat <- get_dist_mat(loc_px)
df_dist_comephore <- reshape_distances(dist_mat)
```

## Plot comephore time series

```{r}
# Plot COMEPHORE time series
pixel <- rain_com$p1

plot(pixel)

# before 2007
rain_com$date <- as.Date(rain_com$date, format = "%Y-%m-%d %H:%M:%S")
rain_com_to2007 <- rain_com[rain_com$date < "2007-01-01", ]
plot(rain_com_to2007$p1)
rain_com_after2007 <- rain_com[rain_com$date >= "2007-01-01", ]
plot(rain_com_after2007$p1)

par(mfrow=c(2,1))
plot(rain_com_to2007$p1, type = "l", col = "blue", xlab = "Time", ylab = "Rainfall (mm)", main = "Rainfall before 2007", ylim=c(0, 80))
plot(rain_com_after2007$p1, type = "l", col = "blue", xlab = "Time", ylab = "Rainfall (mm)", main = "Rainfall after 2007", ylim=c(0, 80))
```

```{r}
library(ggplot2)
library(dplyr)

rain_com <- rain_com %>%
  mutate(
    period = ifelse(date < as.Date("2007-01-01"), "Before 2007", "After 2007")
  )

ggplot(rain_com, aes(x = date, y = p23, color = period)) +
  geom_line() +
  labs(
    title = "Rainfall Before and After 2007",
    x = "Date",
    y = "Rainfall (mm)"
  ) +
  theme_minimal()

rain_com_non_zero %>%
  group_by(period) %>%
  summarise(
    mean_p1 = mean(p1, na.rm = TRUE),
    median_p1 = median(p1, na.rm = TRUE),
    sd_p1 = sd(p1, na.rm = TRUE),
    min_p1 = min(p1, na.rm = TRUE),
    max_p1 = max(p1, na.rm = TRUE)
  )

anova_result <- aov(p1 ~ period, data = rain_com)
summary(anova_result)

rain_com_non_zero <- rain_com %>% filter(p1 > 0)

ggplot(rain_com_non_zero, aes(x = period, y = p1)) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_boxplot(width = 0.1, alpha = 0.8) +
  labs(
    title = "Rainfall Density Before and After 2007",
    x = "Period",
    y = "Rainfall (mm)"
  ) +
  theme_minimal()

```




# Corresponding COMEPHORE pixel for each HSM site

```{r}
# Compute distance between each rain gauge and each pixel
distances <- as.data.frame(matrix(NA, nrow = nrow(location_gauges), 
                            ncol = nrow(loc_px)))

for (i in 1:nrow(location_gauges)) {
  for (j in 1:nrow(loc_px)) {
    distances[i, j] <- distHaversine(
      c(location_gauges$Longitude[i], location_gauges$Latitude[i]),
      c(loc_px$Longitude[j], loc_px$Latitude[j])
    )
  }
}

# Assign column and row names for easier interpretation
colnames(distances) <- loc_px$pixel_name
rownames(distances) <- location_gauges$codestation

# Find the closest pixel for each rain gauge
closest_pixels <- apply(distances, 1, 
                        function(x) colnames(distances)[which.min(x)])

# Add the closest pixel to location_gauges
location_gauges$closest_pixel <- closest_pixels
location_gauges <- location_gauges[-1]
# Save the result or inspect
print(location_gauges)

location_gauges$coord_x_px <- loc_px$Longitude[match(closest_pixels,
                                                     loc_px$pixel_name)]
location_gauges$coord_y_px <- loc_px$Latitude[match(closest_pixels,
                                                    loc_px$pixel_name)]
```


Reduire les dates de COMEPHORE

```{r}
# rain_hsm <- na.omit(rain_hsm)
# df_no_zeros <- rain_hsm[!apply(rain_hsm == 0, 1, any), ]
# rain_hsm <- df_no_zeros
min_date <- min(rain_hsm$dates)
max_date <- max(rain_hsm$date)

extended_min_date <- min_date - 3600  # Subtract 3600 seconds (1 hour)
extended_max_date <- max_date + 3600  # Add 3600 seconds (1 hour)

filtered_comephore <- rain_com %>%
    dplyr::filter(date >= extended_min_date & date <= extended_max_date)

```


```{r}
# output_df <- data.frame()

# # Create the CSV if it doesn't exist
# if (!file.exists("output_dataset2.csv")) {
#     write.csv(data.frame(), "output_dataset2.csv", row.names = FALSE)
# }

# # Pre-calculate the filtered comephore dates once outside the loops
# filtered_comephore$date <- as.POSIXct(filtered_comephore$date, format = "%Y-%m-%d %H:%M:%S", tz = "GMT")

# # Loop over the rows in rain_hsm (time-based)
# for (t in 1:nrow(rain_hsm)) {
#     t_row <- rain_hsm[t, ]
#     t_date <- as.POSIXct(t_row$dates, format = "%Y-%m-%d %H:%M:%S %Z", tz = "GMT")
    
#     # Loop over the rows in location_gauges (site-based)
#     for (i in 1:nrow(location_gauges)) {
#         site <- location_gauges[i, ]

#         # Y_obs: Observation at the site
#         Y_obs <- t_row[[site$codestation]]
#         if (is.na(Y_obs)) next  # Skip if Y_obs is NA

#         # Get closest pixel coordinates
#         X1 <- site$closest_pixel
#         coord_x_Xcenter <- site$coord_x_px
#         coord_y_Xcenter <- site$coord_y_px

#         # Distances between X1 and each Comephore pixel (Using distHaversine)
#         distances <- distHaversine(cbind(coord_x_Xcenter, coord_y_Xcenter),
#                                   cbind(loc_px$Longitude, loc_px$Latitude))

#         # 3 km around X1 i.e., 9 pixels
#         cube_bounds <- loc_px[distances <= 1500, ]

#         # Temporal bounds (3 hours around t_date) i.e., 3 observations by pixel
#         t_start <- t_date - 3600 * 1.5
#         t_end <- t_date + 3600 * 1.5

#         # Find the rows in filtered_comephore that match the temporal bounds using match
#         date_indices <- which(filtered_comephore$date >= t_start & filtered_comephore$date <= t_end)
        
#         # Check if there are any relevant observations in the filtered_comephore data
#         if (length(date_indices) == 0) next  # Skip if no relevant observations

#         # Filter out the date-specific rows
#         cube_obs <- filtered_comephore[date_indices, ]

#         # Now filter the relevant pixel names
#         pixel_names <- cube_bounds$pixel_name
#         relevant_cols <- match(pixel_names, colnames(cube_obs))

#         # Exclude NA columns if any are returned by the match
#         relevant_cols <- relevant_cols[!is.na(relevant_cols)]
        
#         # Extract the observations for these relevant pixels
#         X_obs <- as.vector(as.matrix(cube_obs[, relevant_cols]))  # Remove the date column

#         # Ensure we have exactly 27 values for X_obs
#         if (length(X_obs) < 27) {
#             X_obs <- c(X_obs, rep(NA, 27 - length(X_obs)))  # if less than 27 values
#         }

#         # Create a new row for the output dataframe
#         site_data <- data.frame(
#             t = t_date,
#             site = site$codestation,
#             Y_obs = Y_obs,  # Add actual observation if available
#             coord_x_px = coord_x_Xcenter,
#             coord_y_px = coord_y_Xcenter
#         )
        
#         # Add X1 to X27 as columns
#         site_data <- cbind(site_data, matrix(X_obs, ncol = 27))

#         # Rename the new columns as X1, X2, ..., X27
#         colnames(site_data)[(ncol(site_data) - 26):ncol(site_data)] <- paste0("X", 1:27)

#         # Append the site_data row to the output_df
#         output_df <- rbind(output_df, site_data)

#         # Write the site_data to the CSV file
#         write.table(site_data, "output_dataset2.csv", row.names = FALSE,
#                     col.names = !file.exists("output_dataset2.csv"), sep = ",", append = TRUE)
#     }
# }

# # First rows
# print(head(output_df))

# # Write the final output_df to a new CSV file
# write.csv(output_df, "output_dataset.csv", row.names = FALSE)
```
## Get data 

```{r}
df_Y_X <- read.csv("output_dataset2.csv", header=FALSE)
colnames(df_Y_X)[1:5] <- c("date", "site", "Y_obs", "coord_x_px", "coord_y_px")
colnames(df_Y_X)[(ncol(df_Y_X) - 26):ncol(df_Y_X)] <- paste0("X", 1:27)
head(df_Y_X)

df_site <- df_Y_X[df_Y_X$site == "iem", ]
plot(df_site$Y_obs, type = "l")
nrow(df_site) / 12 / 24 / 365 # in year
```


## Install pinnEV

```{r}
# install.packages("reticulate")
# library(reticulate)
# py_version <- "3.9.18"
# path_to_python <- reticulate::install_python(version=py_version)
# reticulate::virtualenv_remove("pinnEV_env")
# # Create a virtual envionment 'pinnEV_env' with Python 3.9.18. Install tensorflow
# # within this environment.
# reticulate::virtualenv_create(envname = 'pinnEV_env',
#                               python=path_to_python,
#                               version=py_version)

# path<- paste0(reticulate::virtualenv_root(),"/pinnEV_env/bin/python")
# Sys.setenv(RETICULATE_PYTHON = path) # Set Python interpreter
# Sys.setenv(RETICULATE_LOG_LEVEL = "DEBUG")
# tf_version="2.13.1"
# reticulate::use_virtualenv("pinnEV_env", required = T)
# tensorflow::install_tensorflow(method="virtualenv", envname="pinnEV_env",
#                                version=tf_version) #Install version of tensorflow in virtual environment
# keras::install_keras(method = c("virtualenv"), envname = "pinnEV_env",version=tf_version)

# keras::is_keras_available() #Check if keras is available

# tf$constant("Hello TensorFlow!")

#Install spektral 1.3.0 - this is for the graph convolutional neural networks. Remove all code hereafter if not necessary.
# reticulate::virtualenv_install("pinnEV_env",
#                                packages = "spektral", version="1.3.0")

```

## Examples of pinnEV for EGPD 

```{r}
library(keras)
library(tensorflow)
# Create  predictors
preds<-rnorm(prod(c(2500,10,8)))

#Re-shape to a 3d array. First dimension corresponds to observations,
#last to the different components of the predictor set.
#Other dimensions correspond to indices of predictors, e.g., a grid of locations. Can be a 1D or 2D grid.
dim(preds)=c(2500,10,8)
#We have 2000 observations of eight predictors at 10 sites.

#Split predictors into linear, additive and nn. Different for kappa and scale parameters.
X.nn.k=preds[,,1:4] #Four nn predictors for kappa
X.lin.k=preds[,,5:6] #Two additive predictors for kappa
X.add.k=preds[,,7:8] #Two additive predictors for kappa

X.nn.s=preds[,,1:2] #Two nn predictors for sigma
X.lin.s=preds[,,3] #One linear predictor for sigma
dim(X.lin.s)=c(dim(X.lin.s),1) #Change dimension so consistent
X.add.s=preds[,,4] #One additive predictor for sigma
dim(X.add.s)=c(dim(X.add.s),1) #Change dimension so consistent

# Create toy response data

#Contribution to scale parameter
#Linear contribution
m_L_1 = 0.2*X.lin.s[,,1]

# Additive contribution
m_A_1 = 0.1*X.add.s[,,1]^2+0.2*X.add.s[,,1]

plot(X.add.s[,,1],m_A_1)

#Non-additive contribution - to be estimated by NN
m_N_1 = 0.2*exp(-4+X.nn.s[,,2]+X.nn.s[,,1])+
  0.1*sin(X.nn.s[,,1]-X.nn.s[,,2])*(X.nn.s[,,1]+X.nn.s[,,2])

sigma=0.4*exp(0.5+m_L_1+m_A_1+m_N_1+1) #Exponential link


#Contribution to kappa parameter
#Linear contribution
m_L_2 = 0.1*X.lin.k[,,1]-0.02*X.lin.k[,,2]

# Additive contribution
m_A_2 = 0.1*X.add.k[,,1]^2+0.1*X.add.k[,,1]-
  0.025*X.add.k[,,2]^3+0.025*X.add.k[,,2]^2

#Non-additive contribution - to be estimated by NN
m_N_2 = 0.5*exp(-3+X.nn.k[,,4]+X.nn.k[,,1])+
  sin(X.nn.k[,,1]-X.nn.k[,,2])*(X.nn.k[,,4]+X.nn.k[,,2])-
  cos(X.nn.k[,,4]-X.nn.k[,,1])*(X.nn.k[,,3]+X.nn.k[,,1])

kappa=exp(m_L_2+m_A_2+0.05 *m_N_2)  #Exponential link


xi=0.1 # Set xi

theta=array(dim=c(dim(sigma),3))
theta[,,1]=sigma; theta[,,2] = kappa; theta[,,3]=xi
#We simulate data from an eGPD model

#Simulate from eGPD model using same u as given above
Y=apply(theta,1:2,function(x) reGPD(1,sigma=x[1],kappa=x[2],xi=x[3]))

#Create training and validation, respectively.
#We mask 20% of the Y values and use this for validation
#Masked values must be set to -1e10 and are treated as missing whilst training

mask_inds=sample(1:length(Y),size=length(Y)*0.8)

Y.train<-Y.valid<-Y #Create training and validation, respectively.
Y.train[-mask_inds]=-1e10
Y.valid[mask_inds]=-1e10



#To build a model with an additive component, we require an array of evaluations of
#the basis functions for each pre-specified knot and entry to X.add.k and X.add.s

rad=function(x,c){ #Define a basis function. Here we use the radial bases
  out=abs(x-c)^2*log(abs(x-c))
  out[(x-c)==0]=0
  return(out)
}

n.knot.s = 4; n.knot.k = 5# set number of knots.
#Must be the same for each additive predictor,
#but can differ between the parameters sigma and kappa


#Get knots for sigma predictor
knots.s=matrix(nrow=dim(X.add.s)[3],ncol=n.knot.s)
for( i in 1:dim(X.add.s)[3]){
  knots.s[i,]=quantile(X.add.s[,,i],probs=seq(0,1,length=n.knot.s))
}

#Evaluate radial basis functions for s_\beta predictor
X.add.basis.s<-array(dim=c(dim(X.add.s),n.knot.s))
for( i in 1:dim(X.add.s)[3]) {
  for(k in 1:n.knot.s) {
    X.add.basis.s[,,i,k]= rad(x=X.add.s[,,i],c=knots.s[i,k])
    #Evaluate rad at all entries to X.add.k and for all knots
  }}


#Get knots for kappa predictors
knots.k=matrix(nrow=dim(X.add.k)[3],ncol=n.knot.k)

#We set knots to be equally-spaced marginal quantiles
for( i in 1:dim(X.add.k)[3]){
  knots.k[i,]=quantile(X.add.k[,,i],probs=seq(0,1,length=n.knot.k))
}


#Evaluate radial basis functions for kappa predictors
X.add.basis.k<-array(dim=c(dim(X.add.k),n.knot.k))
for( i in 1:dim(X.add.k)[3]) {
  for(k in 1:n.knot.k) {
    X.add.basis.k[,,i,k]= rad(x=X.add.k[,,i],c=knots.k[i,k])
    #Evaluate rad at all entries to X.add.k and for all knots
  }}


#lin+GAM+NN models defined for both scale and kappa parameters
X.s=list("X.nn.s"=X.nn.s, "X.lin.s"=X.lin.s,
         "X.add.basis.s"=X.add.basis.s) #Predictors for sigma
X.k=list("X.nn.k"=X.nn.k, "X.lin.k"=X.lin.k,
         "X.add.basis.k"=X.add.basis.k) #Predictors for kappa


#Fit the eGPD model. Note that training is not run to completion.
NN.fit<-eGPD.NN.train(Y.train, Y.valid,X.s,X.k, type="MLP",
                      n.ep=50, batch.size=50,init.scale=1, init.kappa=1,init.xi=0.1,
                      widths=c(6,3),seed=1)
out<-eGPD.NN.predict(X.s=X.s,X.k=X.k,NN.fit$model)

print("sigma linear coefficients: "); print(round(out$lin.coeff_s,2))
print("kappa linear coefficients: "); print(round(out$lin.coeff_k,2))

# Note that this is a simple example that can be run in a personal computer.


# #To save model, run
# NN.fit$model %>% save_model_tf("model_eGPD")
# #To load model, run
#  model  <- load_model_tf("model_eGPD",
#   custom_objects=list(
#     "eGPD_loss__"=
#       eGPD_loss())
#         )


# Plot splines for the additive predictors



#Sigma predictors
n.add.preds_s=dim(X.add.s)[length(dim(X.add.s))]
par(mfrow=c(1,n.add.preds_s))
for(i in 1:n.add.preds_s){
  plt.x=seq(from=min(knots.s[i,]),to=max(knots.s[i,]),length=1000)  #Create sequence for x-axis
  
  tmp=matrix(nrow=length(plt.x),ncol=n.knot.s)
  for(j in 1:n.knot.s){
    tmp[,j]=rad(plt.x,knots.s[i,j]) #Evaluate radial basis function of plt.x and all knots
  }
  plt.y=tmp%*%out$gam.weights_s[i,]
  plot(plt.x,plt.y,type="l",main=paste0("sigma spline: predictor ",i),xlab="x",ylab="f(x)")
  points(knots.s[i,],rep(mean(plt.y),n.knot.s),col="red",pch=2)
  #Adds red triangles that denote knot locations
  
}

#Kappa predictors
n.add.preds_k=dim(X.add.k)[length(dim(X.add.k))]
par(mfrow=c(1,n.add.preds_k))
for(i in 1:n.add.preds_k){
  plt.x=seq(from=min(knots.k[i,]),to=max(knots.k[i,]),length=1000)  #Create sequence for x-axis
  
  tmp=matrix(nrow=length(plt.x),ncol=n.knot.k)
  for(j in 1:n.knot.k){
    tmp[,j]=rad(plt.x,knots.k[i,j]) #Evaluate radial basis function of plt.x and all knots
  }
  plt.y=tmp%*%out$gam.weights_k[i,]
  plot(plt.x,plt.y,type="l",main=paste0("kappa spline: predictor ",i),xlab="x",ylab="f(x)")
  points(knots.k[i,],rep(mean(plt.y),n.knot.k),col="red",pch=2)
  #Adds red triangles that denote knot locations
}

```

## Application to our data


```{r}
library(lubridate)
df_downscaled <- df_Y_X
# get hour, min, d, m, y from date
df_downscaled$hour <- hour(df_downscaled$date)
df_downscaled$minute <- minute(df_downscaled$date)
df_downscaled$day <- day(df_downscaled$date)
df_downscaled$month <- month(df_downscaled$date, label = FALSE)
df_downscaled$year <- year(df_downscaled$date)

# get coords of each site and put it in the df_downscaled
df_downscaled$coord_x_s <- location_gauges$Longitude[match(df_downscaled$site, location_gauges$codestation)]
df_downscaled$coord_y_s <- location_gauges$Latitude[match(df_downscaled$site, location_gauges$codestation)]

# rearrange columns
df_downscaled <- df_downscaled[, c(2, 3, 33, 34, 35, 36, 37, 38, 39, 4, 5, 6:32)]
head(df_downscaled)

# Diviser les données par site
site_data <- split(df_downscaled, df_downscaled$site)
length(site_data) == length(unique(df_downscaled$site))

site_predictors <- lapply(site_data, function(site_df) {
    site_df[-1] # Supposons que les prédicteurs sont nommés "X1", "X2", etc.
})

my_array <- array(unlist(site_predictors), dim = c(nrow(df_downscaled), ncol(df_downscaled) - 1, length(site_predictors)))
dim(my_array)
tail(my_array)

head_df <- head(df_downscaled)

# Print latex table
kable(head_df, format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "hold_position") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2:9, bold = TRUE) %>%
  column_spec(10:35, bold = TRUE)
```



```{r}
# Get predictors from df_Y_X
preds<-df_downscaled[, 2:ncol(df_downscaled)]
head(preds)
dim(preds)

n_obs <- nrow(preds)
n_sites <- 17
n_components <- ncol(preds) / n_sites

# Reshape en 3D
dim(preds) <- c(n_obs, n_sites, n_components)

Y <- df_downscaled$Y_obs
mask_inds=sample(1:length(Y),size=length(Y)*0.8)

Y.train <- Y.valid <- Y
Y.train[-mask_inds]=-1e10
Y.valid[mask_inds]=-1e10

#Split predictors into linear, additive and nn. Different for kappa and scale parameters.
X.nn.k=preds[,,1:4] #Four nn predictors for kappa
X.lin.k=preds[,,5:6] #Two additive predictors for kappa
X.add.k=preds[,,7:8] #Two additive predictors for kappa

X.nn.s=preds[,,1:2] #Two nn predictors for sigma
X.lin.s=preds[,,3] #One linear predictor for sigma
dim(X.lin.s)=c(dim(X.lin.s),1) #Change dimension so consistent
X.add.s=preds[,,4] #One additive predictor for sigma
dim(X.add.s)=c(dim(X.add.s),1) #Change dimension so consistent



```