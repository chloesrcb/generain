---
title: "Optimisation sans advection"
author: " "
date: "`r Sys.Date()`"  # Affiche la date actuelle
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 3.5,
                      fig.align = 'center', message = FALSE, warning = FALSE)
```


```{r lib}
setwd("./script")
library(generain)
library(reshape2)
library(ggplot2)
source("load_libraries.R")
library(animation)
```


## Anciennes simulations


```{r}
save_results_optim <- function(result, filename) {
  if (result$convergence == 0) {
    rmse <- sqrt((result$par - true_param)^2)
    df_rmse <- data.frame(estim = result$par, rmse = rmse)
    rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2", "Vx", "Vy")
    # save the results
    write.csv(t(df_rmse), file = paste0("../data/simulations_BR/results/",
                          filename, ".csv"))
  } else {
    print("No convergence")
  }
}

get_results_optim <- function(filename) {
  df_rmse <- read.csv(paste0("../data/simulations_BR/results/", filename, 
                      ".csv"))
  return(df_rmse)
}
```


```{r}
sim_BR <- function(beta1, beta2, alpha1, alpha2, x, y, z, n.BR) { 
  ## Setup 
  RandomFields::RFoptions(spConform=FALSE) 
  lx <- length(sx <- seq_along(x)) 
  ly <- length(sy <- seq_along(y)) 
  lz <- length(sz <- seq_along(z)) 
  ## Model-Variogram BuhlCklu 
  modelBuhlCklu <- RandomFields::RMfbm(alpha=alpha1, var=beta1, proj=1) + 
                  RandomFields::RMfbm(alpha=alpha1, var=beta1, proj=2) + 
                  RandomFields::RMfbm(alpha=alpha2, var=beta2, proj=3)
  
  ## Construct grid 
  Nxy <- lx * ly 
  N <- Nxy * lz 
  grid <- matrix(0, nrow=N, ncol=3) # (N,3)-matrix 

  for (i in sx) 
    for (j in seq_len(ly*lz)) 
      grid[i+(j-1)*ly, 1] <- i 
  
  for (i in sy) 
    for (j in sx) 
      for(k in sz) 
        grid[j+lx*(i-1)+(k-1)*Nxy, 2] <- i 
  
  for (i in sz) 
    for (j in seq_len(Nxy)) 
      grid[j+Nxy*(i-1), 3] <- i

  ## Construct shifted variogram
  Varm1 <- vapply(seq_len(N), function(n) 
      RandomFields::RFvariogram(modelBuhlCklu,
        x=sx-grid[n,1], 
        y=sy-grid[n,2], 
        z=sz-grid[n,3]), 
        array(NA_real_, dim=c(lx, ly, lz))) ## => (lx, ly, lz, N)-array

  ## Main 
  set.seed(123)
  Z <- array(, dim=c(lx, ly, lz, n.BR)) # 4d array 
  E <- matrix(rexp(n.BR * N), nrow=n.BR, ncol=N) 
  for (i in seq_len(n.BR)) { ## n=1 
    V <- 1/E[i,1] 
    W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z, n=1) 
    Y <- exp(W - W[1] - Varm1[,,,1]) 
    Z[,,,i] <- V * Y 
    ## n in {2,..,N} 
    for(n in 2:N) { 
      Exp <- E[i,n] 
      V <- 1/Exp 
      while(V > Z[N*(i-1)+n]) { 
        W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z) 
        Y <- exp(W - W[n] - Varm1[,,,n]) 
        if(all(V*Y[seq_len(n-1)] < Z[(N*(i-1)+1):(N*(i-1)+(n-1))])) 
          Z[,,,i] <- pmax(V*Y, Z[,,,i]) 
          Exp <- Exp + rexp(1) 
          V <- 1/Exp 
      } 
    } 
  } 
  ## Return 
  Z 
}
```


```{r}
true_param <- c(0.4, 0.2, 1.5, 1)
ngrid <- 5
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:300
n.BR <- 1

# generate the simulations
BR <- sim_BR(true_param[1] * 2, true_param[2] * 2, true_param[3], true_param[4],
             spa, spa, temp, n.BR)
save_simulations(BR, ngrid, n.BR,
        folder = paste0("../data/simulations_BR/sim_", ngrid^2, "s_",
                                length(temp), "t/"),
                  file = paste0("br_", ngrid^2, "s_",
                                length(temp), "t"), forcedind = 1)

# load the simulations
file_path <- paste0("../data/simulations_BR/sim_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df <- read.csv(file_path)
```


```{r}
nsites <- ncol(simu_df)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
```

## Validation du modèle de Buhl séparable

Pour la simulation avec 25 sites et 300 pas de temps
et un quantile de 0.9 on obtient une bonne estimation des
paramètres du modèle de Buhl séparable avec WLSE.

En revanche, pour une simulation avec 49 sites et 300 pas de temps avec un quantile de 0.9
on obtient une mauvaise estimation notamment pour le paramètre \(\beta_1\) et le choix
du quantile a un impact sur l'estimation des paramètres mais aucune valeur de quantile ne 
permet une bonne estimation de tous les paramètres.

```{r}
get_lag_vectors <- function(df_coords, params, hmax = NA, tau_vect = 1:10) {

  # Advection vector
  adv <- if (length(params) == 6) params[5:6] else c(0, 0)

  n <- nrow(df_coords)
  tau_len <- length(tau_vect)

  # Create index combinations
  indices <- combn(n, 2)
  i_vals <- indices[1, ]
  j_vals <- indices[2, ]

  # Calculate lags
  lag_latitudes <- df_coords$Latitude[j_vals] - df_coords$Latitude[i_vals]
  lag_longitudes <- df_coords$Longitude[j_vals] - df_coords$Longitude[i_vals]

  # Calculate hnorm for all pairs
  hnorms <- sqrt(lag_latitudes^2 + lag_longitudes^2)

  # Filter based on hmax
  if (!is.na(hmax)) {
    valid_indices <- which(hnorms <= hmax)
    i_vals <- i_vals[valid_indices]
    j_vals <- j_vals[valid_indices]
    lag_latitudes <- lag_latitudes[valid_indices]
    lag_longitudes <- lag_longitudes[valid_indices]
    hnorms <- hnorms[valid_indices]
  }

  # Replicate for tau_vect
  num_pairs <- length(i_vals)
  i_vals <- rep(i_vals, each = tau_len)
  j_vals <- rep(j_vals, each = tau_len)
  lag_latitudes <- rep(lag_latitudes, each = tau_len)
  lag_longitudes <- rep(lag_longitudes, each = tau_len)
  hnorms <- rep(hnorms, each = tau_len)
  taus <- rep(tau_vect, times = num_pairs)

  # Apply advection
  if (all(adv != c(0, 0))) {
    lag_latitudes <- lag_latitudes - adv[1] * taus
    lag_longitudes <- lag_longitudes - adv[2] * taus
    hnorms <- sqrt(lag_latitudes^2 + lag_longitudes^2)
  }

  # Create final dataframe
  lags <- data.frame(
    s1 = i_vals,
    s2 = j_vals,
    h1 = lag_latitudes,
    h2 = lag_longitudes,
    tau = taus,
    hnorm = hnorms
  )

  return(lags)
}

```

```{r}
sites_coords <- generate_grid_coords(sqrt(nsites))
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = 0:10)

hmax <- sqrt(17)
q <- 0.85
chispa <- spatial_chi_alldist(df_dist, simu_df, quantile = q,
                               hmax = hmax)
spa_estim <- get_estimate_variospa(chispa, weights = "exp", summary = FALSE)
print(spa_estim)

q <- 0.92
tmax <- 10
chitemp <- temporal_chi(simu_df, tmax = tmax, quantile = q)
temp_estim <- get_estimate_variotemp(chitemp, tmax, npoints = ncol(simu_df),
                                      weights = "exp", summary = FALSE)
print(temp_estim)
df_result <- data.frame(beta1 =  spa_estim[1],
                        alpha1 = spa_estim[2],
                        beta2 = temp_estim[1],
                        alpha2 = temp_estim[2])
colnames(df_result) <- c("beta1", "alpha1", "beta2", "alpha2")

df_valid <- get_criterion(df_result, true_param)
colnames(df_valid) <- c("estim", "rmse", "mae")
print(df_valid)
```


## Optimisation

### Get excesses

```{r}
empirical_excesses <- function(data_rain, quantile, df_lags) {
  excesses <- df_lags # copy the dataframe
  unique_tau <- unique(df_lags$tau) # unique temporal lags

  for (t in unique_tau) { # loop over temporal lags
    df_h_t <- df_lags[df_lags$tau == t, ] # get the dataframe for each lag

    for (i in seq_len(nrow(df_h_t))) { # loop over each pair of sites
      # get the indices of the sites
      ind_s2 <- as.numeric(as.character(df_h_t$s2[i]))
      ind_s1 <- df_h_t$s1[i]

      # get the data for the pair of sites
      rain_cp <- data_rain[, c(ind_s1, ind_s2), drop = FALSE]
      rain_cp <- na.omit(rain_cp)
      colnames(rain_cp) <- c("s1", "s2")

      Tmax <- nrow(rain_cp) # number of time steps
      rain_nolag <- rain_cp$s1[1:(Tmax - t)] # get the data without lag
      rain_lag <- rain_cp$s2[(1 + t):Tmax] # get the data with lag

      Tobs <- length(rain_nolag) # number of observations
      # transform the data in uniform data
      rain_unif <- cbind(rank(rain_nolag) / (Tobs + 1), 
                         rank(rain_lag) / (Tobs + 1))
      # get the conditional excesses on s2
      cp_cond <- rain_unif[rain_unif[, 2] > quantile, , drop = FALSE]
      joint_excesses <- sum(cp_cond[, 1] > quantile) # number of excesses for s1
                                                   # given those of s2
      marginal_excesses <- nrow(cp_cond) # number excesses for s2

      # store the number of excesses
      excesses$Tobs[excesses$s1 == ind_s1
                      & excesses$s2 == ind_s2
                      & excesses$tau == t] <- Tobs
      excesses$nj[excesses$s1 == ind_s1
                      & excesses$s2 == ind_s2
                      & excesses$tau == t] <- marginal_excesses
      excesses$kij[excesses$s1 == ind_s1
                    & excesses$s2 == ind_s2
                    & excesses$tau == t] <- joint_excesses
    }
  }
  return(excesses)
}

q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
print(head(excesses))

# density plot of the number of excesses
ggplot(excesses, aes(x = kij)) +
  geom_density() +
  labs(title = "Density plot of the number of excesses",
       x = "Number of excesses", y = "Density")

```


```{r}
neg_ll_nopj <- function(params, simu, df_lags, locations,
                  latlon = FALSE, quantile = 0.9,
                  simu_exp = FALSE, excesses = NULL) {
  hmax <- max(df_lags$hnorm)
  tau <- unique(df_lags$tau)

  print(params)
  if (length(params) == 6) {
    adv <- params[5:6]
  } else {
    adv <- c(0, 0)
  }

  lower.bound <- c(1e-6, 1e-6, 1e-6, 1e-6)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)
  if (length(params) == 6) {
    lower.bound <- c(lower.bound, -Inf, -Inf)
    upper.bound <- c(upper.bound, Inf, Inf)
  }

  # Check if the parameters are in the bounds
  if (any(params < lower.bound) || any(params > upper.bound)) {
    message("out of bounds")
    return(1e9)
  }

  if (all(adv == c(0, 0))) { # if we have the advection parameters
    df_lags <- get_lag_vectors(locations, params, tau = tau, hmax = hmax)
    excesses <- empirical_excesses(simu, quantile, df_lags)
  }

  if (is.null(excesses)) {
    excesses <- empirical_excesses(simu, quantile, df_lags)
  }

  nj <- excesses$nj # number of marginal excesses
  kij <- excesses$kij # number of joint excesses
  chi <- theoretical_chi(params, df_lags) # get chi matrix
  # transform in chi vector
  chi_vect <- as.vector(chi$chi)
  chi_vect <- ifelse(chi_vect <= 0, 0.000001, chi_vect) # avoid log(0)

  non_excesses <- nj - kij # number of non-excesses
  # log-likelihood vector
  ll_vect <- kij * log(chi_vect) + non_excesses * log(1 - chi_vect)

  # final negative log-likelihood
  nll <- -sum(ll_vect, na.rm = TRUE)
  return(nll)
}

neg_ll <- function(params, simu, df_lags, locations,
                  latlon = FALSE, quantile = 0.9,
                  simu_exp = FALSE, excesses = NULL) {
  hmax <- max(df_lags$hnorm)
  tau <- unique(df_lags$tau)

  print(params)
  if (length(params) == 6) {
    adv <- params[5:6]
  } else {
    adv <- c(0, 0)
  }

  lower.bound <- c(1e-6, 1e-6, 1e-6, 1e-6)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)
  if (length(params) == 6) {
    lower.bound <- c(lower.bound, -Inf, -Inf)
    upper.bound <- c(upper.bound, Inf, Inf)
  }

  # Check if the parameters are in the bounds
  if (any(params < lower.bound) || any(params > upper.bound)) {
    message("out of bounds")
    return(1e9)
  }

  if (!all(adv == c(0, 0))) { # if we have the advection parameters
    df_lags <- get_lag_vectors(locations, params, tau = tau, hmax = hmax)
    # excesses <- empirical_excesses(simu, quantile, df_lags)
  }

  if (is.null(excesses)) {
    excesses <- empirical_excesses(simu, quantile, df_lags)
  }

  nj <- excesses$nj # number of marginal excesses
  Tobs <- excesses$Tobs # T - tau
  p <- nj[1] / nrow(simu) # probability of marginal excesses
  kij <- excesses$kij # number of joint excesses
  chi <- theoretical_chi(params, df_lags) # get chi matrix
  # transform in chi vector
  chi_vect <- as.vector(chi$chi)
  chi_vect <- ifelse(chi_vect <= 0, 0.000001, chi_vect) # avoid log(0)

  non_excesses <- Tobs - kij # number of non-excesses
  # log-likelihood vector
  ll_vect <- kij * log(chi_vect) + non_excesses * log(1 - p * chi_vect)

  # final negative log-likelihood
  nll <- -sum(ll_vect, na.rm = TRUE)
  return(nll)
}
```


Pour la simulation avec 25 sites et 300 pas de temps
avec un quantile de 0.9 on obtient une bonne 
estimation des paramètres du modèle de Buhl séparable avec l'optimisation
de la vraisemblance composite.
En changeant le quantile, on garde des résultats similaires pour \(q > 0.9\) et proche de 0.9.
Pour \(q <= 0.9\), on obtient des résultats moins bons pour le paramètre \(\alpha_1\).


Pour la simulation avec 49 sites et 300 pas de temps
avec un quantile de 0.9 on obtient une mauvaise 
estimation des paramètres du modèle  avec l'optimisation
de la vraisemblance composite. Les alphas sont fortement sous-estimés.

Peu importe la méthode d'optimisation, on obtient des résultats similaires.

# Verif optimisation : distribution des dépassements

```{r}
q <- 0.82
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

n_marg <- max(excesses$nj)
Tobs <- excesses$Tobs # T - tau
Tmax <- nrow(simu_df)
p_hat <- n_marg / Tmax # probability of marginal excesses
kij <- excesses$kij # number of joint excesses

chi_theorical <- theoretical_chi(true_param, df_lags)

tau <- 1
hnorm <- 2
chi_tau_h <- chi_theorical$chi[chi_theorical$tau == tau & 
                               chi_theorical$hnorm == hnorm]
k_tau_h <- excesses$kij[excesses$tau == tau & 
                          excesses$hnorm == hnorm]
proba_tau_h <- unique(chi_tau_h * p_hat)
n <- Tmax - tau 
Tobs_tau_h <- unique(Tobs[excesses$tau == tau & 
                   excesses$hnorm == hnorm])
par(mfrow = c(1, 1))
# Histogram
# hist(k_tau_h, probability = TRUE, main = "Histogram vs Binomial distribution",
#      xlab = "Number of excesses")

x <- 0:Tobs_tau_h
# theoretical_probs <- dbinom(x, size = n, prob = proba_tau_h)
# lines(x, theoretical_probs, col = "red", type = "h", lwd = 2)

# Density
plot(density(k_tau_h), main = "Density vs Binomial distribution",
     xlab = "Number of excesses", ylim = c(0, 0.2))
# binomial density
lines(x, dbinom(x, size = Tobs_tau_h, prob = proba_tau_h), col = "red", lwd = 2)
```


```{r}
library(ggplot2)

q <- 0.82
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

n_marg <- max(excesses$nj)
# Tobs <- excesses$Tobs # T - tau
Tmax <- nrow(simu_df)
p_hat <- n_marg / Tmax # probability of marginal excesses
kij <- excesses$kij # number of joint excesses

chi_theorical <- theoretical_chi(true_param, df_lags)

tau_values <- unique(excesses$tau)

plot_data <- data.frame()

for (tau in tau_values) {
  hnorm <- 1 # fixed hnorm
  chi_tau_h <- chi_theorical$chi[chi_theorical$tau == tau &
                                 chi_theorical$hnorm == hnorm]
  k_tau_h <- excesses$kij[excesses$tau == tau &
                          excesses$hnorm == hnorm]

  proba_tau_h <- unique(chi_tau_h * p_hat)
  Tobs <- Tmax - tau # t - tau

  # Empirical density
  density_data <- density(k_tau_h)
  density_df <- data.frame(x = density_data$x, 
          y = density_data$y, tau = as.factor(tau), type = "Empirical")

  # Theorical binomial distribution
  x_vals <- 0:Tobs
  binom_y <- dbinom(x_vals, size = Tobs, prob = proba_tau_h)
  binom_df <- data.frame(x = x_vals, y = binom_y,
                         tau = as.factor(tau), type = "Binomial")

  # Combine
  plot_data <- rbind(plot_data, density_df, binom_df)
}

ggplot(plot_data, aes(x = x, y = y, color = type, linetype = type)) +
  geom_line(size = 1) +
  facet_wrap(~tau, scales = "free_y") +
  labs(title = paste0("Density vs Binomial distribution for q = ", q,
              " and hnorm = ", hnorm, " for each tau"),
       x = "Number of excesses",
       y = "Density") +
  theme_minimal() +
  xlim(0, 40) +
  theme(legend.title = element_blank())

```

On a \(k_{h, \tau} \sim Bin(T - \tau, p \chi_\Theta(h, \tau))\).




# OPTIM

En ajoutant l'advection en paramètre comme vecteur nul:

```{r}
q <- 0.9
excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1, 0, 0)
result <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1, 1, 1),
                                        maxit = 10000))
print(result$convergence) # 0 if it has converged
print(result$par)

filename <- paste0("optim_results_noadv_", ngrid^2, "s_q", q * 100)
save_results_optim(result, filename = filename)

df_rmse <- get_results_optim(filename)
print(df_rmse) # meme resultat que precedemment
```


## Avec moins de sites: 16 sites et 400 temps

```{r}
true_param <- c(0.4, 0.2, 1.5, 1)
ngrid <- 4
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:400
n.BR <- 1

# generate the simulations
# BR <- sim_BR(true_param[1] * 2, true_param[2] * 2, true_param[3], true_param[4],
#              spa, spa, temp, n.BR)
# save_simulations(BR, ngrid, n.BR,
#         folder = paste0("../data/simulations_BR/oldsim_", ngrid^2, "s_",
#                                 length(temp), "t/"),
#                   file = paste0("br_", ngrid^2, "s_",
#                                 length(temp), "t"), forcedind = 1)

# load the simulations
file_path_16 <- paste0("../data/simulations_BR/oldsim_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df_16 <- read.csv(file_path_16)
```


```{r}
nsites <- ncol(simu_df_16)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
# spatial lags
hmax <- sqrt(17)
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = hmax, tau_vect = 0:10)

```

Buhl:

```{r}
q <- 0.8
chispa <- spatial_chi_alldist(df_dist, simu_df_16, quantile = q,
                               hmax = hmax)
spa_estim <- get_estimate_variospa(chispa, weights = "exp", summary = FALSE)
print(spa_estim)

q <- 0.9
tmax <- 10
chitemp <- temporal_chi(simu_df_16, tmax = tmax, quantile = q)
temp_estim <- get_estimate_variotemp(chitemp, tmax, npoints = ncol(simu_df),
                                      weights = "exp", summary = FALSE)
print(temp_estim)
df_result <- data.frame(beta1 =  spa_estim[1],
                        alpha1 = spa_estim[2],
                        beta2 = temp_estim[1],
                        alpha2 = temp_estim[2])
colnames(df_result) <- c("beta1", "alpha1", "beta2", "alpha2")

df_valid <- get_criterion(df_result, true_param)
colnames(df_valid) <- c("estim", "rmse", "mae")
print(df_valid)
```

Optim:

```{r}
q <- 0.9
excesses <- empirical_excesses(simu_df_16, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)
result <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df_16,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))
print(result$convergence) # 0 if it has converged
print(result$par)

rmse <- sqrt((result$par - true_param)^2)
df_rmse <- data.frame(estim = result$par, rmse = rmse)
rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2")
print(t(df_rmse))
```


## 100 sites 100 temps

```{r}
true_param <- c(0.4, 0.2, 1.5, 1)
ngrid <- 10
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:100
n.BR <- 1

# generate the simulations
# BR <- sim_BR(true_param[1] * 2, true_param[2] * 2, true_param[3], 
#                     true_param[4], spa, spa, temp, n.BR)
# save_simulations(BR, ngrid, n.BR,
#         folder = paste0("../data/simulations_BR/oldsim_", ngrid^2, "s_",
#                                 length(temp), "t/"),
#                   file = paste0("br_", ngrid^2, "s_",
#                                 length(temp), "t"), forcedind = 1)

# load the simulations
file_path_100 <- paste0("../data/simulations_BR/oldsim_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df_100 <- read.csv(file_path_100)
```


```{r}
nsites <- ncol(simu_df_100)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
# spatial lags
hmax <- sqrt(17)
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = hmax, tau_vect = 0:10)

```

Buhl:

```{r}
q <- 0.8
chispa <- spatial_chi_alldist(df_dist, simu_df_100, quantile = q,
                               hmax = hmax)
spa_estim <- get_estimate_variospa(chispa, weights = "exp", summary = FALSE)
print(spa_estim)

q <- 0.75
tmax <- 10
chitemp <- temporal_chi(simu_df_100, tmax = tmax, quantile = q)
temp_estim <- get_estimate_variotemp(chitemp, tmax, npoints = ncol(simu_df),
                                      weights = "exp", summary = FALSE)
print(temp_estim)
df_result <- data.frame(beta1 =  spa_estim[1],
                        alpha1 = spa_estim[2],
                        beta2 = temp_estim[1],
                        alpha2 = temp_estim[2])
colnames(df_result) <- c("beta1", "alpha1", "beta2", "alpha2")

df_valid <- get_criterion(df_result, true_param)
colnames(df_valid) <- c("estim", "rmse", "mae")
print(df_valid)
```

Optim:

```{r}
q <- 0.7
excesses <- empirical_excesses(simu_df_100, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)
result <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df_100,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))
print(result$convergence) # 0 if it has converged
print(result$par)

rmse <- sqrt((result$par - true_param)^2)
df_rmse <- data.frame(estim = result$par, rmse = rmse)
rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2")
print(t(df_rmse))
```

## 36 sites 500 temps

Si je prends 36 sites et 300 pas de temps: marche pas 
Si je prends 36 sites et 500 pas de temps: marche

```{r}
true_param <- c(0.4, 0.2, 1.5, 1)
ngrid <- 6
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:500
n.BR <- 1

# generate the simulations
BR <- sim_BR(true_param[1] * 2, true_param[2] * 2, true_param[3], 
                    true_param[4], spa, spa, temp, n.BR)
save_simulations(BR, ngrid, n.BR,
        folder = paste0("../data/simulations_BR/oldsim_", ngrid^2, "s_",
                                length(temp), "t/"),
                  file = paste0("br_", ngrid^2, "s_",
                                length(temp), "t"), forcedind = 1)

# load the simulations
file_path_36 <- paste0("../data/simulations_BR/oldsim_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df_36 <- read.csv(file_path_36)
```


```{r}
nsites <- ncol(simu_df_36)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
# spatial lags
hmax <- sqrt(17)
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = hmax, tau_vect = 0:10)

```

Buhl:

```{r}
q <- 0.92
chispa <- spatial_chi_alldist(df_dist, simu_df_36, quantile = q,
                               hmax = hmax)
spa_estim <- get_estimate_variospa(chispa, weights = "exp", summary = FALSE)
print(spa_estim)

q <- 0.92
tmax <- 10
chitemp <- temporal_chi(simu_df_36, tmax = tmax, quantile = q)
temp_estim <- get_estimate_variotemp(chitemp, tmax, npoints = ncol(simu_df_36),
                                      weights = "exp", summary = FALSE)
print(temp_estim)
df_result <- data.frame(beta1 =  spa_estim[1],
                        alpha1 = spa_estim[2],
                        beta2 = temp_estim[1],
                        alpha2 = temp_estim[2])
colnames(df_result) <- c("beta1", "alpha1", "beta2", "alpha2")

df_valid <- get_criterion(df_result, true_param)
colnames(df_valid) <- c("estim", "rmse", "mae")
print(df_valid)
```

Optim:

```{r}
q <- 0.9 # change les estimations
excesses <- empirical_excesses(simu_df_36, quantile = q, df_lags = df_lags)
true_param <- c(0.4, 0.2, 1.5, 1)
result <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df_36,
                        quantile = q,
                        df_lags = df_lags,
                        excesses = excesses,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1),
                                        maxit = 10000))
print(result$convergence) # 0 if it has converged
print(result$par)

rmse <- sqrt((result$par - true_param)^2)
df_rmse <- data.frame(estim = result$par, rmse = rmse)
rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2")
print(t(df_rmse))
```



## Ajout de l'advection dans la simulation


```{r}
x <- 1:3
y <- 1:3
z <- 1:3

lx <- length(sx <- seq_along(x))
ly <- length(sy <- seq_along(y))
lz <- length(sz <- seq_along(z))

## Construct grid
Nxy <- lx * ly
N <- Nxy * lz
grid <- matrix(0, nrow = N, ncol = 3) # (N,3)-matrix

# Spatial x coordinates
for (i in sx) {
  for (j in seq_len(ly * lz)) {
    grid[i + (j - 1) * ly, 1] <- i
  }
}

# Spatial y coordinates
for (i in sy) {
  for (j in sx) {
    for (k in sz) {
      grid[j + lx * (i - 1) + (k - 1) * Nxy, 2] <- i
    }
  }
}

# Spatial z coordinates
for (i in sz) {
  for (j in seq_len(Nxy)) {
    grid[j + Nxy * (i - 1), 3] <- i
  }
}

print(head(grid))

# Advection vector
Vx <- 0.5
Vy <- 0.2
V <- c(Vx, Vy)

grid_adv <- grid
# Construct shifted grid
grid_adv[, 1] <- grid_adv[, 1] - grid_adv[, 3] * Vx
grid_adv[, 2] <- grid_adv[, 2] - grid_adv[, 3] * Vy


print(head(grid_adv))

```


```{r}
sim_BR_adv <- function(beta1, beta2, alpha1, alpha2, x, y, z, n.BR,
                       adv) {
  ## Setup
  RandomFields::RFoptions(spConform = FALSE)
  lx <- length(sx <- seq_along(x))
  ly <- length(sy <- seq_along(y))
  lz <- length(sz <- seq_along(z))

  ## Model-Variogram BuhlCklu
  modelBuhlCklu <- RandomFields::RMfbm(alpha = alpha1, var = beta1, proj = 1) +
                   RandomFields::RMfbm(alpha = alpha1, var = beta1, proj = 2) +
                   RandomFields::RMfbm(alpha = alpha2, var = beta2, proj = 3)

  ## Construct grid
  Nxy <- lx * ly
  N <- Nxy * lz
  grid <- matrix(0, nrow = N, ncol = 3) # (N,3)-matrix

  for (i in sx)
    for (j in seq_len(ly * lz))
      grid[i + (j - 1) * ly, 1] <- i

  for (i in sy)
    for (j in sx)
      for (k in sz)
        grid[j + lx * (i - 1) + (k - 1) * Nxy, 2] <- i

  for (i in sz)
    for (j in seq_len(Nxy))
      grid[j + Nxy * (i - 1), 3] <- i

  # Construct shifted grid with advected coordinates
  grid[, 1] <- grid[, 1] - grid[, 3] * adv[1]
  grid[, 2] <- grid[, 2] - grid[, 3] * adv[2]

  ## Construct shifted variogram
  Varm1 <- vapply(seq_len(N), function(n)
      RandomFields::RFvariogram(modelBuhlCklu,
        x = sx - grid[n, 1],
        y = sy - grid[n, 2],
        z = sz - grid[n, 3]),
        array(NA_real_, dim = c(lx, ly, lz))) ## => (lx, ly, lz, N)-array

  ## Main
  set.seed(123)
  Z <- array(, dim = c(lx, ly, lz, n.BR)) # 4d array
  E <- matrix(rexp(n.BR * N), nrow = n.BR, ncol = N)
  for (i in seq_len(n.BR)) { ## n=1 
    V <- 1 / E[i, 1]
    W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z, n = 1)
    Y <- exp(W - W[1] - Varm1[, , , 1])
    Z[, , , i] <- V * Y
    ## n in {2,..,N}
    for (n in 2:N) {
      Exp <- E[i, n]
      V <- 1 / Exp 
      while(V > Z[N * (i - 1) + n]) {
        W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, z)
        Y <- exp(W - W[n] - Varm1[, , , n])
        if(all(V * Y[seq_len(n-1)] < Z[(N*(i-1)+1):(N*(i-1)+(n-1))]))
          Z[, , , i] <- pmax(V * Y, Z[, , , i])
          Exp <- Exp + rexp(1) 
          V <- 1 / Exp 
      }
    }
  }
  ## Return
  Z
}
```




```{r}
adv <- c(0.5, 0.5)
true_param <- c(0.4, 0.2, 1.5, 1, adv)
ngrid <- 5
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:300
n.BR <- 1

# load the simulations
file_path <- paste0("../data/simulations_BR/oldsim_adv_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df <- read.csv(file_path)
```


```{r}
nsites <- ncol(simu_df)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
```


```{r, message=FALSE}
sites_coords <- generate_grid_coords(sqrt(nsites))
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = 0:10)

hmax <- sqrt(17)
q <- 0.8

excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

result <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        excesses = excesses,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1, 1, 1),
                                      maxit = 10000))

print(result$convergence) # 0 if it has converged
print(result$par)

if (result$convergence == 0) {
  rmse <- sqrt((result$par - true_param)^2)
  df_rmse <- data.frame(estim = result$par, rmse = rmse)
  rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2", "Vx", "Vy")
  # print(t(df_rmse))
  # save the results
  write.csv(t(df_rmse), file = "../data/optim_results_85.csv")
} else {
  print("No convergence")
}

# get result from csv
# df_rmse <- read.csv("../data/optim_results_80.csv")
# print(df_rmse)

# df_rmse <- read.csv("../data/optim_results_85.csv")
# print(df_rmse)

# df_rmse <- read.csv("../data/optim_results_90.csv")
# print(df_rmse)
```


### Plus petite advection

```{r}
adv <- c(0.5, 0.5) # pixel by time?
true_param <- c(0.4, 0.2, 1.5, 1, adv)
ngrid <- 2
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:100
n.BR <- 1

BR <- sim_BR_adv(true_param[1] * 2, true_param[2] * 2, true_param[3],
                    true_param[4], spa, spa, temp, n.BR, adv)


save_simulations(BR, ngrid, n.BR,
        folder = paste0("../data/simulations_BR/sim_adv_", ngrid^2, "s_",
                                length(temp), "t/"),
                  file = paste0("br_", ngrid^2, "s_",
                                length(temp), "t"), forcedind = 1)

# load the simulations
file_path <- paste0("../data/simulations_BR/sim_adv_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df <- read.csv(file_path)
```


```{r}
nsites <- ncol(simu_df)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
```


```{r, message=FALSE}
sites_coords <- generate_grid_coords(sqrt(nsites))
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = 0:10)

hmax <- sqrt(17)
q <- 0.8

# excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

result <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1, 1, 1),
                                      maxit = 10000))

print(result$convergence) # 0 if it has converged
print(result$par)

if (result$convergence == 0) {
  rmse <- sqrt((result$par - true_param)^2)
  df_rmse <- data.frame(estim = result$par, rmse = rmse)
  rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2", "Vx", "Vy")
  # print(t(df_rmse))
  # save the results
  namefile <- paste0("../data/optim_results_adv_9s_100t_", q * 100, ".csv")
  write.csv(t(df_rmse), file = namefile)
  } else {
  print("No convergence")
}

# get result from csv
df_rmse <- read.csv("../data/optim_results_adv_9s_100t_80.csv")
print(df_rmse)

df_rmse <- read.csv("../data/optim_results_adv2_92.csv")
print(df_rmse)

df_rmse <- read.csv("../data/optim_results_adv2_95.csv")
print(df_rmse)
```



### Advection nulle

```{r}
adv <- c(0, 0) # pixel by time?
true_param <- c(0.4, 0.2, 1.5, 1, adv)
ngrid <- 5
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:300
n.BR <- 1

BR <- sim_BR_adv(true_param[1] * 2, true_param[2] * 2, true_param[3],
                    true_param[4], spa, spa, temp, n.BR)


save_simulations(BR, ngrid, n.BR,
        folder = paste0("../data/simulations_BR/oldsim_noadv_", ngrid^2, "s_",
                                length(temp), "t/"),
                  file = paste0("br_", ngrid^2, "s_",
                                length(temp), "t"), forcedind = 1)

# load the simulations
file_path <- paste0("../data/simulations_BR/oldsim_noadv_", ngrid^2, "s_",
                                length(temp), "t/br_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")
simu_df <- read.csv(file_path)
```


```{r, message=FALSE}
sites_coords <- generate_grid_coords(sqrt(nsites))
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = 0:10)

hmax <- sqrt(17)
q <- 0.9

# excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

result <- optim(par = c(true_param), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1, 1, 1),
                                      maxit = 10000))

print(result$convergence) # 0 if it has converged
print(result$par)

if (result$convergence == 0) {
  rmse <- sqrt((result$par - true_param)^2)
  df_rmse <- data.frame(estim = result$par, rmse = rmse)
  rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2", "Vx", "Vy")
  # print(t(df_rmse))
  # save the results
  namefile <- paste0("../data/optim_results_noadv_", q * 100, ".csv")
  write.csv(t(df_rmse), file = namefile)
} else {
  print("No convergence")
}

# get result from csv
df_rmse <- read.csv("../data/optim_results_noadv_90.csv")
print(df_rmse) # super rapide, pq?
```


```{r}
neg_ll_par <- function(beta1, beta2, alpha1, alpha2,
                  adv1, adv2,
                  simu, df_lags, locations,
                  latlon = FALSE, quantile = 0.9,
                  simu_exp = FALSE, excesses = NULL) {
  params <- c(beta1, beta2, alpha1, alpha2, adv1, adv2)
  print(params)
  hmax <- max(df_lags$hnorm)
  tau <- unique(df_lags$tau)
  # print(params)

  lower.bound <- c(1e-6, 1e-6, 1e-6, 1e-6)
  upper.bound <- c(Inf, Inf, 1.999, 1.999)
  if (length(params) == 6) {
    lower.bound <- c(lower.bound, -1e-6, -1e-6)
    upper.bound <- c(upper.bound, Inf, Inf)
  }

  # Check if the parameters are in the bounds
  if (any(params < lower.bound) || any(params > upper.bound)) {
    message("out of bounds")
    return(1e9)
  }

  df_lags <- get_lag_vectors(locations, params, tau = tau, hmax = hmax)
  excesses <- empirical_excesses(simu, quantile, df_lags)

  nj <- excesses$nj # number of marginal excesses
  n <- nrow(simu)
  pj <- nj / n
  kij <- excesses$kij # number of joint excesses
  chi <- theoretical_chi(params, df_lags) # get chi matrix
  # transform in chi vector
  chi_vect <- as.vector(chi$chi)
  chi_vect <- ifelse(chi_vect <= 0, 0.000001, chi_vect) # avoid log(0)

  non_excesses <- n - kij # number of non-excesses
  # log-likelihood vector
  ll_vect <- kij * log(chi_vect) + non_excesses * log(1 - pj * chi_vect)

  # final negative log-likelihood
  nll <- -sum(ll_vect, na.rm = TRUE)
  return(nll)
}
```

```{r}
library(bbmle)

# q = 0.62 (reminder)
res <- mle2(neg_ll_par, start = list(beta1 = true_param[1],
                                 beta2 = true_param[2],
                                 alpha1 = true_param[3],
                                 alpha2 = true_param[4],
                                 adv1 =  true_param[5],
                                 adv2 = true_param[6]),
                 data = list(simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        method = "CG"),
                  control = list(maxit = 10000),
                  fixed = list(beta1 = 0.4, beta2 = 0.2,
                               alpha1 = 1.5, alpha2 = 1,
                               adv2 = 0.5))

print(res@details$conv) # 0 means convergence
print(res@coef)
```


## R-pareto

```{r}
sim_rpareto <- function(beta1, beta2, alpha1, alpha2, x, y, t, n.res,
                        adv = c(0, 0)) {
  # beta1, beta2, alpha1, alpha2 are variogram parameters
  # x is the first dimension (spatial x in our case)
  # y is the second dimension (spatial y in our case)
  # z is the third dimension (time in our case)
  # (adv1, adv2) advection coordinates vector
  ## Setup
  RandomFields::RFoptions(spConform = FALSE, install = "no")
  lx <- length(sx <- seq_along(x))  # spatial
  ly <- length(sy <- seq_along(y))  # spatial
  lt <- length(st <- seq_along(t))  # temporal

  ## Model-Variogram BuhlCklu
  modelBuhlCklu <- RandomFields::RMfbm(alpha = alpha1, var = beta1, proj = 1) +
                   RandomFields::RMfbm(alpha = alpha1, var = beta1, proj = 2) +
                   RandomFields::RMfbm(alpha = alpha2, var = beta2, proj = 3)

  ## Construct grid
  Nxy <- lx * ly # spatial grid size
  N <- Nxy * lt # spatio-temporal grid size
  grid <- matrix(0, nrow = N, ncol = 3) # (N,3)-matrix

  for (i in sx)
    for (j in seq_len(ly * lt))
      grid[i + (j - 1) * ly, 1] <- i

  for (i in sy)
    for (j in sx)
      for (k in st)
        grid[j + lx * (i - 1) + (k - 1) * Nxy, 2] <- i

  for (i in st)
    for (j in seq_len(Nxy))
      grid[j + Nxy * (i - 1), 3] <- i

  # Construct shifted grid with advected coordinates
  grid[, 1:2] <- grid[, 1:2] - grid[, 3] * adv

  s0_x <- 1 # Spatial x conditioning point
  s0_y <- 5  # Spatial y conditioning point
  t0 <- 1  # Temporal conditioning point
  s0_t0 <- s0_x + (s0_y - 1) * lx + (t0 - 1) * Nxy
  # s0_t0 <- s0 + (t0 - 1) * lx^2
  # grid[,s0]

  ## Construct shifted variogram
  gamma <- vapply(seq_len(N), function(n)
      RandomFields::RFvariogram(modelBuhlCklu,
        x = sx - grid[n, 1],
        y = sy - grid[n, 2],
        z = st - grid[n, 3]),
        array(NA_real_, dim = c(lx, ly, lt))) ## => (lx, ly, lt, N)-array


  # Main
  # s0 <- 1
  Z <- array(, dim = c(lx, ly, lt, n.res)) # 3d array
  for (i in seq_len(n.res)) {
    W <- RandomFields::RFsimulate(modelBuhlCklu, x, y, t) # Gaussian process
    Y <- exp(W - W[s0_x, s0_y, t0] - gamma[,,, s0_t0]) # TODO
    R <- evd::rgpd(n = 1, loc = 1, scale = 1, shape = 1)
    Z[,,, i] <- R * Y
  }
  # Return
  Z
}
```

```{r}
true_param <- c(0.4, 0.05, 1.5, 1)
ngrid <- 5
spa <- 1:ngrid
nsites <- ngrid^2 # if the grid is squared
temp <- 1:300
n.BR <- 1

adv <- c(1, 2)

rpar <- sim_rpareto(true_param[1] * 2, true_param[2] * 2, true_param[3],
                    true_param[4], spa, spa, temp, n.BR, adv = adv)

save_simulations(rpar, ngrid, n.BR,
        folder = paste0("../data/simulations_rpar/sim_", ngrid^2, "s_",
                                length(temp), "t/"),
                  file = paste0("rpar_", ngrid^2, "s_",
                                length(temp), "t"), forcedind = 1)

# load the simulations
file_path <- paste0("../data/simulations_rpar/sim_", ngrid^2, "s_",
                                length(temp), "t/rpar_",
                      ngrid^2, "s_", length(temp), "t_", 1, ".csv")

simu_df <- read.csv(file_path)

plot(simu_df[, 5, 5])
```



```{r}
# simulation gif
create_simu_gif(simu_df, true_param, type = "rpar", forcedtemp = 50)
```


```{r}
nsites <- ncol(simu_df)
sites_coords <- generate_grid_coords(sqrt(nsites))
dist_mat <- get_dist_mat(sites_coords,
                         latlon = FALSE) # distance matrix
df_dist <- reshape_distances(dist_mat) # reshape the distance matrix
```

```{r, message=FALSE}
sites_coords <- generate_grid_coords(sqrt(nsites))
df_lags <- get_lag_vectors(sites_coords, true_param,
                          hmax = sqrt(17), tau_vect = 0:10)

hmax <- sqrt(17)
q <- 0.9

# excesses <- empirical_excesses(simu_df, quantile = q, df_lags = df_lags)

result <- optim(par = c(true_param, adv), fn = neg_ll,
                        simu = simu_df,
                        quantile = q,
                        df_lags = df_lags,
                        locations = sites_coords,
                        method = "CG",
                        control = list(parscale = c(1, 1, 1, 1, 1, 1),
                                      maxit = 10000))

print(result$convergence) # 0 if it has converged
print(result$par)

if (result$convergence == 0) {
  rmse <- sqrt((result$par - true_param)^2)
  df_rmse <- data.frame(estim = result$par, rmse = rmse)
  rownames(df_rmse) <- c("beta1", "beta2", "alpha1", "alpha2", "Vx", "Vy")
  # print(t(df_rmse))
  # save the results
  namefile <- paste0("../data/optim_results_noadv_", q * 100, ".csv")
  write.csv(t(df_rmse), file = namefile)
} else {
  print("No convergence")
}

# get result from csv
df_rmse <- read.csv("../data/optim_results_noadv_90.csv")
print(df_rmse) # super rapide, pq?
```




## Profile log-likelihood

```{r}
profiled_neg_ll <- function(theta, simu, df_lags, locations,
                            latlon = FALSE, quantile = 0.9,
                            simu_exp = FALSE, excesses = NULL) {

  # Function to optimize
  optim_function <- function(psi) {
    params <- c(theta, psi)
    return(neg_ll(params, simu, df_lags, locations,
                  latlon, quantile, simu_exp, excesses))
  }

  # Initialize the parameters of nuisance
  init_psi <- c(0, 0)

  # Optim on psi
  opt_result <- optim(par = init_psi, fn = optim_function, method = "BFGS")
  return(opt_result$value)
}
```

```{r}
# Exemple d'utilisation
theta <- c(0.4, 0.2, 1.5, 1)
psi <- c(0, 0)
profiled_neg_ll(theta, simu_df, df_lags, sites_coords)
```
